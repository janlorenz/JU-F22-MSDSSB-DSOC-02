[
  {
    "objectID": "late.html",
    "href": "late.html",
    "title": "Info for late coming students",
    "section": "",
    "text": "We know there are reasons for coming late which may not lie in your hands. We try to help you to get into the course in this situation. The following checklist should guide you.\n\nCheck list for late coming students\n\nRead this Syllabus\nMost urgent\n\n\nLook at slides 30 to 32 of the Week 1: https://docs.google.com/presentation/d/1GyEClkRDo5aOKuHeOZxgvz2uqo_x5t-c1CQ-ksGXTq0/edit#slide=id.g14992662fb0_0_115 and start to build your data science toolkit on your computer.\nPart of it is that you should submit your GitHub-username in a Google Form. To speed up thing, also send your GitHub-username to Jan Lorenz via Teams!\n\n\nWork through the materials of the Sessions you have missed.\n\n\nYou find the materials in the Section “Course Material and Schedule” at the end of the syllabus.\nThere are links to the slides.\nOn MS Teams you find recordings in the Team F22_MDSSB-DSOC-02_Data Science Concepts in the General Channel -> Files -> Recordings\n\n\nDo the Homework you have missed even if you are beyond the deadline. You find the Homework instructions on this website. Homework 01 introduces you to the workflow needed for further Homework.\nVisit the next lectures in Data Science Concepts and the sessions in the two Data Science Tools course. When you cannot be there in person visit via Teams. Meetings will be started in the General Channel when the course starts.\nWhen you get stuck or lost, ask your fellow students or contact us!"
  },
  {
    "objectID": "W6.html#functions-mathematically",
    "href": "W6.html#functions-mathematically",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Functions mathematically",
    "text": "Functions mathematically\nConsider two sets: The domain \\(X\\) and the codomain \\(Y\\).\nA function \\(f\\) assigns each element of \\(X\\) to exactly one element of \\(Y\\).\n\n\nWe write \\(f : X \\to Y\\)\n“\\(f\\) maps from \\(X\\) to \\(Y\\)”\nand \\(x \\mapsto f(x)\\)\n“\\(x\\) maps to \\(f(x)\\)”\nThe yellow set is called the image of \\(f\\).\n\n\n\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W6.html#conventions-in-mathematical-text",
    "href": "W6.html#conventions-in-mathematical-text",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Conventions in mathematical text",
    "text": "Conventions in mathematical text\n\nSets are denoted with capital letters.\nTheir elements with (corresponding) small letters.\nFunctions are often called \\(f\\), \\(g\\), or \\(h\\).\nOther terminology can be used!\n\nResponsibility of the mathematical writer: Define objects.\nResponsibility of the mathematical reader: Keep track of what objects are."
  },
  {
    "objectID": "W6.html#is-this-a-function",
    "href": "W6.html#is-this-a-function",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Is this a function?",
    "text": "Is this a function?\nInput from \\(X = \\{\\text{A picture where a face can be recognized}\\}\\).\nFunction: Upload input at https://funny.pho.to/lion/ and download output.\n \\(\\ \\mapsto\\ \\) \nOutput from \\(Y = \\{\\text{Set of pictures with a specific format.}\\}\\)\n\nYes, it is a function. Important: Output is the same for the same input!"
  },
  {
    "objectID": "W6.html#is-this-a-function-1",
    "href": "W6.html#is-this-a-function-1",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Is this a function?",
    "text": "Is this a function?\nInput a text snippet. Function: Enter text at https://www.craiyon.com. Output a picture.\n\n\n\n\nOther examples:\n\n“Nuclear explosion broccoli”\n“The Eye of Sauron reading a newspaper”\n“The legendary attack of Hamster Godzilla wearing a tiny Sombrero”\n\n  \n\n\n\nNo, it is not a function. It has nine outcomes and these change when run again."
  },
  {
    "objectID": "W6.html#graphs-of-functions",
    "href": "W6.html#graphs-of-functions",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Graphs of functions",
    "text": "Graphs of functions\n\nA function is characterized by the set all possible pairs \\((x,f(x))\\).\nThis is called its graph. Note, this can be infinitely many.\nWhen domain and codomain are real numbers then the graph can be shown in a Cartesian coordinate system. Example \\(f(x) = x^3 - x^2\\)"
  },
  {
    "objectID": "W6.html#some-functions-f-mathbbr-to-mathbbr",
    "href": "W6.html#some-functions-f-mathbbr-to-mathbbr",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)",
    "text": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)\n\n\n\\(f(x) = x\\) identity function\n\\(f(x) = x^2\\) square function\n\\(f(x) = \\sqrt{x}\\) square root function\n\\(f(x) = e^x\\) exponential function\n\\(f(x) = \\log(x)\\) natural logarithm\n\nSquare function and square root function are the inverse of each other. Exponential and natural logarithm, too.\n\n\\(\\sqrt[2]{x}^2 = \\sqrt[2]{x^2} = x\\), \\(\\log(e^x) = e^{\\log(x)} = x\\)\n\nTheir graphs reflect each other with identity function graph as mirror axis.\n\n\n\n\n\n\n\n\n\n\n\n\\(e\\) is Euler’s number \\(2.71828\\dots\\). The natural logarithm is also often called \\(\\ln\\). The square root function is \\(\\mathbb{R}_{\\geq 0} \\to \\mathbb{R}\\), the logarithm \\(\\mathbb{R}_{>0} \\to \\mathbb{R}\\)."
  },
  {
    "objectID": "W6.html#shifts-and-scales",
    "href": "W6.html#shifts-and-scales",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Shifts and scales",
    "text": "Shifts and scales\nHow can we shift, stretch, or shrink a graph vertically and horizontally?\n\n\n\\(y\\)-shift\\(x\\)-shift\\(y\\)-scale\\(x\\)-scale\n\n\n\n\nAdd a constant to the function.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = x^3 - x^2 + a\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\n\n\n\n\n\n\n\n\n\n\n\n\nSubtract a constant from all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x - a)^3 - (x - a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nAttention:\nShifting as you think needs subtracting \\(a\\)!\nYou can think of the coordinate system being shifted in direction \\(a\\) while the graph stays.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiply a constant to all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = a(x^3 - x^2)\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(x\\)-axis.\n\n\n\n\n\n\n\n\n\n\n\n\nDivide all \\(x\\) within the function definition by a constant.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x/a)^3 - (x/a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(y\\)-axis.\nAttention: Stretching needs a division by \\(a\\)!\nYou can think of the coordinate system being stretched multiplicatively by \\(a\\) while the graph stays."
  },
  {
    "objectID": "W6.html#polynomials-and-exponentials",
    "href": "W6.html#polynomials-and-exponentials",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Polynomials and exponentials",
    "text": "Polynomials and exponentials\nA polynomial is a function which is composed of (many) addends of the form \\(ax^n\\) for different values of \\(a\\) and \\(n\\).\nIn an exponential the \\(x\\) appears in the exponent.\n\\(f(x) = x^3\\) vs. \\(f(x) = e^x\\)\n\nFor \\(x\\to\\infty\\), any exponential will finally “overtake” any polynomial."
  },
  {
    "objectID": "W6.html#rules-for-exponentiation",
    "href": "W6.html#rules-for-exponentiation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Rules for exponentiation",
    "text": "Rules for exponentiation\n\n\n\\(x^0\\)\n\\(0^x\\)\n\\(0^0\\)\n\\((x\\cdot y)^a\\)\n\\(x^{-a}\\), \\(x^{-1}\\)\n\\(x^\\frac{a}{b}\\), \\(x^\\frac{1}{2}\\)\n\\((x^a)^b\\)\n\n\n\\(x^0 = 1\\)\n\n\n\\(0^x = 0\\) for \\(x\\neq 0\\)\n\n\n\\(0^0 = 1\\) (discontinuity in \\(0^x\\))\n\n\n\\((x\\cdot y)^a = x^a\\cdot x^b\\)\n\n\n\\(x^{-a} = \\frac{1}{x^a}\\), \\(x^{-1} = \\frac{1}{x}\\)\n\n\n\\(x^\\frac{a}{b} = \\sqrt[b]{x^a} = (\\sqrt[b]{x})^a,\\ x^\\frac{1}{2} = \\sqrt{x}\\)\n\n\n\\((x^a)^b = x^{a\\cdot b} = (x^b)^a \\neq x^{a^b} = x^{(a^b)}\\)\nExample: \\((4^3)^2 = 64^2 = 4096 \\qquad 4^{3^2} = 4^9 = 262144\\)"
  },
  {
    "objectID": "W6.html#more-rules-for-exponentiation",
    "href": "W6.html#more-rules-for-exponentiation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "More rules for exponentiation",
    "text": "More rules for exponentiation\n\n\n\\(x^a\\cdot x^b\\)\n\n\n\\(x^a\\cdot x^b = x^{a+b}\\) Multiplication of powers (with same base \\(x\\)) becomes addition of exponents.\n\n\n\n\n\n\n\\((x+y)^a\\)\n\n\nNo “simple” form! For \\(a\\) integer use binomial expansion. \\((x+y)^2 = x^2 + 2xy + y^2\\)\n\\((x+y)^3 = x^3 + 3x^2y + 3xy^2 + y^3\\)\n\\((x+y)^n = \\sum_{k=0}^n {n \\choose k} x^{n-k}y^k\\)\n\n\n\nPascal’s triangle\n\n\n\n\n\n\n\n\n\n\n\n\nFrom wikipedia\n\n\n\nWe meet it again in Probability. (Binomial distribution, Central Limit Theorem)"
  },
  {
    "objectID": "W6.html#logarithms",
    "href": "W6.html#logarithms",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Logarithms",
    "text": "Logarithms\nDefinition: A logarithm of \\(a\\) for some base \\(b\\) is the value of the exponent which brings \\(b\\) to \\(a\\): \\(\\log_b(a) = x\\) means that \\(b^x =a\\)\nMost common:\n\n\\(\\log_{10}\\) for logarithmic axes in plots\n\\(\\log_{e}\\) natural logarithm (also \\(\\log\\) or \\(\\ln\\))\n\n\n\n\n\\(\\log_{10}(100) =\\)\n\n\n\\(2\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(1) =\\)\n\n\n\\(0\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(6590) =\\)\n\n\n\\(3.818885\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(0.02) =\\)\n\n\n\\(-1.69897\\)"
  },
  {
    "objectID": "W6.html#rules-for-logarithms",
    "href": "W6.html#rules-for-logarithms",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Rules for logarithms",
    "text": "Rules for logarithms\nUsually only one base is used in the same context, because changing base is easy:\n\\(\\log_c(x) = \\frac{log_b(x)}{\\log_b(c)} = \\frac{\\log(x)}{\\log(c)}\\)\n\n\n\\(\\log(x\\cdot y)\\)\n\n\n\\(= \\log(x) + \\log(y)\\) Multiplication \\(\\to\\) addition.\n\n\n\n\n\n\n\\(\\log(x^y)\\)\n\n\n\\(= y\\cdot\\log(x)\\)\n\n\n\n\n\n\\(\\log(x+y)\\)\n\n\ncomplicated!\n\n\n\n\n\nAlso changing bases for powers is easy: \\(x^y = (e^{\\log(x)})^y = e^{y\\cdot\\log(x)}\\)"
  },
  {
    "objectID": "W6.html#input-to-output",
    "href": "W6.html#input-to-output",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Input \\(\\to\\) output",
    "text": "Input \\(\\to\\) output\n\n\nMetaphorically, a function is a machine or a blackbox that for each input yields and output.\nThe inputs of a function are also called arguments.\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W6.html#function-as-objects-in-r",
    "href": "W6.html#function-as-objects-in-r",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Function as objects in R",
    "text": "Function as objects in R\nfunction is a class of an object in R\n\nclass(c)\n\n[1] \"function\"\n\nclass(ggplot2::ggplot)\n\n[1] \"function\"\n\n\nCalling the function without brackets writes its code or some information.\n\nsd # This function is written in R\n\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n<bytecode: 0x55d7fa83ba00>\n<environment: namespace:stats>\n\nc  \n\nfunction (...)  .Primitive(\"c\")\n\nggplot2::ggplot \n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \n{\n    UseMethod(\"ggplot\")\n}\n<bytecode: 0x55d7fa947020>\n<environment: namespace:ggplot2>"
  },
  {
    "objectID": "W6.html#functions-in-r",
    "href": "W6.html#functions-in-r",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Functions in R",
    "text": "Functions in R\nDefine your own functions like this\n\nadd_one <- function(x) {\n  x + 1 \n}\n# Test it\nadd_one(10)\n\n[1] 11\n\n\nThe skeleton for a function definition is\nfunction_name <- function(input){\n  # do something with the input(s)\n  # return something as output\n}\n\nfunction_name should be a short but evocative verb.\nThe input can be empty or one or more name or name=expression terms as arguments.\nThe last evaluated expression is returned as output.\nWhen the body or the function is only one line {} can be omitted. For example\nadd_one <- function(x) x + 1"
  },
  {
    "objectID": "W6.html#flexibility-of-inputs-and-outputs",
    "href": "W6.html#flexibility-of-inputs-and-outputs",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Flexibility of inputs and outputs",
    "text": "Flexibility of inputs and outputs\n\nArguments can be specified by name=expression or just expression (then they are taken as the next argument)\nDefault values for arguments can be provided. Useful when an argument is a parameter.\n\n\n\nmymult <- function(x = 2, y = 3) x * (y - 1)\nmymult(3,4)\n\n\n[1] 9\n\n\n\n\n\nmymult()\n\n\n[1] 4\n\n\n\n\n\nmymult(y = 3,4)\n\n\n[1] 8\n\n\n\n\n\nmymult(5)\n\n\n[1] 10\n\n\n\n\n\nmymult(y = 2)\n\n\n[1] 2\n\n\n\n\nFor complex output use a list\n\n\nmymult <- function(x = 2, y = 3) \n  list(out1 = x * (y - 1), out2 = x * (y - 2))\nmymult()\n\n\n$out1\n[1] 4\n\n$out2\n[1] 2"
  },
  {
    "objectID": "W6.html#vectorized-functions",
    "href": "W6.html#vectorized-functions",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Vectorized functions",
    "text": "Vectorized functions\nMathematical functions in programming are often “vectorized”:\n\nOperations on a single value are applied to each component of the vector.\nOperations on two values are applied “component-wise” (for vectors of the same length)\n\n\nlog10(c(1,10,100,1000,10000))\n\n[1] 0 1 2 3 4\n\nc(1,1,2) + c(3,1,0)\n\n[1] 4 2 2\n\n(0:5)^2\n\n[1]  0  1  4  9 16 25"
  },
  {
    "objectID": "W6.html#vector-creation-functions",
    "href": "W6.html#vector-creation-functions",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Vector creation functions",
    "text": "Vector creation functions\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from=-0.5, to=1.5, by=0.1)\n\n [1] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9\n[16]  1.0  1.1  1.2  1.3  1.4  1.5\n\nseq(from=0, to=1, length.out=10)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000\n\nrep(1:3, times=3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\nrep(1:3, each=3)\n\n[1] 1 1 1 2 2 2 3 3 3"
  },
  {
    "objectID": "W6.html#plotting-and-transformation",
    "href": "W6.html#plotting-and-transformation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Plotting and transformation",
    "text": "Plotting and transformation\nVector creation and vectorized functions are key for plotting and transformation.\n\nfunc <- function(x) x^3 - x^2    # Create a vectorized function\ndata <- tibble(x = seq(-0.5,1.5,by =0.01)) |>    # Vector creation\n    mutate(y = func(x))        # Vectorized transformation using the function\ndata |> ggplot(aes(x,y)) + geom_line()"
  },
  {
    "objectID": "W6.html#convenient-function-ggploting",
    "href": "W6.html#convenient-function-ggploting",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Convenient function ggploting",
    "text": "Convenient function ggploting\n\nggplot() +\n    geom_function(fun = log) +\n    geom_function(fun = function(x) 3*x - 4, color = \"red\")"
  },
  {
    "objectID": "W6.html#purpose-of-modeling",
    "href": "W6.html#purpose-of-modeling",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Purpose of modeling",
    "text": "Purpose of modeling\nWe use models to\n\nexplain relations between variables\nmake predictions\n\nFirst, we focus on linear models."
  },
  {
    "objectID": "W6.html#palmer-penguins",
    "href": "W6.html#palmer-penguins",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Palmer Penguins",
    "text": "Palmer Penguins\nWe use the dataset Palmer Penguins\nChinstrap, Gentoo, and Adélie Penguins\n  \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g"
  },
  {
    "objectID": "W6.html#body-mass-in-grams",
    "href": "W6.html#body-mass-in-grams",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Body mass in grams",
    "text": "Body mass in grams\n\npenguins |>\n  ggplot(aes(body_mass_g)) +\n  geom_histogram()"
  },
  {
    "objectID": "W6.html#flipper-length-in-millimeters",
    "href": "W6.html#flipper-length-in-millimeters",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Flipper length in millimeters",
    "text": "Flipper length in millimeters\n\npenguins |>\n  ggplot(aes(flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "W6.html#relate-variables-as-a-line",
    "href": "W6.html#relate-variables-as-a-line",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Relate variables as a line",
    "text": "Relate variables as a line\nA line is a shift-scale transformation of the identity function usually written in the form\n\\[f(x) = a\\cdot x + b\\]\nwhere \\(a\\) is the slope, \\(b\\) is the intercept.1\n\n\na <- 0.5\nb <- 1\nfunc <- function(x) a*x + b\nggplot() + geom_function(fun = func, size = 2) + \n    xlim(c(0,2)) + ylim(c(0,2)) + coord_fixed() + # Set axis limits and make axis equal\n    # intercept line:\n    geom_line(data=tibble(x=c(0,0),y=c(0,1)), mapping = aes(x,y), color = \"blue\") +\n    # slope:\n    geom_line(data=tibble(x=c(1.5,1.5),y=c(1.25,1.75)), mapping = aes(x,y), color = \"red\") +\n    # x-interval of length one:\n    geom_line(data=tibble(x=c(0.5,1.5),y=c(1.25,1.25)), mapping = aes(x,y), color = \"gray\") +\n    theme_classic(base_size = 24)\n\n\n\n\n\n\nThis a scale and a shift in the \\(y\\) direction. Note: For lines there are always an analog transformations on the \\(x\\) direction."
  },
  {
    "objectID": "W6.html#penguins-linear-model",
    "href": "W6.html#penguins-linear-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Penguins: Linear model",
    "text": "Penguins: Linear model\nFlipper length as a function of body mass.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W6.html#penguins-other-smoothing-method",
    "href": "W6.html#penguins-other-smoothing-method",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Penguins: Other smoothing method",
    "text": "Penguins: Other smoothing method\nFlipper length as a function of body mass with loess1 smoothing.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"loess\") + \n theme_classic(base_size = 24)\n\n\n\n\n\n\nThis is a less theory-driven and more data-driven model. Why? We don’t have a simple mathematical form of the function.\nloess = locally estimated scatterplot smoothing"
  },
  {
    "objectID": "W6.html#terminology",
    "href": "W6.html#terminology",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Terminology",
    "text": "Terminology\n\nResponse variable:1 Variable whose behavior or variation you are trying to understand, on the y-axis\nExplanatory variable(s):2 Other variable(s) that you want to use to explain the variation in the response, on the x-axis\nPredicted value: Output of the model function.\n\nThe model function gives the typical (expected) value of the response variable conditioning on the explanatory variables\nResidual(s): A measure of how far away a case is from its predicted value (based on a particular model)\nResidual = Observed value - Predicted value\nThe residual tells how far above/below the expected value each case is\n\n\nAlso dependent variable in statistics or empirical social sciences.Also independent variable(s) in statistics or empirical social sciences."
  },
  {
    "objectID": "W6.html#more-explanatory-variables",
    "href": "W6.html#more-explanatory-variables",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "More explanatory variables",
    "text": "More explanatory variables\nHow does the relation between flipper length and body mass change with different species?\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm, \n            color = species)) +\n geom_point() +\n geom_smooth(method = \"lm\",\n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W6.html#technical-how-to-color-penguins-but-keep-one-model",
    "href": "W6.html#technical-how-to-color-penguins-but-keep-one-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Technical: How to color penguins but keep one model?",
    "text": "Technical: How to color penguins but keep one model?\nPut the mapping of the color aesthetic into the geom_point command.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n                                                y = flipper_length_mm)) +\n geom_point(aes(color = species)) +\n geom_smooth(method = \"lm\",\n                                                    se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W6.html#models---upsides-and-downsides",
    "href": "W6.html#models---upsides-and-downsides",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can reveal patterns that are not evident in a graph of the data. This is an advantage of modeling over simple visual inspection of data.\nThe risk is that a model is imposing structure that is not really there in the real world data.\n\nPeople imagined animal shapes in the stars. This is maybe a good model to detect and memorize shapes, but it has nothing to do with these animals.\nEvery model is a simplification of the real world, but there are good and bad models (for particular purposes).\nA skeptical (but constructive) approach to a model is always advisable."
  },
  {
    "objectID": "W6.html#variation-around-a-model",
    "href": "W6.html#variation-around-a-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Variation around a model",
    "text": "Variation around a model\nis as interesting and important as the model!\nStatistics is the explanation of uncertainty of variation in the context of what remains unexplained.\n\nThe scattered data of flipper length and body mass suggests that there maybe other factors that account for some parts of the variability.\nOr is it randomness?\nAdding more explanatory variables can help (but need not)"
  },
  {
    "objectID": "W6.html#all-models-are-wrong",
    "href": "W6.html#all-models-are-wrong",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "All models are wrong …",
    "text": "All models are wrong …\n… but some are useful. (George Box)\nExtending the range of the model:\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE, \n                                                fullrange = TRUE) +\n    xlim(c(0,7000)) + ylim(c(0,230)) +\n theme_classic(base_size = 24)\n\n\n\n\n\n\n\nThe model predicts that penguins with zero weight still have flippers of about 140 mm on average.\nIs the model useless?"
  },
  {
    "objectID": "W6.html#two-model-purposes",
    "href": "W6.html#two-model-purposes",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Two model purposes",
    "text": "Two model purposes\nModels can be used for:\n\nExplanation: Understand the relations hip of variables in a quantitative way.\nFor the linear model, interpret slope and intercept.\nPrediction: Plug in new values for the explanatory variable(s) and receive the expected response value.\nFor the linear model, predict the flipper length of new penguins by their body mass."
  },
  {
    "objectID": "W6.html#in-r-tidymodels",
    "href": "W6.html#in-r-tidymodels",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "In R: tidymodels",
    "text": "In R: tidymodels\n\n\n\nFrom https://datasciencebox.org"
  },
  {
    "objectID": "W6.html#our-goal",
    "href": "W6.html#our-goal",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Our goal",
    "text": "Our goal\nPredict flipper length from body mass\naverage flipper_length_mm \\(= \\beta_0 + \\beta_1\\cdot\\) body_mass_g"
  },
  {
    "objectID": "W6.html#step-1-specify-model",
    "href": "W6.html#step-1-specify-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlibrary(tidymodels)\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W6.html#step-2-set-the-model-fitting-engine",
    "href": "W6.html#step-2-set-the-model-fitting-engine",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Step 2: Set the model fitting engine",
    "text": "Step 2: Set the model fitting engine\n\nlinear_reg() |> \n    set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W6.html#step-3-fit-model-and-estimate-parameters",
    "href": "W6.html#step-3-fit-model-and-estimate-parameters",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Step 3: Fit model and estimate parameters",
    "text": "Step 3: Fit model and estimate parameters\nOnly now, the data and the variable selection comes in.\nUse of formula syntax\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\n\nNote: The fit command does not follow the tidyverse principle the the data comes first. Instead, the formula comes first. This is to relate to existing traditions of a much older established way of modeling in R."
  },
  {
    "objectID": "W6.html#what-does-the-output-say",
    "href": "W6.html#what-does-the-output-say",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "What does the output say?",
    "text": "What does the output say?\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\naverage flipper_length_mm \\(= 136.72956 + 0.01528\\cdot\\) body_mass_g\n\n\nInterpretation:\nThe penguins have a flipper length of 138 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg). Penguins with zero mass have a flipper length of 138 mm. However, this is not in the range where the model was fitted …"
  },
  {
    "objectID": "W6.html#show-output-in-tidy-form",
    "href": "W6.html#show-output-in-tidy-form",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Show output in tidy form",
    "text": "Show output in tidy form\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n    tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107"
  },
  {
    "objectID": "W6.html#parameter-estimation",
    "href": "W6.html#parameter-estimation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nNotation from statistics: \\(\\beta\\)’s for the population parameters and \\(b\\)’s for the parameters estimated from the sample statistics.\n\\[\\hat y = \\beta_0 + \\beta_1 x\\]\nIs what we cannot have. (\\(\\hat y\\) stands for predicted value of \\(y\\). )\n\nWe estimate \\(b_0\\) and \\(b_1\\) in\n\\[\\hat y = b_0 + b_1 x\\]\n\nA typical follow-up data analysis question is what the fitted values \\(b_0\\) and \\(b_1\\) tell us about the population-wide values \\(\\beta_0\\) and \\(\\beta_1\\)?\nWhat type of question is it?\nDescriptive, Exploratory, Inferential, Predictive, Causal, Mechanistic\n\n\n\n\nA typical inferential question."
  },
  {
    "objectID": "W6.html#fitting-method-least-squares-regression",
    "href": "W6.html#fitting-method-least-squares-regression",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Fitting method: Least squares regression",
    "text": "Fitting method: Least squares regression\n\nThe regression line shall minimize the sum of the squared residuals (or, identically, their mean).\nMathematically: The residual for case \\(i\\) is \\(e_i = \\hat y_i - y_i\\). Now we want to minimize \\(\\sum_{i=1}^n e_i^2\\) (or $_{i=1}^n e_i^2 the the mean of squared errors)."
  },
  {
    "objectID": "W6.html#visualization-of-residuals",
    "href": "W6.html#visualization-of-residuals",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Visualization of residuals",
    "text": "Visualization of residuals\nThe residuals are the gray lines between predictid values on the regression line and the actual values."
  },
  {
    "objectID": "W6.html#proporties-of-least-squares-regression",
    "href": "W6.html#proporties-of-least-squares-regression",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nThe regression lines goes through the point (mean(x), mean(y)).\n\nmean(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4201.754\n\nmean(penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 200.9152"
  },
  {
    "objectID": "W6.html#proporties-of-least-squares-regression-1",
    "href": "W6.html#proporties-of-least-squares-regression-1",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nResiduals sum up to zero\n\npengmod <- linear_reg() |>  set_engine(\"lm\") |> fit(flipper_length_mm ~ body_mass_g, data = penguins)\npengmod$fit$residuals |> sum()\n\n[1] -3.765044e-14\n\n\nThere is no correlation between residuals and the explanatory variable\n\ncor(pengmod$fit$residuals, na.omit(penguins$body_mass_g))\n\n[1] -1.353445e-16\n\n\nThe correlation of \\(x\\) and \\(y\\) is the slope \\(b_1\\) corrected by their standard deviations.\n\ncorrelation <- cor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")\nsd_flipper <- sd(penguins$flipper_length_mm, na.rm = T)\nsd_mass <- sd(penguins$body_mass_g, na.rm = T)\nc(correlation, sd_flipper, sd_mass)\n\n[1]   0.8712018  14.0617137 801.9545357\n\ncorrelation * sd_flipper / sd_mass\n\n[1] 0.01527592\n\npengmod$fit$coefficients\n\n (Intercept)  body_mass_g \n136.72955927   0.01527592"
  },
  {
    "objectID": "W6.html#linear-model-when-explanatory-variables-are-categorical",
    "href": "W6.html#linear-model-when-explanatory-variables-are-categorical",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Linear model when explanatory variables are categorical",
    "text": "Linear model when explanatory variables are categorical\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\nWhat happened? Two of the three species categories appear as variables now.\n\nCategorical variables are automatically encoded to dummy variables\nEach coefficient describes the expected difference between flipper length of that particular species compared to the baseline level\nWhat is the baseline level?\n\n\n\nThe first category! (Here alphabetically \"Adelie\")"
  },
  {
    "objectID": "W6.html#how-do-dummy-variables-look",
    "href": "W6.html#how-do-dummy-variables-look",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "How do dummy variables look",
    "text": "How do dummy variables look\n\n\n\nspecies\nspeciesChinstrap\nspeciesGentoo\n\n\n\n\nAdelie\n0\n0\n\n\nChinstrap\n1\n0\n\n\nGentoo\n0\n1\n\n\n\nThen the computation is as usual with the zero-one variables."
  },
  {
    "objectID": "W6.html#interpretation",
    "href": "W6.html#interpretation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Interpretation",
    "text": "Interpretation\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nFlipper length of the baseline species is the intercept.\nFlipper length of the two other species add their coefficient"
  },
  {
    "objectID": "W6.html#compare-to-a-visualization",
    "href": "W6.html#compare-to-a-visualization",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Compare to a visualization",
    "text": "Compare to a visualization\n\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nThe red dots are the average values for species.\nThe rest is a boxplot. More on these later."
  },
  {
    "objectID": "W6.html#where-a-linear-model-is-bad",
    "href": "W6.html#where-a-linear-model-is-bad",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Where a linear model is bad",
    "text": "Where a linear model is bad\nTotal corona cases in Germany in the first wave 2020."
  },
  {
    "objectID": "W6.html#log-transformation",
    "href": "W6.html#log-transformation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\nInstead of Cumulative_cases we look at \\(\\log(\\)Cumulative_cases\\()\\)\n\nAlmost perfect fit of a linear model.\nThe model is \\(y=\\beta_0 e^{\\beta_1\\cdot x}\\) (\\(y=\\) Cumulative cases, \\(x=\\) Days).\n\\(\\log(y)=\\log(\\beta_0) + \\beta_1\\cdot x\\) (A linear model!)"
  },
  {
    "objectID": "W6.html#log-transformation-1",
    "href": "W6.html#log-transformation-1",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\n\nWhat is the difference to the penguin model?\n\n\n\\(x\\) has an ordered structure and no duplicates\n\nThe fit looks so good. Why?\n\n\nThis shows exponential growth (more later).\nMaybe we can go after a mechanistic explanation here."
  },
  {
    "objectID": "W6.html#however-it-works-only-in-a-certain-range",
    "href": "W6.html#however-it-works-only-in-a-certain-range",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "However, it works only in a certain range …",
    "text": "However, it works only in a certain range …"
  },
  {
    "objectID": "W7.html#descriptive-vs.-inferential-statistics",
    "href": "W7.html#descriptive-vs.-inferential-statistics",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Descriptive vs. Inferential Statistics",
    "text": "Descriptive vs. Inferential Statistics\n\nThe process of using and analyzing summary statistics\n\nSolely concerned with properties of the observed data.\n\nDistinct from inferential statistics:\n\nInference of properties of an underlying distribution given sampled observations from a larger population.\n\n\nSummary Statistics are used to summarize a set of observations to communicate the largest amount of information as simple as possible."
  },
  {
    "objectID": "W7.html#summary-statistics",
    "href": "W7.html#summary-statistics",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Summary statistics",
    "text": "Summary statistics\nUnivariate (for one variable)\n\nMeasures of location, or central tendency\nMeasures of statistical dispersion\nMeasure of the shape of the distribution like skewness or kurtosis\n\nBivariate (for two variables)\n\nMeasures of statistical dependence or correlation"
  },
  {
    "objectID": "W7.html#measures-of-central-tendency-1",
    "href": "W7.html#measures-of-central-tendency-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nGoal: For a sequence of numerical observations \\(x_1,\\dots,x_n\\) we want to measure\n\nthe “typical” value.\na value summarizing the location of values on the numerical axis.\n\nThree different ways:\n\nArithmetic mean (also mean, average): Sum of the all observations divided by the number of observations \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\)\nMedian: Assume \\(x_1 \\leq x_2 \\leq\\dots\\leq x_n\\). Then the median is middlemost values in the sequence \\(x_\\frac{n+1}{2}\\) when \\(n\\) odd. For \\(n\\) even there are two middlemost values and the median is \\(\\frac{x_\\frac{n}{2} + x_\\frac{n+1}{2}}{2}\\)\nMode: The value that appears most often in the sequence."
  },
  {
    "objectID": "W7.html#philosophy-of-aggregation",
    "href": "W7.html#philosophy-of-aggregation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Philosophy of aggregation",
    "text": "Philosophy of aggregation\n\nThe mean represents total value per value.\nExample: per capita income in a town is the total income per individual\nThe median represents the value such that half of the values are lower and higher.\nIn a democracy where each value is represented by one voter preferring it, the median is the value which is unbeatable by an absolute majority. Half of the people prefer higher the other half lower values. (Median voter model)\nThe mode represents the most common value.\nIn a democracy, the mode represents the winner of a plurality vote where each value runs as a candidate and the winner is the one with the most votes."
  },
  {
    "objectID": "W7.html#mean-median-mode-properties",
    "href": "W7.html#mean-median-mode-properties",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Mean, Median, Mode properties",
    "text": "Mean, Median, Mode properties\nAre all of these well defined? (That means, they deliver one unambiguous answer for any sequence.)\n\nMean and median, yes. The mode has no rules for a tie.\n\n\nCan they by generalized to variables with ordered or even unordered categories?\n\n\nMean: No. Median: For ordered categories. Mode: For any categorical variable.\n\n\nIs measure always also in the data sequence?\n\n\nMean: No. Median: Yes, for sequences of odd length. Mode: Yes."
  },
  {
    "objectID": "W7.html#generalized-means",
    "href": "W7.html#generalized-means",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Generalized means1",
    "text": "Generalized means1\nFor \\(x_1, \\dots, x_n > 0\\) and \\(p\\in \\mathbb{R}_{\\neq 0}\\) the generalized mean is\n\\[M_p(x_1, \\dots, x_n) = (\\frac{1}{n}\\sum_{i=1}^n x_i^p)^\\frac{1}{p}\\]\nFor \\(p = 0\\) it is \\(M_0(x_1, \\dots, x_n) = (\\prod_{i=1}^n x_i)^\\frac{1}{n}\\).\n\\(M_1\\) is the arithmetic mean. \\(M_0\\) is called the geometric mean. \\(M_{-1}\\) the harmonic mean.\nNote: Generalized means are often only reasonable when all values are positive \\(x_i > 0\\).\n\n\n\\(M_0\\) can also be expressed as the exponential (\\(\\exp(x) = e^x\\)) of the mean of the the \\(\\log\\)’s of the \\(x_i\\)’s: \\(\\exp(\\log((\\prod_{i=1}^n x_i)^\\frac{1}{n})) = \\exp(\\frac{1}{n}\\sum_{i=1}^n\\log(x_i))\\).\nAlso called power mean or \\(p\\)-mean."
  },
  {
    "objectID": "W7.html#box-cox-transformation-function",
    "href": "W7.html#box-cox-transformation-function",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Box-Cox transformation function",
    "text": "Box-Cox transformation function\nFor \\(p \\in \\mathbb{R}\\): \\(f(x) = \\begin{cases}\\frac{x^p - 1}{p} & \\text{for $p\\neq 0$} \\\\ \\log(x) & \\text{for $p= 0$}\\end{cases}\\)\n\n\nThe \\(p\\)-mean is\n\\[M_p(x) = f^{-1}(\\frac{1}{n}\\sum_{i=1}^n f(x_i))\\]\nwith \\(x = [x_1, \\dots, x_n]\\). \\(f^{-1}\\) is the inverse1 of \\(f\\).\n\n\n\n\n\n\n\n\nThat means \\(f^-1(f(x)) = x =f(f^-1(x))\\)."
  },
  {
    "objectID": "W7.html#application-the-wisdom-of-the-crowd",
    "href": "W7.html#application-the-wisdom-of-the-crowd",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Application: The Wisdom of the Crowd",
    "text": "Application: The Wisdom of the Crowd\n\n\n\nThe collective opinion of a diverse group of independent individuals rather than that of a single expert.\nThe classical wisdom-of-the-crowds finding is about point estimation of a continuous quantity.\nPopularized by James Surowiecki (2004).\nThe opening anecdote is about Francis Galton’s1 surprise in 1907 that the crowd at a county fair accurately guessed the weight of an ox’s meat when their individual guesses were averaged.\n\n\n\n\n\n\nGalton (1822-1911) was a half-cousin to Charles Darwin and one of the founding fathers of statistics. He also was a scientific racist, see https://twitter.com/kareem_carr/status/1575506343401775104?s=20&t=8T5TzrayAWNShmOSzJgCJQ.."
  },
  {
    "objectID": "W7.html#galtons-data",
    "href": "W7.html#galtons-data",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Galton’s data1",
    "text": "Galton’s data1\nWhat is the weight of the meat of this ox?\n\n\n\n\nlibrary(readxl)\ngalton <- read_excel(\"data/galton_data.xlsx\")\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\") + geom_vline(xintercept = median(galton$Estimate), color = \"blue\") + geom_vline(xintercept = Mode(galton$Estimate), color = \"purple\")\n\n\n\n\n787 estimates, true value 1198, mean 1196.7, median 1208, mode 1218\nKenneth Wallis dug out the data from Galton’s notebook and put it here https://warwick.ac.uk/fac/soc/economics/staff/kfwallis/publications/galton_data.xlsx"
  },
  {
    "objectID": "W7.html#viertelfest-bremen-2008",
    "href": "W7.html#viertelfest-bremen-2008",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Viertelfest Bremen 20081",
    "text": "Viertelfest Bremen 20081\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> ggplot(aes(`Schätzung`)) + geom_histogram() + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\")\n\n\n\n\n1226 estimates, the maximal value is 29530000!\nWe should filter out the highest values for the histogram…\nData collected as additional guessing game at the Lottery “Haste mal ’nen Euro?”, data provided by Jan Lorenz https://docs.google.com/spreadsheets/d/1HiYhUrYrsbeybJ10mwsae_hQCawZlUQFOOZzcugXzgA/edit#gid=0"
  },
  {
    "objectID": "W7.html#viertelfest-bremen-2008-1",
    "href": "W7.html#viertelfest-bremen-2008-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Viertelfest Bremen 2008",
    "text": "Viertelfest Bremen 2008\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\") + geom_vline(xintercept = exp(mean(log(viertel$Schätzung))), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W7.html#log_10-transformation-viertelfest",
    "href": "W7.html#log_10-transformation-viertelfest",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "\\(\\log_{10}\\) transformation Viertelfest",
    "text": "\\(\\log_{10}\\) transformation Viertelfest\n\nviertel |> mutate(log10Est = log10(Schätzung)) |> ggplot(aes(log10Est)) + geom_histogram(binwidth = 0.05) + geom_vline(xintercept = log10(10788), color = \"green\") + \n geom_vline(xintercept = log10(mean(viertel$Schätzung)), color = \"red\") + geom_vline(xintercept = log10(median(viertel$Schätzung)), color = \"blue\") + geom_vline(xintercept = log10(Mode(viertel$Schätzung)), color = \"purple\") + geom_vline(xintercept = mean(log10(viertel$Schätzung)), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W7.html#wisdom-of-the-crowd-insights",
    "href": "W7.html#wisdom-of-the-crowd-insights",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Wisdom of the crowd insights",
    "text": "Wisdom of the crowd insights\n\n\nIn Galton’s sample the different measures do not make a big difference\nIn the Viertelfest data the arithmetic mean performs very bad!\nThe mean is vulnerable to extreme values. Galton on the mean as a democratic aggregation function: “The mean gives voting power to the cranks in proportion to their crankiness.”\nThe mode tends to be on focal values as round numbers (10,000). In Galton’s data this is not so pronounced beause estimators used several units which Galton had to convert.\nHow to choose a measure to aggreagte the wisdom?\n\nBy the nature of the estimate problem? Is the scale mostly clear? (Are we in the hundreds, thousands, ten thousands, …)\nBy the nature of the distribution?\nThere is no real insurance against a systematic bias in the population."
  },
  {
    "objectID": "W7.html#measures-of-dispersion-1",
    "href": "W7.html#measures-of-dispersion-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Measures of dispersion1",
    "text": "Measures of dispersion1\nGoal: We want to measure\n\nhow spread out values are around the central tendency.\nHow stretched or squeezed is the distribution?\n\nVariance is the mean of the squared deviation from the mean: \\(\\text{Var}(x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2\\) where \\(\\mu\\) (mu) is the mean.\nStandard deviation is the square root of the variance \\(\\text{SD}(x) = \\sqrt{\\text{Var}(x)}\\).\nThe standard deviation is often denoted \\(\\sigma\\) (sigma) and the variance \\(\\sigma^2\\).\nMean absolute deviation (MAD) is the mean of the absolute deviation from the mean: \\(\\text{MAD}(x) = \\frac{1}{n}\\sum_{i=1}^n|x_i - \\mu|\\).\nRange is the difference of the maximal and the minimal value \\(\\max(x) - \\min(x)\\).\nAlso called variability, scatter, or spread."
  },
  {
    "objectID": "W7.html#examples-of-measures-of-dispersion",
    "href": "W7.html#examples-of-measures-of-dispersion",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Examples of measures of dispersion",
    "text": "Examples of measures of dispersion\n\n\n\nvar(galton$Estimate)\n\n[1] 5415.013\n\nsd(galton$Estimate)\n\n[1] 73.58677\n\nmad(galton$Estimate)\n\n[1] 51.891\n\nrange(galton$Estimate)\n\n[1]  896 1516\n\ndiff(range(galton$Estimate))\n\n[1] 620\n\n\n\n\nvar(viertel$Schätzung)\n\n[1] 719774887849\n\nsd(viertel$Schätzung)\n\n[1] 848395.5\n\nmad(viertel$Schätzung)\n\n[1] 8771.803\n\nrange(viertel$Schätzung)\n\n[1]      120 29530000\n\ndiff(range(viertel$Schätzung))\n\n[1] 29529880\n\n\n\n\n\n\nVariance (and standard deviation) in statistics is usually computed with \\(\\frac{1}{n-1}\\) instead of \\(\\frac{1}{n}\\) to provide an unbiased estimator of the potentially underlying population variance. We omit more detail here."
  },
  {
    "objectID": "W7.html#standardization",
    "href": "W7.html#standardization",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Standardization",
    "text": "Standardization\nVariables are standardized by subtracting their mean and then dividing by their standard deviations.\nA value from a standardized variable is called a standard score or z-score.\n\\(z_i = \\frac{x_i - \\mu}{\\sigma}\\)\nwhere \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation of the vector \\(x\\).\n\nThis is a shift-scale transformation. We shift by the mean and scale by the standard deviation.\nA standard score \\(z_i\\) shows how mean standard deviations \\(x_i\\) is away from the mean of \\(x\\)."
  },
  {
    "objectID": "W7.html#quantiles",
    "href": "W7.html#quantiles",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Quantiles",
    "text": "Quantiles\nCut points specifying intervals which contain equal amounts of values of the distribution.\n\\(q\\)-quantiles divide into \\(q\\) intervals covering all values.\nThe quantiles are the cut points: For \\(q\\) intervals there are \\(q-1\\) cut points of interest.\n\nThe one 2-quantile is the median.\nThe three 4-quantiles are called quartiles. The second quartile is the median.\n100-quantiles are called percentiles\n\n\n\nWe omit problems of estimating quantiles from a sample where the number of estimates does not fit to a desired partion of equal size here."
  },
  {
    "objectID": "W7.html#examples-of-quantiles",
    "href": "W7.html#examples-of-quantiles",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Examples of quantiles",
    "text": "Examples of quantiles\n\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.25))\n\n    0%    25%    50%    75%   100% \n 896.0 1162.5 1208.0 1236.0 1516.0 \n\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.25))\n\n      0%      25%      50%      75%     100% \n     120     5000     9843    20000 29530000 \n\n\nInterpretation: What does the value at 25% mean?\n\nThe 25% of all values are lower than the value. 75% are larger."
  },
  {
    "objectID": "W7.html#interquartile-range",
    "href": "W7.html#interquartile-range",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Interquartile range",
    "text": "Interquartile range\nThe difference between the 1st and the 3rd quartile.\n\nA very common measure of dispersion.\n\nExamples:\n\n# Min, 3 Quartiles, Max\nIQR(galton$Estimate)\n\n[1] 73.5\n\nsd(galton$Estimate) # for comparison\n\n[1] 73.58677\n\nIQR(viertel$Schätzung)\n\n[1] 15000\n\nsd(viertel$Schätzung)\n\n[1] 848395.5"
  },
  {
    "objectID": "W7.html#summary-of-numerical-vectors-in-r",
    "href": "W7.html#summary-of-numerical-vectors-in-r",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Summary of numerical vectors in R",
    "text": "Summary of numerical vectors in R\n\n\n\nsummary(galton$Estimate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    896    1162    1208    1197    1236    1516 \n\n\n\n\nsummary(viertel$Schätzung)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n     120     5000     9843    53164    20000 29530000 \n\n\n\n\nIt also works for data frames.\n\nlibrary(palmerpenguins)\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2"
  },
  {
    "objectID": "W7.html#boxplots",
    "href": "W7.html#boxplots",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Boxplots",
    "text": "Boxplots\nA condensed visualization of a distribution showing location, spread, skewness and outliers.\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\nThe box shows the median in the middle and the other two quartiles as their borders.\nWhiskers: From above the upper quartile, a distance of 1.5 times the IQR is measured out and a whisker is drawn up to the largest observed data point from the dataset that falls within this distance. Similarly, for the lower quartile.\nWhiskers must end at an observed data point! (So lengths can differ.)\nAll other values outside of box and whiskers are shown as points and often called outliers. (There may be none.)"
  },
  {
    "objectID": "W7.html#boxplots-vs.-histograms",
    "href": "W7.html#boxplots-vs.-histograms",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nHistograms can show the shape of the distribution well, but not the summary statistics like the median.\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_histogram(binwidth = 5)"
  },
  {
    "objectID": "W7.html#boxplots-vs.-histograms-1",
    "href": "W7.html#boxplots-vs.-histograms-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nBoxplots can not show the patterns of bimodal or multimodal distributions.\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_boxplot()\n\n\n\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "W7.html#minimizing-proporties-of-mean-and-median",
    "href": "W7.html#minimizing-proporties-of-mean-and-median",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Minimizing proporties of Mean and Median",
    "text": "Minimizing proporties of Mean and Median\nMean minimizes the mean of squared deviations from it. No other value \\(a\\) has a lower mean of square distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n(x_i - a)^2\\).\nMedian minimizes the sum of the absolute deviation. No other value \\(a\\) has a lower mean of absolute distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n|x_i - a|\\)."
  },
  {
    "objectID": "W7.html#two-families-of-summary-statistics",
    "href": "W7.html#two-families-of-summary-statistics",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Two families of summary statistics",
    "text": "Two families of summary statistics\n\nMeasures based on sums (related to mathematical moments)\n\nMean\nStandard deviation\n\nMeasures based on the ordered sequence of these observations (order statistics)\n\nMedian (and all quantiles)\nInterquartile range"
  },
  {
    "objectID": "W7.html#covariance",
    "href": "W7.html#covariance",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Covariance",
    "text": "Covariance\nGoal: We want to measure the joint variation in a sequences of numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nCovariance \\(\\text{cov}(x,y) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)\\)\nwhere \\(\\mu_x\\) and \\(\\mu_y\\) are the arithmetic means of \\(x\\) and \\(y\\).\nNote: \\(\\text{cov}(x,x) = \\text{Var}(x)\\)"
  },
  {
    "objectID": "W7.html#correlation",
    "href": "W7.html#correlation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation",
    "text": "Correlation\nGoal: We want to measure the linear correlation in a sequences of numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nPearson correlation coefficient \\(r_{xy} = \\frac{\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^n(x_i - \\mu_x)^2}\\sqrt{\\sum_{i=1}^n(y_i - \\mu_y)^2}}\\)\nNote, \\(r_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}\\)\nwhere \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(x\\) and \\(y\\).\nNote, when \\(x\\) and \\(y\\) are standard scores (each with mean zero and standard deviation one), then \\(\\text{cov}(x,y) = r_{xy}\\).\n\n\nThere are other correlation coefficients which we omit hear."
  },
  {
    "objectID": "W7.html#interpretation-of-correlation",
    "href": "W7.html#interpretation-of-correlation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Interpretation of correlation",
    "text": "Interpretation of correlation\nCorrelation between two vectors \\(x\\) and \\(y\\) is “normalized”.\n\nThe maximal possible values is \\(r_{xy} = 1\\)\n\n\\(x\\) and \\(y\\) are fully correlated\n\nThe minimal values is \\(r_{xy} = -1\\)\n\n\\(x\\) and \\(y\\) are anticorrelated\n\nWhen \\(r_{xy} \\approx 0\\) the variables are uncorrelated\n\\(r_{xy} = r_{yx}\\)"
  },
  {
    "objectID": "W7.html#correlation-and-linear-regression",
    "href": "W7.html#correlation-and-linear-regression",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation and linear regression",
    "text": "Correlation and linear regression\n\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n\n\n\npenguins_standardized <- penguins |> \n mutate(flipper_length_mm_s = scale(flipper_length_mm)[,1],\n        body_mass_g_s = scale(body_mass_g)[,1])\nlinear_reg() |> \n set_engine(\"lm\") |> \n fit(flipper_length_mm_s ~ body_mass_g_s, data = penguins_standardized) |> tidy()\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   -9.33e-16    0.0266 -3.51e-14 1.00e+  0\n2 body_mass_g_s  8.71e- 1    0.0266  3.27e+ 1 4.37e-107\n\n\n\ncor(penguins_standardized$body_mass_g_s,\n    penguins_standardized$flipper_length_mm_s, \n    use = \"pairwise.complete.obs\")\n\n[1] 0.8712018"
  },
  {
    "objectID": "W7.html#correlation-and-linear-regression-1",
    "href": "W7.html#correlation-and-linear-regression-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation and linear regression",
    "text": "Correlation and linear regression\nWhen the two variables in the linear regression are standardized (standard scores)\n\nthe intercept is zero\nthe coefficient coincides with the correlation\n\n\n\n\nThe function scale is a function for standardization from base R which delivers a matrix which is unconventional for tidyverse.\nThe function cor is also from base R. It outputs NA whenever one value in one variable is NA. Therefore a methods to use has to be specified. use = \"pairwise.complete.obs\" keeps all values where both variables are not NA."
  },
  {
    "objectID": "W7.html#correlation-in-exploratory-data-analysis",
    "href": "W7.html#correlation-in-exploratory-data-analysis",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation in exploratory data analysis",
    "text": "Correlation in exploratory data analysis\nUsing corrr from tidymodels\n\nlibrary(corrr)\ness <- read_csv(\"data/ESS-Data-Wizard-subset-2022-09-17.csv\") \ness_na <- function(x) ifelse(x > 10, NA, x)\ness_sel <- ess |> select(cntry, essround, euftf:stflife) |> \n mutate(across(euftf:stflife, ess_na))\ness_sel |> select(euftf:stflife) |> \n correlate()\n\n# A tibble: 5 × 6\n  term       euftf  gincdif  lrscale  polintr stflife\n  <chr>      <dbl>    <dbl>    <dbl>    <dbl>   <dbl>\n1 euftf   NA        0.00580 -0.0247  -0.0658   0.0788\n2 gincdif  0.00580 NA        0.153   -0.00153  0.132 \n3 lrscale -0.0247   0.153   NA        0.00466  0.113 \n4 polintr -0.0658  -0.00153  0.00466 NA       -0.120 \n5 stflife  0.0788   0.132    0.113   -0.120   NA"
  },
  {
    "objectID": "W7.html#correlation-in-eda",
    "href": "W7.html#correlation-in-eda",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation in EDA",
    "text": "Correlation in EDA\n\ness_sel |> filter(cntry == \"DE\", essround == 9) |> \n select(euftf:stflife) |> \n correlate()\n\n# A tibble: 5 × 6\n  term      euftf gincdif lrscale polintr stflife\n  <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 euftf   NA      -0.0565 -0.147  -0.136   0.171 \n2 gincdif -0.0565 NA       0.161   0.0671  0.0727\n3 lrscale -0.147   0.161  NA       0.0222  0.0682\n4 polintr -0.136   0.0671  0.0222 NA      -0.104 \n5 stflife  0.171   0.0727  0.0682 -0.104  NA"
  },
  {
    "objectID": "W7.html#correlation-in-eda-1",
    "href": "W7.html#correlation-in-eda-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation in EDA",
    "text": "Correlation in EDA\nUsing correlate and see from easystats\n\nlibrary(correlation)\nlibrary(see)\nresults <- ess_sel |> filter(cntry == \"DE\", essround == 9) |> \n select(euftf:stflife) |> \n correlation()\nresults %>%\n  summary(redundant = TRUE) %>%\n  plot()"
  },
  {
    "objectID": "W7.html#epidemic-modeling",
    "href": "W7.html#epidemic-modeling",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Epidemic Modeling",
    "text": "Epidemic Modeling\n\nAssume a population of N individuals.\n\nIndividuals can have different states, e.g.: Susceptible, Infectious, Recovered, …\nThe population divides into compartments of these states which change over time, e.g.: \\(S(t), I(t), R(t)\\) number of susceptible, infectious, recovered individuals\n\nNow we define dynamics like\n\nwhere the numbers on the arrows represent transition probabilities."
  },
  {
    "objectID": "W7.html#si-model",
    "href": "W7.html#si-model",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI model",
    "text": "SI model\nToday we only treat the SI part of the model.\n\nPeople who are susceptible can become infected through contact with infectious.\nPeople who are infectious stay infectious\n\nThe parameter \\(\\beta\\) is the average number of contacts per unit time multiplied with the probability that an infection happens during such a contact.\nTwo compartments:\n\\(S(t)\\) is the number of susceptible people at time \\(t\\).\n\\(I(t)\\) is the number of infected people at time \\(t\\).\nIt always holds \\(S(t) + I(t) = N\\). (The total population is constant.)"
  },
  {
    "objectID": "W7.html#how-many-infections-per-time",
    "href": "W7.html#how-many-infections-per-time",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "How many infections per time?",
    "text": "How many infections per time?\nThe change of the number of infectious\n\\[\\frac{dI}{dt} = \\underbrace{\\beta}_\\text{infection prob.} \\cdot \\underbrace{\\frac{S}{N}}_\\text{frac. of $S$ still there} \\cdot \\underbrace{\\frac{I}{N}}_\\text{frac. $I$ to meet} \\cdot N = \\frac{\\beta\\cdot S\\cdot I}{N}\\]\nwhere \\(dI\\) is the change of \\(I\\) (the newly infected here) and \\(dt\\) the time interval.\nInterpretation: The newly infected are from the fraction of susceptible times the probability that they meet an infected times the infection probability times the total number of individuals.\nUsing \\(S = N - I\\) we rewrite\n\\[\\frac{dI}{dt} = \\frac{\\beta (N-I)I}{N}\\]"
  },
  {
    "objectID": "W7.html#ordinary-differential-equation",
    "href": "W7.html#ordinary-differential-equation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Ordinary differential equation",
    "text": "Ordinary differential equation\nWe interpret \\(I(t)\\) as a function of time which gives us the number of infectious at each point in time. The change function is now\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nand \\(\\frac{dI(t)}{dt}\\) is also called the derivative of \\(I(t)\\)."
  },
  {
    "objectID": "W7.html#derivatives",
    "href": "W7.html#derivatives",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Derivatives",
    "text": "Derivatives\n\n\n\nThe derivative of a function is also a function with the same domain.\nMeasures the sensitivity to change of the function output when the input changes (a bit)\nExample from physics: The derivative of the position of a moving object is its speed. The derivative of its speed is its acceleration.\nGraphically: The derivative is the slope of a tangent line of the graph of a function."
  },
  {
    "objectID": "W7.html#differentiation",
    "href": "W7.html#differentiation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Differentiation",
    "text": "Differentiation\nis the process to compute the derivative. For parameters \\(a\\) and \\(b\\) and other functions \\(g\\) and \\(h\\), rules of differentiation are\n\n\nFunction \\(f(x)\\)\n\\(a\\cdot x\\)\n\\(b\\)\n\\(x^2,\\ x^{-1} = \\frac{1}{x},\\ x^k\\)\n\\(g(x) + h(x)\\)\n\\(g(x)\\cdot h(x)\\)\n\\(g(h(x))\\)\n\\(e^x,\\ 10^x = e^{\\log(10)x}\\)\n\\(\\log(x)\\)\n\nIts derivative \\(\\frac{df(x)}{dx}\\) or \\(\\frac{d}{dx}f(x)\\) or \\(f'(x)\\)\n\n\\(a\\)\n\n\n\\(0\\)\n\n\n\\(2\\cdot x,\\ -x^{-2} = -\\frac{1}{x^2},\\ k\\cdot x^{k-1}\\)\n\n\n\\(g'(x) + h'(x)\\)\n\n\n\\(g'(x)\\cdot h(x) + g(x)\\cdot h'(x)\\) (product rule)\n\n\n\\(g'(h(x))\\cdot h'(x)\\) (chain rule)\n\n\n\\(e^x,\\ 10^x = \\log(10)\\cdot10^x\\)\n\n\n\\(\\frac{1}{x}\\) (This is a “surprising” relation …)"
  },
  {
    "objectID": "W7.html#differential-equation",
    "href": "W7.html#differential-equation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Differential equation",
    "text": "Differential equation\nIn a differential equation the unknown is a function!\nWe are looking for a function which derivative is a function of the function itself.\nExample: SI-model\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nWhich function \\(I(t)\\) fulfills this equation?\nThe analytical solution1 is\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nWhich is called the logistic equation.\nNote, we need to specify the initial number of infectious individuals \\(I(0)\\).\nCan you check that this is correct? If you want but can’t, don’t hesitate to ask later."
  },
  {
    "objectID": "W7.html#si-model-logistic-equation",
    "href": "W7.html#si-model-logistic-equation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI-model: Logistic Equation",
    "text": "SI-model: Logistic Equation\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nPlot the equation for \\(N = 10000\\), \\(I_0 = 1\\), and \\(\\beta = 0.3\\)\n\nN <- 10000\nI0 <- 1\nbeta <- 0.3\nggplot() + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ) + \n xlim(c(0,75))"
  },
  {
    "objectID": "W7.html#si-model-numerical-integration",
    "href": "W7.html#si-model-numerical-integration",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI-model: Numerical integration",
    "text": "SI-model: Numerical integration\nAnother way of solution using, e.g., using Euler’s method.\nWe compute the solution step-by-step using small increments of, e.g. \\(dt = 0.5\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 0.5 # time increment, supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) # this is the vector of timesteps\nIt <- I0 # this will become the vector of the number infected I(t) over time\nfor (i in 2:length(t)) { # We iterate over the vector of time steps and incrementally compute It\n  It[i] = It[i-1] + dt * dI(It[i-1], N, beta) # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ,\n                color = \"red\") + # Analytical solution for comparison\n geom_line() # The numerical solution in black"
  },
  {
    "objectID": "W7.html#numerical-integration-more-precise-with-small-dt",
    "href": "W7.html#numerical-integration-more-precise-with-small-dt",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Numerical integration more precise with small \\(dt\\)",
    "text": "Numerical integration more precise with small \\(dt\\)\nWe compute the solution step-by-step using small increments of, e.g. \\(dt = 0.01\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 0.01 # time increment, supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) # this is the vector of timesteps\nIt <- I0 # this will become the vector of the number infected I(t) over time\nfor (i in 2:length(t)) { # We iterate over the vector of time steps and incrementally compute It\n  It[i] = It[i-1] + dt * dI(It[i-1], N, beta) # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ,\n                color = \"red\") + # Analytical solution for comparison\n geom_line() # The numerical solution in black"
  },
  {
    "objectID": "W7.html#si-model-simulation",
    "href": "W7.html#si-model-simulation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI-model: Simulation",
    "text": "SI-model: Simulation\nAnother more basic solution is direct individual-based simulation.\n\nWe produce a vector of length \\(N\\) with entries representing the state of each individual as \"S\" or \"I\".\nWe model the random infection process in each step of unit time\n\n\n\nN <- 10000\nbeta <- 0.3\nrandomly_infect <- function(N, prob) { runif(N) < prob }\n# Gives a logical vector of length N where TRUE appears with probability 0.3\ninit <- rep(\"S\",N) # All susceptible\ninit[sample.int(N, size=1)] <- \"I\" # Infect one individual\ntmax <- 75\nsim_run <- list(init)\nfor (i in 2:tmax) {\n contacts <- sample(sim_run[[i-1]], size = N)\n sim_run[[i]] <- if_else(contacts == \"I\" & randomly_infect(N, beta), \n                         true = \"I\", \n                         false = sim_run[[i-1]])\n}\nsim_output <- tibble(t = 0:(tmax-1), \n       # Compute a vector with length tmax with the count of \"I\" in sim_run list\n       infected = map_dbl(sim_run, function(x) sum(x == \"I\"))) \nsim_output |> ggplot(aes(t,infected)) + geom_line()"
  },
  {
    "objectID": "W7.html#questions-on-programming-concepts",
    "href": "W7.html#questions-on-programming-concepts",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Questions on programming concepts?",
    "text": "Questions on programming concepts?\nFrom base R:\nrunif random numbers\nsample.int and sample\nfor loops\nif and else ifelse\nFrom purrr:\nmap apply function to lists and collect output"
  },
  {
    "objectID": "W7.html#three-ways-to-explore-mechanistic-models",
    "href": "W7.html#three-ways-to-explore-mechanistic-models",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Three ways to explore mechanistic models",
    "text": "Three ways to explore mechanistic models\n\nIndividual-based simulation\n\nWe model every individual explicitly\nSimulation involve random numbers! So simulation runs can be different!\n\nNumerical integration of differential equation\n\nNeeds a more abstract concept of compartments\n\nAnalytical solutions of differential equation\n\noften not possible or not in nice form\n\n\nSuch mechanistic models are typical for natural sciences, but they also make sense for many processes in societies and ecomonies."
  },
  {
    "objectID": "W7.html#differentiation-with-data",
    "href": "W7.html#differentiation-with-data",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Differentiation with data",
    "text": "Differentiation with data\nIn empirical data we can compute the increase in a vector with the function diff:\n\nx <- c(1,2,4,5,5,3,0)\ndiff(x)\n\n[1]  1  2  1  0 -2 -3\n\n\nMore convenient for in a data frame is to use x - lag(x) because the vector has the same length.\n\nx - lag(x)\n\n[1] NA  1  2  1  0 -2 -3"
  },
  {
    "objectID": "W7.html#the-diff-of-our-simulation-output",
    "href": "W7.html#the-diff-of-our-simulation-output",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "The diff of our simulation output",
    "text": "The diff of our simulation output\n\ng2 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative_infected))\ng1 <- sim_output |> ggplot(aes(x = t)) + geom_line(aes(y = infected))\ng2\n\n\n\ng1"
  },
  {
    "objectID": "W7.html#nd-derivative-change-of-change",
    "href": "W7.html#nd-derivative-change-of-change",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "2nd derivative: Change of change",
    "text": "2nd derivative: Change of change\n\ng3 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected),\n        derivative2_infected = derivative_infected - lag(derivative_infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative2_infected))\ng3\n\n\nIn empirical data: Derivatives of higher order tend to show fluctuation"
  },
  {
    "objectID": "W7.html#interpretation-in-si-model",
    "href": "W7.html#interpretation-in-si-model",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Interpretation in SI-model",
    "text": "Interpretation in SI-model\n\n\n\\(I(t)\\) total number of infected\n\\(I'(t)\\) number of new cases per day (time step)\n\\(I''(t)\\) how the number of new cases has changes compared to yesterday\n\nCan be a good early indicator for the end of a wave."
  },
  {
    "objectID": "W7.html#integration",
    "href": "W7.html#integration",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Integration",
    "text": "Integration\nThe integral of the daily new cases from the beginning to day \\(s\\) is \\(\\int_{-\\infty}^s f(t)dt\\) and represents the total cases at day \\(s\\).\n\nThe integral of a function \\(f\\) up to time \\(s\\) is also called the anti-derivative \\(F(s) = \\int_{-\\infty}^s f(t)dt\\).\nCompute the anti-derivative of data vector with cumsum.\n\n\nx <- c(1,2,4,5,5,3,0)\ncumsum(x)\n\n[1]  1  3  7 12 17 20 20\n\n\n\nEmpirically derivatives tend to become noisy, while integrals tend to become smooth."
  },
  {
    "objectID": "W7.html#fundamental-theorem-of-calculus",
    "href": "W7.html#fundamental-theorem-of-calculus",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Fundamental theorem of calculus",
    "text": "Fundamental theorem of calculus\nThe integral of the derivative is the function itself.\nThis is not a proof but shows the idea:\n\nf <- c(1,2,4,5,5,3,0)\nantiderivative <- cumsum(f)\ndiff(c(0, antiderivative)) # We have to put 0 before to regain the full vector\n\n[1] 1 2 4 5 5 3 0\n\nderivative <- diff(f)\ncumsum(c(1,derivative)) # We have to put in the first value (here 1) manually because it was lost during the diff\n\n[1] 1 2 4 5 5 3 0"
  },
  {
    "objectID": "W8.html#strange-airports-homework-02",
    "href": "W8.html#strange-airports-homework-02",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Strange Airports (Homework 02)",
    "text": "Strange Airports (Homework 02)\n\nlibrary(nycflights13)\nggplot(data = airports, mapping = aes(x = lon, y = lat)) + geom_point(aes(color = tzone)) \n\n\n\nairports %>% filter(lon >= 0) \n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…"
  },
  {
    "objectID": "W8.html#airport-errors",
    "href": "W8.html#airport-errors",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Airport errors",
    "text": "Airport errors\n\n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…\n\n\nCorrect locations (internet research and location of maps):\n\n\n\nDeer Valley Municipal Airport: Phoenix\n33°41′N 112°05′W Missing minus for lon (W)\nDillant Hopkins Airport: New Hampshire\n42°54′N 72°16′W lon-lat switched, minus (W)\nMontgomery Field: San Diego\n32°44′N 117°11″W Missing minus for lon (W)\nEareckson As: Alaska\n52°42′N 174°06′E No error: Too west,it’s east!"
  },
  {
    "objectID": "W8.html#conclusions-on-data-errors",
    "href": "W8.html#conclusions-on-data-errors",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Conclusions on data errors",
    "text": "Conclusions on data errors\n\nIn real-world datasets errors like the 3 airport are quite common.\nErrors of this type are often hard to detect and remain unnoticed.\n\nThis can (but need not) change results drastically!\n\n\n\nConclusions\n\nAlways remain alert for inconsistencies and be ready to check the plausibility of results.\nSkills in exploratory data analysis (EDA) are essential to find errors and explore their nature and implication\nErrors are unpredictable, of diverse types, and often deeply related to the reality the data presents.\n\nOne reason why EDA can not be a fully formalized and automatized process."
  },
  {
    "objectID": "W8.html#same-phenomenon-different-data",
    "href": "W8.html#same-phenomenon-different-data",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Same phenomenon different data?",
    "text": "Same phenomenon different data?\nQuestion: Is the data of OWiD and WHO the same?\n\n\n\nNo.\nWhy?\nWhat are the data sources of WHO and OWiD?\n\n\n\n\n\n\n\n\n\nOWiD documentation refers to have data from CSSE at Johns Hopkins University which document various data sources (e.g., a newspaper from Germany)\nWHO documentation says: “WHO collected the numbers of confirmed COVID-19 cases and deaths through official communications under the International Health Regulations (IHR, 2005), complemented by monitoring the official ministries of health websites and social media accounts.” For Germany this is the Robert-Koch-Institut RKI."
  },
  {
    "objectID": "W8.html#good-reasons-for-different-data",
    "href": "W8.html#good-reasons-for-different-data",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Good reasons for different data?",
    "text": "Good reasons for different data?\n\nDuring the pandemic daily new case numbers were relevant for decisions about safety measures.\nIn reality, data comes with delays.\n\nExample: Recent new cases in Germany (RKI). Notice many new cases several days ago."
  },
  {
    "objectID": "W8.html#conflict-day-to-day-consistency-and-correctness",
    "href": "W8.html#conflict-day-to-day-consistency-and-correctness",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Conflict day-to-day consistency and correctness",
    "text": "Conflict day-to-day consistency and correctness\n\nFixing daily cases is useful to record the numbers on which daily safety decisions are based.\nCorrected cases (which also change data from the past) are better for analysis in retrospect. It reflects the actual pandemic better."
  },
  {
    "objectID": "W8.html#reported-cases-and-real-cases",
    "href": "W8.html#reported-cases-and-real-cases",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Reported cases and real cases?",
    "text": "Reported cases and real cases?\nCase numbers are to inform us about real cases. What type of data analysis question is this? (Descriptive, Exploratory, Inferential, Predictive, Causal, Mechanistic)\n\n\nInferential: “Quantify whether the discovery is likely to hold in a new sample.”\n\nHere: What do reported cases tell us about cases in the whole population?\n\nLimitations\n\nWe cannot test all\nTests are not on a random sample\nMild/asymptomatic cases remain unnoticed even to individuals\n…\n\n\nThe unknown: What is the dark figure?"
  },
  {
    "objectID": "W8.html#excercise-for-german-new-case-counts",
    "href": "W8.html#excercise-for-german-new-case-counts",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Excercise for German new case counts",
    "text": "Excercise for German new case counts\n\n\nCan we infer the real incidence (= new cases per 100,000)?\nWhat can we infer the trend of the real incidence?\n\n\nIncidence: Not really, we would need a either a random sample (then we can infer the fraction of infected), or an idea how to estimate the dark figure.\nTrend: Yes! Under the assumptions that reported cases do reflect a relevant part of the pandemic and the limitation remain mostly constant during the observed trend."
  },
  {
    "objectID": "W8.html#smoothing-time-series",
    "href": "W8.html#smoothing-time-series",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Smoothing time series",
    "text": "Smoothing time series\n\n\n\nx <- c(1, 2, 5, 3, 0)\nx\n\n\n[1] 1 2 5 3 0\n\n\n\n\n\nzoo::rollmean(x, k = 3, na.pad = TRUE) # for centered window\n\n\n[1]       NA 2.666667 3.333333 2.666667       NA\n\n\n\n\n\n(x + lag(x, n = 1) + lag(x, n = 2))/3 # for lagged window\n\n\n[1]       NA       NA 2.666667 3.333333 2.666667\n\n\n\n\nCentered: Leaves smoothed data close to real data.\nLagged: Lags the smoothed data, but can be consistently computed for the newest day\nRemember: Data with weekly seasonality is best smoothed with a weekly window!"
  },
  {
    "objectID": "W8.html#total-death-per-million-and-human-development",
    "href": "W8.html#total-death-per-million-and-human-development",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Total death per million and human development",
    "text": "Total death per million and human development\n\nWhat findings? Explanations?"
  },
  {
    "objectID": "W8.html#level-of-measurement",
    "href": "W8.html#level-of-measurement",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Level of measurement",
    "text": "Level of measurement\n\nNominal: the data can only be categorized\nOrdinal: the data can be categorized and ranked\nInterval: the data can be categorized, ranked, and evenly spaced\nRatio: the data can be categorized, ranked, evenly spaced, and has a natural zero.\n\nWhat is the difference of level of measurement and data type?\n\nMainly perspective:\n\nLevel of measurement is about the variable/the thing which is measured.\nData type is more about the technical way to store data."
  },
  {
    "objectID": "W8.html#scales-in-surveysquestionaires",
    "href": "W8.html#scales-in-surveysquestionaires",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Scales in surveys/questionaires",
    "text": "Scales in surveys/questionaires\nLikert scale: Strongly disagree … [scale steps] … Strongly agree\nRating scales: Extreme statement … [scale steps] … Opposite statement\nWhat level of measurement do these questions have?\n \n\nOrdinal clearly, interval assuming scale steps are equal, ratio assuming 5 as natural zero"
  },
  {
    "objectID": "W8.html#dealing-with-missing-values",
    "href": "W8.html#dealing-with-missing-values",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nCoding in the ESS data:\n\nInterval data coded numerically 0, 1, …, 10\nMissing values with numerical codes 77, 88, 99\n\nWhat is the reason for missing data? This can be important for inferential questions!\n\n\n\n\nFor numerical computations these must be filtered out or coded as NA!\n\ness |> select(euftf) |> \n mutate(euftf_na = euftf  |> na_if(77) |> na_if(88) |> na_if(99)) |> \n summarize(across(.fns = function(x) mean(x, na.rm = TRUE)))\n\n# A tibble: 1 × 2\n  euftf euftf_na\n  <dbl>    <dbl>\n1  13.7     5.20"
  },
  {
    "objectID": "W8.html#emotional-attachment",
    "href": "W8.html#emotional-attachment",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Emotional attachment",
    "text": "Emotional attachment\nQuestion: What is the relation of the emotional attachment of Europeans to their own country and to Europe?\n\n\ness |> \n filter(essround == 9) |> \n count(atchctr, atcherp) |> \n na.omit() |> \n ggplot(aes(atchctr, atcherp, \n            size = n, color = n)) + \n geom_point() +\n geom_smooth(aes(weight = n), \n             method = 'loess', \n             formula = 'y ~ x') +\n scale_color_continuous(type = \"viridis\") +\n scale_size_area(max_size = 10) + ylim(c(0,10)) +\n coord_fixed() + \n guides(size = \"none\") +\n theme_classic(base_size = 24)\n\n\n\n\n\n\n\n\nDifferences between lowess used in seaborn.regplot and loess, the default in ggplot::geom_smooth: https://stats.stackexchange.com/questions/161069/difference-between-loess-and-lowess"
  },
  {
    "objectID": "W8.html#emotional-attachment-eu-integration",
    "href": "W8.html#emotional-attachment-eu-integration",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Emotional attachment EU integration",
    "text": "Emotional attachment EU integration\nQuestion: What is the relation of the emotional attachment to the own country to attachment to Europe compared to the attitude about European integration?\n\n\nEmotional attachment to the own country and Europe is positively related. (“Positive” here means the sign of the correlation. It does not mean “good”!)\nNo compensation like “Emotion must be split between both.”\nRelation of country attachment to EU integration is weak but non-linear."
  },
  {
    "objectID": "W8.html#weighting-after-count",
    "href": "W8.html#weighting-after-count",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Weighting after count",
    "text": "Weighting after count\nHow many rows does the the data frame ess |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() have?\n\n11 times 11 = 121 (when each combination has a non-zero number of cases)\nHow many observations (not NA) do we have in the data set?\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> summarize(n = sum(n))\n\n# A tibble: 1 × 1\n      n\n  <int>\n1 45552\n\n\nWeighting points correctly is important!"
  },
  {
    "objectID": "W8.html#three-different-smooth-plots",
    "href": "W8.html#three-different-smooth-plots",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Three different smooth plots",
    "text": "Three different smooth plots\n\nA: Counts unweightedB: Counts weightedC: Individual cases\n\n\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> \n ggplot(aes(atchctr, euftf)) + geom_smooth() \n\n\n\n\nMakes no sense because, just 121 points in a square.\n\n\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> \n ggplot(aes(atchctr, euftf, weight = n)) + geom_smooth() \n\n\n\n\nMakes sense weighted by the counts.\n\n\n\ness |> filter(essround == 9) |> \n ggplot(aes(atchctr, euftf)) + geom_smooth()\n\n\n\n\nSimilar to B. Lower uncertainty! Different y-axis limits!\nNote, geom_smooth uses stats::loess for less than 1,000 cases (as before), otherwise (as here) mgcv::gam() because it is more efficient computationally. We omit details here."
  },
  {
    "objectID": "W8.html#significance-of-nonlinear-relation",
    "href": "W8.html#significance-of-nonlinear-relation",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Significance of nonlinear relation?",
    "text": "Significance of nonlinear relation?\n\n\nTaking into account the real number of cases the uncertainty range indicates that the non-linear relationship is fairly certain, although small in magnitude.\nNote, we have not looked at uncertainty measures in detail yet.\nMain message here: Low uncertainty and large effect size is not the same!\n\nIn statistics the first is called significant.\nIn common language the second is often called significant.\n\nThis dual use of significant is a source of confusion in science communication!"
  },
  {
    "objectID": "W8.html#linear-models",
    "href": "W8.html#linear-models",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear models",
    "text": "Linear models\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n fit(atcherp ~ atchctr, data = ess) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    2.62    0.0411       63.8       0\n2 atchctr        0.414   0.00504      82.1       0\n\n\nInterpretation?\n\nWhen atchctr = 0 the average atcherp is 2.62. For an increase of country attachment by one there is an average increase of 0.414 in European attachment.\n\n\nFor EU integration attitude.\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(euftf ~ atchctr, data = ess) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)   5.00     0.0481     104.   0           \n2 atchctr       0.0333   0.00589      5.65 0.0000000157"
  },
  {
    "objectID": "W8.html#r-squared-of-a-fitted-model",
    "href": "W8.html#r-squared-of-a-fitted-model",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "R-squared of a fitted model",
    "text": "R-squared of a fitted model\n\\(R^2\\) is the percentage of variability in the response explained by the regression model.\nR-squared is also called coefficient of determination.\nDefinition:\n\\(R^2 = 1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}\\)\nwhere \\(SS_\\text{res} = \\sum_i(y_i - f_i)^2 = \\sum_i e_i^2\\) is the sum of the squared residuals, and\n\\(SS_\\text{tot} = \\sum_i(y_i - \\bar y)^2\\) the total sum of squares which is proportional to the variance of \\(y\\). (\\(\\bar y\\) is the mean of \\(y\\).)\n\\(R^2\\) is the square of the correlation coefficient, hence the name. (No math on this today.)"
  },
  {
    "objectID": "W8.html#linear-models-r-squared",
    "href": "W8.html#linear-models-r-squared",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear models R-squared",
    "text": "Linear models R-squared\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n fit(atcherp ~ atchctr, data = ess) |>\n glance()  # glance shows summary statistics of model fit\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³\n      <dbl>      <dbl> <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n1     0.122      0.122  2.45   6747.       0     1 -1.12e5 2.25e5 2.25e5 291696.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nInterpretation R-square?\n\n12.2% of the variance of European emotional attachment can be explained by a linear relation with country emotional attachment.\n\n\nFor EU integration attitude.\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(euftf ~ atchctr, data = ess) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³\n      <dbl>      <dbl> <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n1  0.000702   0.000680  2.75    32.0 1.57e-8     1 -1.11e5 2.21e5 2.21e5 343473.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance"
  },
  {
    "objectID": "W8.html#linear-model-with-more-predictors",
    "href": "W8.html#linear-model-with-more-predictors",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear model with more predictors",
    "text": "Linear model with more predictors\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(euftf ~ atchctr + atcherp, data = ess) |> tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    4.05    0.0478       84.8 0       \n2 atchctr       -0.114   0.00599     -19.1 1.12e-80\n3 atcherp        0.354   0.00509      69.7 0       \n\n\n\nNote, that atchctr now has a negative coefficient!\nThe tiny bit of positive relation explained by atchctr in a one predictor model can better be explained by atcherp (which we know is correlated with atchctr)."
  },
  {
    "objectID": "W8.html#corona-deaths-vs.-human-development",
    "href": "W8.html#corona-deaths-vs.-human-development",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Corona deaths vs. Human development",
    "text": "Corona deaths vs. Human development\n\nNote: Not weighted by population!"
  },
  {
    "objectID": "W8.html#linear-model-total-deaths-vs.-hdi",
    "href": "W8.html#linear-model-total-deaths-vs.-hdi",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear model: Total deaths vs. HDI",
    "text": "Linear model: Total deaths vs. HDI\n\nowid_aug22 <- owid |> \n filter(date == \"2022-08-31\", !is.na(human_development_index), \n        !is.na(total_deaths_per_million), !is.na(continent))\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index, data = owid_aug22) |> tidy()\n\n# A tibble: 2 × 5\n  term                    estimate std.error statistic  p.value\n  <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)               -2350.      385.     -6.11 5.84e- 9\n2 human_development_index    4917.      522.      9.43 1.71e-17"
  },
  {
    "objectID": "W8.html#adding-a-main-effect-of-continents",
    "href": "W8.html#adding-a-main-effect-of-continents",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Adding a main effect of continents",
    "text": "Adding a main effect of continents\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index + continent, \n        data = owid_aug22) |> tidy()\n\n# A tibble: 7 × 5\n  term                    estimate std.error statistic       p.value\n  <chr>                      <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)               -754.       392.    -1.92  0.0560       \n2 human_development_index   1901.       666.     2.86  0.00480      \n3 continentAsia               58.5      213.     0.274 0.784        \n4 continentEurope           1685.       279.     6.04  0.00000000867\n5 continentNorth America     742.       254.     2.92  0.00397      \n6 continentOceania          -312.       298.    -1.05  0.297        \n7 continentSouth America    1910.       311.     6.15  0.00000000489\n\n\nA main effect by categorical dummy variables allows for different intercepts per continent."
  },
  {
    "objectID": "W8.html#adding-as-interaction",
    "href": "W8.html#adding-as-interaction",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Adding as interaction",
    "text": "Adding as interaction\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index * continent, \n        data = owid_aug22) |> tidy()\n\n# A tibble: 12 × 5\n   term                                         estim…¹ std.e…² stati…³  p.value\n   <chr>                                          <dbl>   <dbl>   <dbl>    <dbl>\n 1 (Intercept)                                   -1555.    581.  -2.67  8.19e- 3\n 2 human_development_index                        3329.   1019.   3.27  1.31e- 3\n 3 continentAsia                                   563.    940.   0.598 5.50e- 1\n 4 continentEurope                               14759.   1956.   7.55  2.32e-12\n 5 continentNorth America                        -1346.   1512.  -0.890 3.75e- 1\n 6 continentOceania                               1125.   1424.   0.790 4.31e- 1\n 7 continentSouth America                        -4243.   3436.  -1.23  2.19e- 1\n 8 human_development_index:continentAsia         -1027.   1419.  -0.724 4.70e- 1\n 9 human_development_index:continentEurope      -15377.   2351.  -6.54  6.40e-10\n10 human_development_index:continentNorth Amer…   2394.   2099.   1.14  2.56e- 1\n11 human_development_index:continentOceania      -2318.   2063.  -1.12  2.63e- 1\n12 human_development_index:continentSouth Amer…   7684.   4543.   1.69  9.25e- 2\n# … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic\n\n\n\nNote the * for interaction effect!\nAlso main effects for both variables are in as coefficients.\nAfrica has been chosen as reference category (because it is first in the alphabet).\nAn interaction effect allows for different slopes for each continent!"
  },
  {
    "objectID": "W8.html#regression-lines-by-continent",
    "href": "W8.html#regression-lines-by-continent",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Regression lines by continent",
    "text": "Regression lines by continent\n\nThe relation between deaths and human development is reverse in Europe."
  },
  {
    "objectID": "W8.html#simpsons-paradox",
    "href": "W8.html#simpsons-paradox",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nSlopes for all groups can be in the opposite direction of the main effect’s slope!\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W5.html#named-vectors",
    "href": "W5.html#named-vectors",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Named vectors",
    "text": "Named vectors\nAll types of vectors can be named upon creation\n\nc(Num1 = 4, Second = 7, Last = 8)\n\n\n\n  Num1 Second   Last \n     4      7      8 \n\n\n\nor names can be set afterward.\n\nx <- 1:4\ny <- set_names(x, c(\"a\",\"b\",\"c\",\"d\"))\ny\n\n\n\na b c d \n1 2 3 4 \n\n\n\n\nNamed vectors can be used for subsetting.\n\ny[c(\"b\",\"d\")]\n\n\n\nb d \n2 4"
  },
  {
    "objectID": "W5.html#reminder-indexing-and-vectorized-thinking",
    "href": "W5.html#reminder-indexing-and-vectorized-thinking",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Reminder: Indexing and vectorized thinking",
    "text": "Reminder: Indexing and vectorized thinking\n\nx <- set_names(1:10,LETTERS[1:10])\nx\n\n\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\n\n\n\nx[c(4,2,1,1,1,1,4,1,5)]\n\n\n\nD B A A A A D A E \n4 2 1 1 1 1 4 1 5 \n\n\n\n\nRemoving with negative index numbers.\n\nx[c(-3,-5,-2)]\n\n\n\n A  D  F  G  H  I  J \n 1  4  6  7  8  9 10 \n\n\n\n\nMixing does not work.\nx[c(-3,1)]  # Will throw an error"
  },
  {
    "objectID": "W5.html#r-objects-can-have-attributes",
    "href": "W5.html#r-objects-can-have-attributes",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "R objects can have attributes",
    "text": "R objects can have attributes\nIn a named vector, the names are an attribute.\n\nx\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\nattributes(x)\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n\n\nAttributes can be assigned freely.\n\nattr(x, \"SayHi\") <- \"Hi\"\nattr(x, \"SayBye\") <- \"Bye\"\nattributes(x)\n\n\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n$SayHi\n[1] \"Hi\"\n\n$SayBye\n[1] \"Bye\""
  },
  {
    "objectID": "W5.html#attributes-in-data-structures",
    "href": "W5.html#attributes-in-data-structures",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Attributes in data structures",
    "text": "Attributes in data structures\n\nlibrary(nycflights13)\nattributes(airports)\n\n\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n   [1]    1    2    3    4    5    6    7    8    9   10   11   12   13   14\n  [15]   15   16   17   18   19   20   21   22   23   24   25   26   27   28\n  [29]   29   30   31   32   33   34   35   36   37   38   39   40   41   42\n  [43]   43   44   45   46   47   48   49   50   51   52   53   54   55   56\n  [57]   57   58   59   60   61   62   63   64   65   66   67   68   69   70\n  [71]   71   72   73   74   75   76   77   78   79   80   81   82   83   84\n  [85]   85   86   87   88   89   90   91   92   93   94   95   96   97   98\n  [99]   99  100  101  102  103  104  105  106  107  108  109  110  111  112\n [113]  113  114  115  116  117  118  119  120  121  122  123  124  125  126\n [127]  127  128  129  130  131  132  133  134  135  136  137  138  139  140\n [141]  141  142  143  144  145  146  147  148  149  150  151  152  153  154\n [155]  155  156  157  158  159  160  161  162  163  164  165  166  167  168\n [169]  169  170  171  172  173  174  175  176  177  178  179  180  181  182\n [183]  183  184  185  186  187  188  189  190  191  192  193  194  195  196\n [197]  197  198  199  200  201  202  203  204  205  206  207  208  209  210\n [211]  211  212  213  214  215  216  217  218  219  220  221  222  223  224\n [225]  225  226  227  228  229  230  231  232  233  234  235  236  237  238\n [239]  239  240  241  242  243  244  245  246  247  248  249  250  251  252\n [253]  253  254  255  256  257  258  259  260  261  262  263  264  265  266\n [267]  267  268  269  270  271  272  273  274  275  276  277  278  279  280\n [281]  281  282  283  284  285  286  287  288  289  290  291  292  293  294\n [295]  295  296  297  298  299  300  301  302  303  304  305  306  307  308\n [309]  309  310  311  312  313  314  315  316  317  318  319  320  321  322\n [323]  323  324  325  326  327  328  329  330  331  332  333  334  335  336\n [337]  337  338  339  340  341  342  343  344  345  346  347  348  349  350\n [351]  351  352  353  354  355  356  357  358  359  360  361  362  363  364\n [365]  365  366  367  368  369  370  371  372  373  374  375  376  377  378\n [379]  379  380  381  382  383  384  385  386  387  388  389  390  391  392\n [393]  393  394  395  396  397  398  399  400  401  402  403  404  405  406\n [407]  407  408  409  410  411  412  413  414  415  416  417  418  419  420\n [421]  421  422  423  424  425  426  427  428  429  430  431  432  433  434\n [435]  435  436  437  438  439  440  441  442  443  444  445  446  447  448\n [449]  449  450  451  452  453  454  455  456  457  458  459  460  461  462\n [463]  463  464  465  466  467  468  469  470  471  472  473  474  475  476\n [477]  477  478  479  480  481  482  483  484  485  486  487  488  489  490\n [491]  491  492  493  494  495  496  497  498  499  500  501  502  503  504\n [505]  505  506  507  508  509  510  511  512  513  514  515  516  517  518\n [519]  519  520  521  522  523  524  525  526  527  528  529  530  531  532\n [533]  533  534  535  536  537  538  539  540  541  542  543  544  545  546\n [547]  547  548  549  550  551  552  553  554  555  556  557  558  559  560\n [561]  561  562  563  564  565  566  567  568  569  570  571  572  573  574\n [575]  575  576  577  578  579  580  581  582  583  584  585  586  587  588\n [589]  589  590  591  592  593  594  595  596  597  598  599  600  601  602\n [603]  603  604  605  606  607  608  609  610  611  612  613  614  615  616\n [617]  617  618  619  620  621  622  623  624  625  626  627  628  629  630\n [631]  631  632  633  634  635  636  637  638  639  640  641  642  643  644\n [645]  645  646  647  648  649  650  651  652  653  654  655  656  657  658\n [659]  659  660  661  662  663  664  665  666  667  668  669  670  671  672\n [673]  673  674  675  676  677  678  679  680  681  682  683  684  685  686\n [687]  687  688  689  690  691  692  693  694  695  696  697  698  699  700\n [701]  701  702  703  704  705  706  707  708  709  710  711  712  713  714\n [715]  715  716  717  718  719  720  721  722  723  724  725  726  727  728\n [729]  729  730  731  732  733  734  735  736  737  738  739  740  741  742\n [743]  743  744  745  746  747  748  749  750  751  752  753  754  755  756\n [757]  757  758  759  760  761  762  763  764  765  766  767  768  769  770\n [771]  771  772  773  774  775  776  777  778  779  780  781  782  783  784\n [785]  785  786  787  788  789  790  791  792  793  794  795  796  797  798\n [799]  799  800  801  802  803  804  805  806  807  808  809  810  811  812\n [813]  813  814  815  816  817  818  819  820  821  822  823  824  825  826\n [827]  827  828  829  830  831  832  833  834  835  836  837  838  839  840\n [841]  841  842  843  844  845  846  847  848  849  850  851  852  853  854\n [855]  855  856  857  858  859  860  861  862  863  864  865  866  867  868\n [869]  869  870  871  872  873  874  875  876  877  878  879  880  881  882\n [883]  883  884  885  886  887  888  889  890  891  892  893  894  895  896\n [897]  897  898  899  900  901  902  903  904  905  906  907  908  909  910\n [911]  911  912  913  914  915  916  917  918  919  920  921  922  923  924\n [925]  925  926  927  928  929  930  931  932  933  934  935  936  937  938\n [939]  939  940  941  942  943  944  945  946  947  948  949  950  951  952\n [953]  953  954  955  956  957  958  959  960  961  962  963  964  965  966\n [967]  967  968  969  970  971  972  973  974  975  976  977  978  979  980\n [981]  981  982  983  984  985  986  987  988  989  990  991  992  993  994\n [995]  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008\n[1009] 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022\n[1023] 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036\n[1037] 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050\n[1051] 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064\n[1065] 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078\n[1079] 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092\n[1093] 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106\n[1107] 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120\n[1121] 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134\n[1135] 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148\n[1149] 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162\n[1163] 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176\n[1177] 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190\n[1191] 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204\n[1205] 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218\n[1219] 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232\n[1233] 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246\n[1247] 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260\n[1261] 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274\n[1275] 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288\n[1289] 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302\n[1303] 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316\n[1317] 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330\n[1331] 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344\n[1345] 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358\n[1359] 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372\n[1373] 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386\n[1387] 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400\n[1401] 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414\n[1415] 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428\n[1429] 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442\n[1443] 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456\n[1457] 1457 1458\n\n$spec\ncols(\n  id = col_double(),\n  name = col_character(),\n  city = col_character(),\n  country = col_character(),\n  faa = col_character(),\n  icao = col_character(),\n  lat = col_double(),\n  lon = col_double(),\n  alt = col_double(),\n  tz = col_double(),\n  dst = col_character(),\n  tzone = col_character()\n)\n\n$names\n[1] \"faa\"   \"name\"  \"lat\"   \"lon\"   \"alt\"   \"tz\"    \"dst\"   \"tzone\""
  },
  {
    "objectID": "W5.html#three-important-attributes",
    "href": "W5.html#three-important-attributes",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Three important attributes",
    "text": "Three important attributes\n\nNames are used to name element of a vector (counting also lists as vectors and therefore also data frames as lists of atomic vectors of the same length)\nDimensions (dim()) is a short numeric vector making a vector behave as a matrix or a higher dimensional array. A vector 1:6 together with dim being c(2,3) is a matrix with 2 rows and 3 columns\n\\(\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}\\)\nClass is used to implement the S3 object oriented system. We don’t need to know the details here. The class system makes it for example possible that the same function, e.g. print() behaves differently for objects of different a different class.\n\nClass plays a role in specifying augmented vectors like factors, dates, date-times, or tibbles."
  },
  {
    "objectID": "W5.html#factors",
    "href": "W5.html#factors",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Factors",
    "text": "Factors\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values\n\nx <- factor(c(\"BS\", \"MS\", \"PhD\", \"MS\", \"BS\", \"BS\"))\nx\n\n\n\n[1] BS  MS  PhD MS  BS  BS \nLevels: BS MS PhD\n\n\n\nTechnically, a factor is vector of integers with a levels attribute which specifies the categories for the integers.\n\ntypeof(x)\n\n[1] \"integer\"\n\nas.integer(x)\n\n[1] 1 2 3 2 1 1\n\nattributes(x)\n\n$levels\n[1] \"BS\"  \"MS\"  \"PhD\"\n\n$class\n[1] \"factor\"\n\n\n\n\nThe class factor makes R print the level of each element of the vector instead of the underlying integer."
  },
  {
    "objectID": "W5.html#factors-for-data-visualization",
    "href": "W5.html#factors-for-data-visualization",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Factors for data visualization",
    "text": "Factors for data visualization\nWe manipulate factors with functions from the forcats package of the tidyverse core.\n\nPlotReverseOrder by frequencyRegroup\n\n\n\nmpg |> ggplot(aes(y = manufacturer)) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(manufacturer))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(fct_infreq(manufacturer)))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_other(manufacturer, keep = c(\"dodge\", \"toyota\", \"volkswagen\")))) + geom_bar()"
  },
  {
    "objectID": "W5.html#dates",
    "href": "W5.html#dates",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Dates",
    "text": "Dates\n\nISO 8601 standard for dates: YYYY-MM-DD. Today: 2022-10-25.\nDates in R are numeric vectors that represent the number of days since 1 January 1970.\n\n\ny <- as.Date(\"2020-01-01\"); y\n\n[1] \"2020-01-01\"\n\ntypeof(y)\n\n[1] \"double\"\n\nattributes(y)\n\n$class\n[1] \"Date\"\n\nas.double(y)\n\n[1] 18262\n\nas.double(as.Date(\"1970-01-01\"))\n\n[1] 0\n\nas.double(as.Date(\"1969-01-01\"))\n\n[1] -365"
  },
  {
    "objectID": "W5.html#how-many-days-are-you-old",
    "href": "W5.html#how-many-days-are-you-old",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "How many days are you old?",
    "text": "How many days are you old?\n\n\nSys.Date() - as.Date(\"1976-01-16\")  # Sys.Date() gives as the current day your computer is set to\n\nTime difference of 17084 days"
  },
  {
    "objectID": "W5.html#date-times",
    "href": "W5.html#date-times",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Date-times",
    "text": "Date-times\nFor date-time manipulation use lubridate form the tidyverse. Not in the core so it has to be loaded.1\n\nx <- lubridate::ymd_hm(\"1970-01-01 01:00\")\nx\n\n[1] \"1970-01-01 01:00:00 UTC\"\n\nattributes(x)\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"UTC\"\n\nas.double(x)\n\n[1] 3600\n\n\nUTC: Coordinated Universal Time. We are in the UTC+1 timezone.\nPOSIXct: Portable Operating System Interface, calendar time. Stores date and time in seconds with the number of seconds beginning at 1 January 1970.\nInstead of loading package pack to use its function func you can also write pack::func all the time. This works when the package is installed even when not loaded."
  },
  {
    "objectID": "W5.html#how-many-seconds-are-you-old",
    "href": "W5.html#how-many-seconds-are-you-old",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "How many seconds are you old?",
    "text": "How many seconds are you old?\n\nas.double(lubridate::now()) - as.double(lubridate::ymd_hm(\"1976-01-16_12:04\"))\n\n[1] 1476061826"
  },
  {
    "objectID": "W5.html#more-about",
    "href": "W5.html#more-about",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "More about …",
    "text": "More about …\n\nFactors: R for Data Science Chapter 15\nDates and times: R for Data Science Chapter 16"
  },
  {
    "objectID": "W5.html#string-modification",
    "href": "W5.html#string-modification",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "String modification",
    "text": "String modification\nWe modify strings with the stringr package from the tidyverse core.\nAll functions from stringr start with str_.\nVery few examples:\n\nc(\"x\",\"y\")\n\n[1] \"x\" \"y\"\n\nstr_c(\"x\",\"y\")\n\n[1] \"xy\"\n\nstr_c(\"x\",\"y\",\"z\", sep=\",\")\n\n[1] \"x,y,z\"\n\nlength(c(\"x\",\"y\",\"z\"))\n\n[1] 3\n\nstr_length(c(\"x\",\"y\",\"z\"))\n\n[1] 1 1 1\n\nstr_length(c(\"This is a string.\",\"z\"))\n\n[1] 17  1"
  },
  {
    "objectID": "W5.html#string-wrangling-with-variable-names",
    "href": "W5.html#string-wrangling-with-variable-names",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "String wrangling with variable names",
    "text": "String wrangling with variable names\n\ndata <- tibble(Name = c(\"A\",\"B\",\"C\"), Age_2020 = c(20,30,40), Age_2021 = c(21,31,41), Age_2022 = c(22,32,42))\ndata\n\n# A tibble: 3 × 4\n  Name  Age_2020 Age_2021 Age_2022\n  <chr>    <dbl>    <dbl>    <dbl>\n1 A           20       21       22\n2 B           30       31       32\n3 C           40       41       42\n\n\nWe tidy that data set by creating a year variable.\n\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\")\n\n\n\n# A tibble: 9 × 3\n  Name  Year       Age\n  <chr> <chr>    <dbl>\n1 A     Age_2020    20\n2 A     Age_2021    21\n3 A     Age_2022    22\n4 B     Age_2020    30\n5 B     Age_2021    31\n6 B     Age_2022    32\n7 C     Age_2020    40\n8 C     Age_2021    41\n9 C     Age_2022    42\n\n\n\n\nOK, but the year variable is a string but we want numbers."
  },
  {
    "objectID": "W5.html#use-word",
    "href": "W5.html#use-word",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Use word",
    "text": "Use word\nword extracts words from a sentence. However, the separator need not be \" \" but can be any character.\n\nword(\"This is a string.\", start=2, end=-2) \n\n[1] \"is a\"\n\n#Selects from the second to the second last word.\nword(\"Age_2022\", start=2, sep = \"_\")\n\n[1] \"2022\"\n\n\n\nIt also works vectorized.\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\") |> \n  mutate(Year = word(Year, start = 2, sep = \"_\") |> as.numeric())\n\n\n\n# A tibble: 9 × 3\n  Name   Year   Age\n  <chr> <dbl> <dbl>\n1 A      2020    20\n2 A      2021    21\n3 A      2022    22\n4 B      2020    30\n5 B      2021    31\n6 B      2022    32\n7 C      2020    40\n8 C      2021    41\n9 C      2022    42\n\n\n… More on strings and regular expressions: R for Data Science Chapter 14"
  },
  {
    "objectID": "W5.html#working-with-more-data-frames",
    "href": "W5.html#working-with-more-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Working with more data frames",
    "text": "Working with more data frames\n\nData can be distributed in several data frames which have relations which each other.\nFor example, they share variables as the five data frames in nycflights13.\n\n\n\n\nOften variables in different data frame have the same name, but that need not be the case! See the variable faa in airports matches origin and dest in flights."
  },
  {
    "objectID": "W5.html#data-women-in-science",
    "href": "W5.html#data-women-in-science",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Data: Women in science",
    "text": "Data: Women in science\n10 women in science who changed the world: Ada Lovelace, Marie Curie, Janaki Ammal, Chien-Shiung Wu, Katherine Johnson, Rosalind Franklin, Vera Rubin, Gladys West, Flossie Wong-Staal, Jennifer Doudna\n\n\n\n\nProfessionsDatesWorks\n\n\n\nprofessions <- read_csv(\"data/scientists/professions.csv\")\nprofessions\n\n# A tibble: 10 × 2\n   name               profession                        \n   <chr>              <chr>                             \n 1 Ada Lovelace       Mathematician                     \n 2 Marie Curie        Physicist and Chemist             \n 3 Janaki Ammal       Botanist                          \n 4 Chien-Shiung Wu    Physicist                         \n 5 Katherine Johnson  Mathematician                     \n 6 Rosalind Franklin  Chemist                           \n 7 Vera Rubin         Astronomer                        \n 8 Gladys West        Mathematician                     \n 9 Flossie Wong-Staal Virologist and Molecular Biologist\n10 Jennifer Doudna    Biochemist                        \n\n\n\n\n\ndates <- read_csv(\"data/scientists/dates.csv\")\ndates\n\n# A tibble: 8 × 3\n  name               birth_year death_year\n  <chr>                   <dbl>      <dbl>\n1 Janaki Ammal             1897       1984\n2 Chien-Shiung Wu          1912       1997\n3 Katherine Johnson        1918       2020\n4 Rosalind Franklin        1920       1958\n5 Vera Rubin               1928       2016\n6 Gladys West              1930         NA\n7 Flossie Wong-Staal       1947         NA\n8 Jennifer Doudna          1964         NA\n\n\n\n\n\nworks <- read_csv(\"data/scientists/works.csv\")\nworks\n\n# A tibble: 9 × 2\n  name               known_for                                                  \n  <chr>              <chr>                                                      \n1 Ada Lovelace       first computer algorithm                                   \n2 Marie Curie        theory of radioactivity,  discovery of elements polonium a…\n3 Janaki Ammal       hybrid species, biodiversity protection                    \n4 Chien-Shiung Wu    confim and refine theory of radioactive beta decy, Wu expe…\n5 Katherine Johnson  calculations of orbital mechanics critical to sending the …\n6 Vera Rubin         existence of dark matter                                   \n7 Gladys West        mathematical modeling of the shape of the Earth which serv…\n8 Flossie Wong-Staal first scientist to clone HIV and create a map of its genes…\n9 Jennifer Doudna    one of the primary developers of CRISPR, a ground-breaking…\n\n\n\n\n\n\n\nSource: Discover Magazine\nThe data can be downloaded: professions.csv, dates.csv, works.csv"
  },
  {
    "objectID": "W5.html#we-want-this-data-frame",
    "href": "W5.html#we-want-this-data-frame",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "We want this data frame",
    "text": "We want this data frame\n\n\n# A tibble: 10 × 5\n   name               profession                         birth…¹ death…² known…³\n   <chr>              <chr>                                <dbl>   <dbl> <chr>  \n 1 Ada Lovelace       Mathematician                           NA      NA first …\n 2 Marie Curie        Physicist and Chemist                   NA      NA theory…\n 3 Janaki Ammal       Botanist                              1897    1984 hybrid…\n 4 Chien-Shiung Wu    Physicist                             1912    1997 confim…\n 5 Katherine Johnson  Mathematician                         1918    2020 calcul…\n 6 Rosalind Franklin  Chemist                               1920    1958 <NA>   \n 7 Vera Rubin         Astronomer                            1928    2016 existe…\n 8 Gladys West        Mathematician                         1930      NA mathem…\n 9 Flossie Wong-Staal Virologist and Molecular Biologist    1947      NA first …\n10 Jennifer Doudna    Biochemist                            1964      NA one of…\n# … with abbreviated variable names ¹​birth_year, ²​death_year, ³​known_for"
  },
  {
    "objectID": "W5.html#joining-data-frames",
    "href": "W5.html#joining-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Joining data frames",
    "text": "Joining data frames\nsomething_join(x, y)^{The notion join comes from SQL database. In other data manipulation frameworks joining is called merging.} for data frames x and y which have a relation\n\nleft_join(): all rows from x\nright_join(): all rows from y\nfull_join(): all rows from both x and y\ninner_join(): all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches\n…"
  },
  {
    "objectID": "W5.html#simple-setup-for-x-and-y",
    "href": "W5.html#simple-setup-for-x-and-y",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Simple setup for x and y",
    "text": "Simple setup for x and y\n\nx <- tibble(\n  id = c(1, 2, 3),\n  value_x = c(\"x1\", \"x2\", \"x3\")\n  )\ny <- tibble(\n  id = c(1, 2, 4),\n  value_y = c(\"y1\", \"y2\", \"y4\")\n  )\nx\n\n# A tibble: 3 × 2\n     id value_x\n  <dbl> <chr>  \n1     1 x1     \n2     2 x2     \n3     3 x3     \n\ny\n\n# A tibble: 3 × 2\n     id value_y\n  <dbl> <chr>  \n1     1 y1     \n2     2 y2     \n3     4 y4"
  },
  {
    "objectID": "W5.html#left_join",
    "href": "W5.html#left_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "left_join()",
    "text": "left_join()\n\n\n\n\n\nleft_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>"
  },
  {
    "objectID": "W5.html#right_join",
    "href": "W5.html#right_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "right_join()",
    "text": "right_join()\n\n\n\n\n\nright_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     4 <NA>    y4"
  },
  {
    "objectID": "W5.html#full_join",
    "href": "W5.html#full_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "full_join()",
    "text": "full_join()\n\n\n\n\n\nfull_join(x, y)\n\n# A tibble: 4 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>   \n4     4 <NA>    y4"
  },
  {
    "objectID": "W5.html#inner_join",
    "href": "W5.html#inner_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "inner_join()",
    "text": "inner_join()\n\n\n\n\n\ninner_join(x, y)\n\n# A tibble: 2 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2"
  },
  {
    "objectID": "W5.html#women-in-science",
    "href": "W5.html#women-in-science",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Women in science",
    "text": "Women in science\n\nleft_joinright_joinfull_joininner_joinFinal\n\n\n\nprofessions |> left_join(works)\n\n# A tibble: 10 × 3\n   name               profession                         known_for              \n   <chr>              <chr>                              <chr>                  \n 1 Ada Lovelace       Mathematician                      first computer algorit…\n 2 Marie Curie        Physicist and Chemist              theory of radioactivit…\n 3 Janaki Ammal       Botanist                           hybrid species, biodiv…\n 4 Chien-Shiung Wu    Physicist                          confim and refine theo…\n 5 Katherine Johnson  Mathematician                      calculations of orbita…\n 6 Rosalind Franklin  Chemist                            <NA>                   \n 7 Vera Rubin         Astronomer                         existence of dark matt…\n 8 Gladys West        Mathematician                      mathematical modeling …\n 9 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clo…\n10 Jennifer Doudna    Biochemist                         one of the primary dev…\n\n\n\n\n\nprofessions |> right_join(works)\n\n# A tibble: 9 × 3\n  name               profession                         known_for               \n  <chr>              <chr>                              <chr>                   \n1 Ada Lovelace       Mathematician                      first computer algorithm\n2 Marie Curie        Physicist and Chemist              theory of radioactivity…\n3 Janaki Ammal       Botanist                           hybrid species, biodive…\n4 Chien-Shiung Wu    Physicist                          confim and refine theor…\n5 Katherine Johnson  Mathematician                      calculations of orbital…\n6 Vera Rubin         Astronomer                         existence of dark matter\n7 Gladys West        Mathematician                      mathematical modeling o…\n8 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clon…\n9 Jennifer Doudna    Biochemist                         one of the primary deve…\n\n\n\n\n\ndates |> full_join(works)\n\n# A tibble: 10 × 4\n   name               birth_year death_year known_for                           \n   <chr>                   <dbl>      <dbl> <chr>                               \n 1 Janaki Ammal             1897       1984 hybrid species, biodiversity protec…\n 2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioac…\n 3 Katherine Johnson        1918       2020 calculations of orbital mechanics c…\n 4 Rosalind Franklin        1920       1958 <NA>                                \n 5 Vera Rubin               1928       2016 existence of dark matter            \n 6 Gladys West              1930         NA mathematical modeling of the shape …\n 7 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cr…\n 8 Jennifer Doudna          1964         NA one of the primary developers of CR…\n 9 Ada Lovelace               NA         NA first computer algorithm            \n10 Marie Curie                NA         NA theory of radioactivity,  discovery…\n\n\n\n\n\ndates |> inner_join(works)\n\n# A tibble: 7 × 4\n  name               birth_year death_year known_for                            \n  <chr>                   <dbl>      <dbl> <chr>                                \n1 Janaki Ammal             1897       1984 hybrid species, biodiversity protect…\n2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioact…\n3 Katherine Johnson        1918       2020 calculations of orbital mechanics cr…\n4 Vera Rubin               1928       2016 existence of dark matter             \n5 Gladys West              1930         NA mathematical modeling of the shape o…\n6 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cre…\n7 Jennifer Doudna          1964         NA one of the primary developers of CRI…\n\n\n\n\n\nprofessions |> left_join(dates) |> left_join(works)\n\n# A tibble: 10 × 5\n   name               profession                         birth…¹ death…² known…³\n   <chr>              <chr>                                <dbl>   <dbl> <chr>  \n 1 Ada Lovelace       Mathematician                           NA      NA first …\n 2 Marie Curie        Physicist and Chemist                   NA      NA theory…\n 3 Janaki Ammal       Botanist                              1897    1984 hybrid…\n 4 Chien-Shiung Wu    Physicist                             1912    1997 confim…\n 5 Katherine Johnson  Mathematician                         1918    2020 calcul…\n 6 Rosalind Franklin  Chemist                               1920    1958 <NA>   \n 7 Vera Rubin         Astronomer                            1928    2016 existe…\n 8 Gladys West        Mathematician                         1930      NA mathem…\n 9 Flossie Wong-Staal Virologist and Molecular Biologist    1947      NA first …\n10 Jennifer Doudna    Biochemist                            1964      NA one of…\n# … with abbreviated variable names ¹​birth_year, ²​death_year, ³​known_for"
  },
  {
    "objectID": "W5.html#keys",
    "href": "W5.html#keys",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Keys",
    "text": "Keys\n\nA key is a variable or a set of variables which uniquely identifies observations\nWhat was the key in the data frame of women in science?\n\n\n\nSwitching back to nycflights13 as example\nIn simple cases, a single variable is sufficient to identify an observation, e.g. each plane in planes is identified by tailnum.\nSometimes, multiple variables are needed; e.g. to identify an observation in weather you need five variables: year, month, day, hour, and origin"
  },
  {
    "objectID": "W5.html#how-can-we-check",
    "href": "W5.html#how-can-we-check",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "How can we check?",
    "text": "How can we check?\nCounting observation and filter those more than one\n\nlibrary(nycflights13)\nplanes |> count(tailnum) |> filter(n > 1)\n\n# A tibble: 0 × 2\n# … with 2 variables: tailnum <chr>, n <int>\n\nweather |> count(year, month, day, hour, origin) |> filter(n > 1) \n\n# A tibble: 3 × 6\n   year month   day  hour origin     n\n  <int> <int> <int> <int> <chr>  <int>\n1  2013    11     3     1 EWR        2\n2  2013    11     3     1 JFK        2\n3  2013    11     3     1 LGA        2\n\n# OK, here 3 observations are twice. Probably a data error.\n# Example: Without hour it is not a key\nweather |> count(year, month, day, origin) |> filter(n > 1) \n\n# A tibble: 1,092 × 5\n    year month   day origin     n\n   <int> <int> <int> <chr>  <int>\n 1  2013     1     1 EWR       22\n 2  2013     1     1 JFK       22\n 3  2013     1     1 LGA       23\n 4  2013     1     2 EWR       24\n 5  2013     1     2 JFK       24\n 6  2013     1     2 LGA       24\n 7  2013     1     3 EWR       24\n 8  2013     1     3 JFK       24\n 9  2013     1     3 LGA       24\n10  2013     1     4 EWR       24\n# … with 1,082 more rows"
  },
  {
    "objectID": "W5.html#primary-and-foreign-keys",
    "href": "W5.html#primary-and-foreign-keys",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Primary and foreign keys",
    "text": "Primary and foreign keys\n\nA primary key uniquely identifies an observation in its own table. E.g, planes$tailnum in planes.\nA foreign key uniquely identifies an observation in another data frame E.g. flights$tailnum is a foreign key in flights because it matches each flight to a unique plane in planes.\nData frames need not have a key and the joins will still do their work.\nA primary key and a foreign key form a relation.\nRelations are typically 1-to-many. Each plane has many flights\nRelations can also be many-to-many. Airlines can fly to many airports; airport can host many airplanes."
  },
  {
    "objectID": "W5.html#joining-when-key-names-differ",
    "href": "W5.html#joining-when-key-names-differ",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Joining when key names differ?",
    "text": "Joining when key names differ?\nWe have to specify the key relation with a named vector in the by argument.\n\ndim(flights)\n\n[1] 336776     19\n\nflights |> left_join(airports, by = c(\"dest\" = \"faa\"))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 16 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>, lon <dbl>,\n#   alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nWhy does the number of rows stays the same after joining?\n\nfaa is a primary key in airports."
  },
  {
    "objectID": "W5.html#left_join-essentially-right_join-with-switched-data-frames",
    "href": "W5.html#left_join-essentially-right_join-with-switched-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "left_join essentially right_join with switched data frames",
    "text": "left_join essentially right_join with switched data frames\n\nairports_right_flights <- airports |> right_join(flights, by = c(\"faa\" = \"dest\"))\nairports_right_flights \n\n# A tibble: 336,776 × 26\n   faa   name        lat   lon   alt    tz dst   tzone  year month   day dep_t…¹\n   <chr> <chr>     <dbl> <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>   <int>\n 1 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     1    1955\n 2 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     2    2010\n 3 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     3    1955\n 4 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     4    2017\n 5 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     5    1959\n 6 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     6    1959\n 7 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     7    2002\n 8 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     8    1957\n 9 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     9    1957\n10 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10    10    2011\n# … with 336,766 more rows, 14 more variables: sched_dep_time <int>,\n#   dep_delay <dbl>, arr_time <int>, sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, air_time <dbl>,\n#   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated\n#   variable name ¹​dep_time\n\n\nDifferences\n\nIn a join where keys have different column names the name of the first data frame survives (unless you use keep = TRUE). Here, faa instead of dest\nThe columns from the first data frame come first\nThe order of rows is taken from the first data frame, while duplication and dropping of variables is determined by the second data frame (because it is a right_join)\n\nUsing the fact that flights seem to be ordered by year, month, day, dep_time we can re-arrange:\n\nairports_right_flights |> \n  rename(dest = faa) |> \n  select(names(flights)) |> # Use order of flights\n  arrange(year, month, day, dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     924     917       7 UA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nNote of caution: A deeper analysis shows that the order is still not exactly the same."
  },
  {
    "objectID": "W5.html#left_join-with-reversed-data-frames",
    "href": "W5.html#left_join-with-reversed-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "left_join with reversed data frames",
    "text": "left_join with reversed data frames\n\ndim(airports)\n\n[1] 1458    8\n\ndim(flights)\n\n[1] 336776     19\n\nairports |> \n  left_join(flights, by = c(\"faa\" = \"dest\"))\n\n# A tibble: 330,531 × 26\n   faa   name       lat    lon   alt    tz dst   tzone  year month   day dep_t…¹\n   <chr> <chr>    <dbl>  <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>   <int>\n 1 04G   Lansdow…  41.1  -80.6  1044    -5 A     Amer…    NA    NA    NA      NA\n 2 06A   Moton F…  32.5  -85.7   264    -6 A     Amer…    NA    NA    NA      NA\n 3 06C   Schaumb…  42.0  -88.1   801    -6 A     Amer…    NA    NA    NA      NA\n 4 06N   Randall…  41.4  -74.4   523    -5 A     Amer…    NA    NA    NA      NA\n 5 09J   Jekyll …  31.1  -81.4    11    -5 A     Amer…    NA    NA    NA      NA\n 6 0A9   Elizabe…  36.4  -82.2  1593    -5 A     Amer…    NA    NA    NA      NA\n 7 0G6   William…  41.5  -84.5   730    -5 A     Amer…    NA    NA    NA      NA\n 8 0G7   Finger …  42.9  -76.8   492    -5 A     Amer…    NA    NA    NA      NA\n 9 0P2   Shoestr…  39.8  -76.6  1000    -5 U     Amer…    NA    NA    NA      NA\n10 0S9   Jeffers…  48.1 -123.    108    -8 A     Amer…    NA    NA    NA      NA\n# … with 330,521 more rows, 14 more variables: sched_dep_time <int>,\n#   dep_delay <dbl>, arr_time <int>, sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, air_time <dbl>,\n#   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated\n#   variable name ¹​dep_time\n\n\nWhy does the number of rows changes after joining?\ndest is not a primary key in flights. There are more flights with the same destination so rows of airports get duplicated.\nWhy is the number of rows then less than the number of rows in flights?\nLet us do some checks:\n\nlength(unique(airports$faa)) # Unique turns out to be redundant because faa is a primary key\n\n[1] 1458\n\nlength(unique(flights$dest))\n\n[1] 105\n\n# There are much more airports then destinations in flights!\n# ... but the rows of airports prevail when it is the first in a left_join.\n# So, the data frame should even increase because \n# we get several rows of airports without flights\n# Let us dig deeper.\n\nsetdiff( unique(airports$faa), unique(flights$dest)) |> length()\n\n[1] 1357\n\n# 1,357 airports have no flights. But also:\nsetdiff( unique(flights$dest), unique(airports$faa)) |> length()\n\n[1] 4\n\n# There are four destinations in flights, which are not in the airports list!\n\n# How many flights are to these?\nflights |> \n  filter(dest %in% setdiff( unique(flights$dest), unique(airports$faa))) |> \n  nrow()\n\n[1] 7602\n\n# 7,602 flights go to destinations not listed as airport\n\n# Check\nnrow(airports |> left_join(flights, by = c(\"faa\" = \"dest\"))) == nrow(flights) - 7602 + 1357\n\n[1] TRUE\n\n# OK, now we have a clear picture\n# airport with left_joined flights duplicates the rows an airports for each flight flying to it\n# So the total number of rows is the number of flights plus the number of airport which do not \n# appear as a destination minus the flights which go to destinations which are not listed in airports\n\nThe new number of observation after a join can be a complex combination of duplication and dropping."
  },
  {
    "objectID": "W5.html#definition-sets-and-vectors",
    "href": "W5.html#definition-sets-and-vectors",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Definition: Sets and vectors",
    "text": "Definition: Sets and vectors\nA set is mathematical model for the collection of different things.\nExamples:\n\n\\(\\{3, \\text{Hi}, 😀, 🖖 \\}\\)\n\\(\\{1,3,5\\}\\)\nThe natural numbers \\(\\mathbb{N} = \\{1, 2, 3, \\dots\\}\\) (infinite!)\n\\(\\{\\mathtt{\"EWR\"} \\mathtt{\"LGA\"} \\mathtt{\"JFK\"}\\}\\)\nthese are origin airports in flights"
  },
  {
    "objectID": "W5.html#math-sets-and-vectors-1",
    "href": "W5.html#math-sets-and-vectors-1",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Math: Sets and vectors",
    "text": "Math: Sets and vectors\nA vector is an ordered collection of things (elements) of the same type.\nIn a set each thing can only be once and the order does not matter!\n\\(\\{1,3,5\\} = \\{3,5,1\\} = \\{1,1,1,3,5,5\\}\\)\nFor vectors:\n\\([1\\ 3\\ 5] \\neq [3\\ 5\\ 1]\\) because we compare component-wise, so we cannot even compare with = \\([1\\ 1\\ 1\\ 3\\ 5\\ 5]\\)"
  },
  {
    "objectID": "W5.html#math-set-operations",
    "href": "W5.html#math-set-operations",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Math: Set operations",
    "text": "Math: Set operations\nFor sets \\(A = \\{🐺, 🦊, 🐶\\}\\) and \\(B = \\{🐶, 🐷, 🐹\\}\\):\n\nSet union \\(A \\cup B\\) = {🐺, 🦊, 🐶, 🐷, 🐹}\nSet intersection \\(A \\cap B\\) = {🐶}\nSet different \\(A \\setminus B\\) = {🐺, 🦊}$, \\(B \\setminus A\\) = {🐷, 🐹}"
  },
  {
    "objectID": "W5.html#set-operations-in-r",
    "href": "W5.html#set-operations-in-r",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Set operations in R",
    "text": "Set operations in R\nunique shows the set of elements in a vector\n\nunique(flights$origin)\n\n[1] \"EWR\" \"LGA\" \"JFK\"\n\n\nsetequal tests for set equality\n\nsetequal(c(\"EWR\",\"LGA\",\"JFK\"), c(\"EWR\",\"EWR\",\"LGA\",\"JFK\"))\n\n[1] TRUE\n\n\nunion, intersect, setdiff treat vectors as sets and operate as expected\n\nunion(1:5,3:7)\n\n[1] 1 2 3 4 5 6 7\n\nintersect(1:5,3:7)\n\n[1] 3 4 5\n\nsetdiff(1:5,3:7)\n\n[1] 1 2"
  },
  {
    "objectID": "W5.html#exploratory-data-analysis-1",
    "href": "W5.html#exploratory-data-analysis-1",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nEDA is the systematic exploration of data using\n\nvisualization\ntransformation\ncomputation of characteristic values\nmodeling\n\n\n\nWe haven’t talked much about the latter two, but will do soon.\nComputation of characteristic values: Functions like mean, median, mode, standard deviation, or interquartile range\nModeling: Operations like linear regression or dimensionality reduction"
  },
  {
    "objectID": "W5.html#systematic-but-no-standard-routine",
    "href": "W5.html#systematic-but-no-standard-routine",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\n“There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey"
  },
  {
    "objectID": "W5.html#systematic-but-no-standard-routine-1",
    "href": "W5.html#systematic-but-no-standard-routine-1",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\nGoal of EDA: Develop understanding of your data.\nEDA’s iterative cycle\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\n\nEDA is fundamentally a creative process."
  },
  {
    "objectID": "W5.html#questions",
    "href": "W5.html#questions",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Questions",
    "text": "Questions\n\nThe way to ask quality questions:\n\nGenerate many questions!\nYou cannot come up with most interesting questions when you start.\n\nThere is no rule which questions to ask. These are useful\n\nWhat type of variation occurs within my variables?\n(Barplots, Histograms,…)\nWhat type of covariation occurs between my variables?\n(Scatterplots, Timelines,…)"
  },
  {
    "objectID": "W5.html#eda-embedded-in-a-data-science-project",
    "href": "W5.html#eda-embedded-in-a-data-science-project",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "EDA embedded in a data science project",
    "text": "EDA embedded in a data science project\n\nStating and refining the question\nExploring the data\nBuilding formal statistical models\nInterpreting the results\nCommunicating the results\n\n\n\nRoger D. Peng and Elizabeth Matsui. “The Art of Data Science.” A Guide for Anyone Who Works with Data. Skybrude Consulting, LLC (2015)."
  },
  {
    "objectID": "W5.html#six-types-of-questions",
    "href": "W5.html#six-types-of-questions",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Six types of questions",
    "text": "Six types of questions\n\nDescriptive: summarize a characteristic of a set of data\nExploratory: analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\nInferential: analyze patterns, trends, or relationships in representative data from a population\nPredictive: make predictions for individuals or groups of individuals\nCausal: whether changing one factor will change another factor, on average, in a population\nMechanistic: explore “how” as opposed to whether\n\n\n\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146."
  },
  {
    "objectID": "W5.html#data-analysis-flowchart",
    "href": "W5.html#data-analysis-flowchart",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Data Analysis Flowchart",
    "text": "Data Analysis Flowchart"
  },
  {
    "objectID": "W5.html#example-covid-19-and-vitamin-d",
    "href": "W5.html#example-covid-19-and-vitamin-d",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Example: COVID-19 and Vitamin D",
    "text": "Example: COVID-19 and Vitamin D\n\nDescriptive: frequency of hospitalisations due to COVID-19 in a set of data collected from a group of individuals\nExploratory: examine relationships between a range of dietary factors and COVID-19 hospitalisations\nInferential: examine whether any relationship between taking Vitamin D supplements and COVID-19 hospitalisations found in the sample hold for the population at large\nPredictive: what types of people will take Vitamin D supplements during the next year\nCausal: whether people with COVID-19 who were randomly assigned to take Vitamin D supplements or those who were not are hospitalised\nMechanistic: how increased vitamin D intake leads to a reduction in the number of viral illnesses"
  },
  {
    "objectID": "W5.html#questions-to-data-science-problems",
    "href": "W5.html#questions-to-data-science-problems",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Questions to data science problems",
    "text": "Questions to data science problems\n\nDo you have appropriate data to answer your question?\nDo you have information on confounding variables?\nWas the data you’re working with collected in a way that introduces bias?\n\n\n\nExample\nI want to estimate the average number of children in households in Bremen. I conduct a survey at an elementary school and ask pupils how many children, including themselves, live in their house. Then, I take the average of the responses.\n\nIs this a biased or an unbiased estimate of the number of children in households in Bremen?\nIf biased, will the value be an overestimate or underestimate?"
  },
  {
    "objectID": "W5.html#context-information-and-codebooks",
    "href": "W5.html#context-information-and-codebooks",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Context Information and Codebooks",
    "text": "Context Information and Codebooks\n\nNot all information is in the data!\nPotential confounding variables you infer from general knowledge\nInformation about data collection you may receive from an accompanying report\nInformation about computed variables you may need to look up in accompanying documentation\nInformation about certain variables you may find in an accompanying codebook. For example the exact wording of questions in survey data."
  },
  {
    "objectID": "W5.html#next",
    "href": "W5.html#next",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Next",
    "text": "Next\nNext Week\n\nSummarizing functions for data\nSome more math background (linked to programming)\n\nHomework 03\n\nshall come over the weekend, due in two week\nwill move towards\n\nexploratory data analysis\nanswering questions (You have some technical tools now at hand.)\nasking question\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts\n\n\nHint: Everyone has read access to the project repositories https://github.com/JU-F22-MDSSB-MET-01/ess-ind-janlorenz and https://github.com/JU-F22-MDSSB-MET-01/corona-ind-janlorenz as an example for data access. This should help to unify the data for work on Homework 03."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts / Tools",
    "section": "",
    "text": "Information for late coming students\n\n\n\nWelcome! You are in the right place to get into the course. There is a lot of material. Here we provide a checklist for late coming students"
  },
  {
    "objectID": "index.html#modules-data-science-concepts-methods",
    "href": "index.html#modules-data-science-concepts-methods",
    "title": "Data Science Concepts / Tools",
    "section": "1.1 Modules Data Science Concepts / Methods",
    "text": "1.1 Modules Data Science Concepts / Methods\nThese two modules are orchestrated in close cooperation\n\nData Science Concepts (Core module: MSDSSB-DSOC-02)\nData Science Tools (Methods module: MDSSB-MET-01)\n\nYou should know how the courses are integrated in the overall Master program and the module descriptions from the DSSB Handbook."
  },
  {
    "objectID": "index.html#courses-and-instructors",
    "href": "index.html#courses-and-instructors",
    "title": "Data Science Concepts / Tools",
    "section": "1.2 Courses and Instructors",
    "text": "1.2 Courses and Instructors\nThursday is the concepts and tools day!\nData Science Tools in R: Thursday 9:45 - 11:00 Armin Müller\nData Science Tools in Python: Thursday 11:15 - 12:30 Martin Gestefeld\nData Science Concepts Lectures: Thursday 14:15 - 17:00 (2 Sessions) Jan Lorenz"
  },
  {
    "objectID": "index.html#goal",
    "href": "index.html#goal",
    "title": "Data Science Concepts / Tools",
    "section": "1.3 Goal",
    "text": "1.3 Goal\nOur goal in the two modules is to enable you to\n\ncreate and maintain a digital working environment on your computer to do data science\nlearn core concepts in data science, that means\n\nlearn concepts to explore data (visualize, import, wrangle)\nlearn or refresh some mathematics and statistics concepts through the data science lens\nlearn concepts to model and draw conclusions from data (model, infer, predict)\n\nlearn to program in the data science languages R and python, and become able to learn new skills in these independently\ndo a data science project of your own interest\n\nYou can build a good basis for your more and more independent work in the whole program."
  },
  {
    "objectID": "index.html#expectations-for-students",
    "href": "index.html#expectations-for-students",
    "title": "Data Science Concepts / Tools",
    "section": "1.4 Expectations for students",
    "text": "1.4 Expectations for students\nWe rely on your engagement. Taken holistically, engagement is many-faceted and includes\n\nPreparation (looking at readings and material before class, being informed about syllabus and course material)\nFocus (avoid distraction during in class and online activities)\nPresence (listening and responding during group activities)\nAsking questions (in class, out of class, online, offline)\nSpecificity (being specific when referring to ideas from readings and discussions)\nSynthesizing (making connections between concepts from reading and discussion)\n\n(Adapted from Twitter: Mark Sample)"
  },
  {
    "objectID": "index.html#online-infrastructure",
    "href": "index.html#online-infrastructure",
    "title": "Data Science Concepts / Tools",
    "section": "1.5 Online Infrastructure",
    "text": "1.5 Online Infrastructure\nCampusnet This is the official registration site. Besides the final grade input for the modules we will not use it in the courses, but you should be able to get back to this site from there once you are lost.\njMoodle An e-learning platform provided by Jacobs University. Log in with your campusnet credentials. Under the Data Science Concepts Course (identifier MDSSB-DSOC-02_f2022_1) we publish information (e.g. survey links) which shall be shared only among participants of the course. (After all this is a public website, although it is not intended to be announced publicly.)\n\n\n\n\n\n\nImportant\n\n\n\nTest to log in and visit the Data Science Concepts course on jMoodle. Be prepared to go there during class!\n\n\nMicrosoft Teams: In MS Teams there is a Team for the course. Teams will be used for hybrid online and in class teaching. It is expected that you come to class in presence if possible. Participation should only be online when abroad or CoViD-isolated at home. The “General” channel can be used for announcements. You can also post there. You can also post to me directly over Teams and hope for a quick answer.\nIf possible, recordings are made via Teams and saved in the Teams general channel under Files -> Recordings.\nGitHub: All delivery and submission of homework is via Github. Details in the lecture slides of week 1 and “Homework 01” (on this page)."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Data Science Concepts / Tools",
    "section": "1.6 Software",
    "text": "1.6 Software\nWe use a mandatory set of software. Details are in the slides of week 1. Our task with highest priority is to realize that everyone has full installation and functionality on their local machines. All software is freely available."
  },
  {
    "objectID": "index.html#assessment-and-grading",
    "href": "index.html#assessment-and-grading",
    "title": "Data Science Concepts / Tools",
    "section": "1.7 Assessment and Grading",
    "text": "1.7 Assessment and Grading\nThis will be updated and made more specific later.\nData Science Concepts: Assessment is based on an exam of 120 minutes. It will be scheduled officially on campusnet by academic offices probably in the range around Dec 15.\nData Science Tools: Assessment is based on a team project of approximately 4000-5000 words. As a requirement, enough homework assignment tasks need to be correctly solved. These do not determine the grade, but can improve it."
  },
  {
    "objectID": "index.html#week-1-sep-1-what-is-data-science-course-organization-toolkit",
    "href": "index.html#week-1-sep-1-what-is-data-science-course-organization-toolkit",
    "title": "Data Science Concepts / Tools",
    "section": "Week 1, Sep 1: What is Data Science? Course organization, toolkit",
    "text": "Week 1, Sep 1: What is Data Science? Course organization, toolkit\nSlides Week 1\nFind the homework assignment “Homework 01” on this website. Deadline: Sunday, Sep 18. The content will be discussed in the Tools course in Week 2 and 3"
  },
  {
    "objectID": "index.html#week-2-sep-8-no-lectures",
    "href": "index.html#week-2-sep-8-no-lectures",
    "title": "Data Science Concepts / Tools",
    "section": "Week 2, Sep 8: No lectures",
    "text": "Week 2, Sep 8: No lectures\nThere will be no lectures, because of a mandatory central events!\nThe first Data Science Tools courses in the morning will take place!\nReading instead of lecture: R for Data Science: Chapter 3 on Data Visualization"
  },
  {
    "objectID": "index.html#week-3-sep-15-data-visualization-data-formats",
    "href": "index.html#week-3-sep-15-data-visualization-data-formats",
    "title": "Data Science Concepts / Tools",
    "section": "Week 3, Sep 15: Data visualization, Data formats",
    "text": "Week 3, Sep 15: Data visualization, Data formats\nSlides Week 3\nLab work on code. Clone from: https://github.com/JU-F22-MDSSB-MET-01/codebase-janlorenz.git In RStudio, work through\n\n2022-09-15_ggplot_and_pipe.R\n2022-09-15_profiles.qmd"
  },
  {
    "objectID": "index.html#week-4-sep-22-data-import-data-wrangling",
    "href": "index.html#week-4-sep-22-data-import-data-wrangling",
    "title": "Data Science Concepts / Tools",
    "section": "Week 4, Sep 22: Data import, data Wrangling",
    "text": "Week 4, Sep 22: Data import, data Wrangling\nSlides Week 4\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nIncluding some recap of the toolkit.\nQuestions and advice on Homework 02 (mostly in Recording)."
  },
  {
    "objectID": "index.html#week-5-sep-29-more-under-the-hood-relational-data-exploratory-data-analysis",
    "href": "index.html#week-5-sep-29-more-under-the-hood-relational-data-exploratory-data-analysis",
    "title": "Data Science Concepts / Tools",
    "section": "Week 5, Sep 29: More under the hood, Relational Data, Exploratory Data Analysis",
    "text": "Week 5, Sep 29: More under the hood, Relational Data, Exploratory Data Analysis\nSlides Week 5\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nTopics:\n\nMore under the hood\nAugmented vectors: Factors and Dates\nStrings\nRelational Data: Joins, joins, joins …\nMath: Sets and vectors\nExploratory Data Analysis\nData Science Projects\n\nHomework 02 due next Sunday.\nHomework 03 comes over the weekend."
  },
  {
    "objectID": "index.html#week-6-oct-6-functions-logarithms-and-exponentials-modeling-fitting-a-linear-model",
    "href": "index.html#week-6-oct-6-functions-logarithms-and-exponentials-modeling-fitting-a-linear-model",
    "title": "Data Science Concepts / Tools",
    "section": "Week 6, Oct 6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "text": "Week 6, Oct 6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model\nSlides Week 6\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 03 is due Oct 16."
  },
  {
    "objectID": "index.html#week-7-oct-13-descriptive-statistics-wisdom-of-crowds-calculus-epidemic-models",
    "href": "index.html#week-7-oct-13-descriptive-statistics-wisdom-of-crowds-calculus-epidemic-models",
    "title": "Data Science Concepts / Tools",
    "section": "Week 7, Oct 13: Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "text": "Week 7, Oct 13: Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models\nSlides Week 7\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 03 is due Oct 16."
  },
  {
    "objectID": "index.html#week-8-oct-20-homework-topics-typical-data-issual-more-linear-models-and-interpretation",
    "href": "index.html#week-8-oct-20-homework-topics-typical-data-issual-more-linear-models-and-interpretation",
    "title": "Data Science Concepts / Tools",
    "section": "Week 8, Oct 20: Homework topics: Typical Data Issual, More Linear Models and Interpretation",
    "text": "Week 8, Oct 20: Homework topics: Typical Data Issual, More Linear Models and Interpretation\nTopics:\n\nErrors, Differences, Missings in Data\nMore Linear Models\n\nMore predictors\nMain effects and interaction effects\n\nTo be finished\n\nSlides Week 8\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 should come. Due Nov 6."
  },
  {
    "objectID": "index.html#week-9-oct-27",
    "href": "index.html#week-9-oct-27",
    "title": "Data Science Concepts / Tools",
    "section": "Week 9, Oct 27:",
    "text": "Week 9, Oct 27:"
  },
  {
    "objectID": "index.html#week-10-nov-3",
    "href": "index.html#week-10-nov-3",
    "title": "Data Science Concepts / Tools",
    "section": "Week 10, Nov 3:",
    "text": "Week 10, Nov 3:\nHomework 04 due in 3 days\nHomework 05 should come. Due Nov 20."
  },
  {
    "objectID": "index.html#week-11-nov-10",
    "href": "index.html#week-11-nov-10",
    "title": "Data Science Concepts / Tools",
    "section": "Week 11, Nov 10:",
    "text": "Week 11, Nov 10:"
  },
  {
    "objectID": "index.html#week-12-nov-17",
    "href": "index.html#week-12-nov-17",
    "title": "Data Science Concepts / Tools",
    "section": "Week 12, Nov 17:",
    "text": "Week 12, Nov 17:\nHomework 05 due in 3 days\nHomework 06 should come. Due Dec 4."
  },
  {
    "objectID": "index.html#week-13-nov-24",
    "href": "index.html#week-13-nov-24",
    "title": "Data Science Concepts / Tools",
    "section": "Week 13, Nov 24:",
    "text": "Week 13, Nov 24:"
  },
  {
    "objectID": "index.html#week-14-dec-1-course-review-exam-preparation",
    "href": "index.html#week-14-dec-1-course-review-exam-preparation",
    "title": "Data Science Concepts / Tools",
    "section": "Week 14, Dec 1: Course Review, Exam preparation",
    "text": "Week 14, Dec 1: Course Review, Exam preparation\nHomework 06 due in 3 days"
  },
  {
    "objectID": "index.html#exam-data-science-concepts-module",
    "href": "index.html#exam-data-science-concepts-module",
    "title": "Data Science Concepts / Tools",
    "section": "Exam Data Science Concepts module",
    "text": "Exam Data Science Concepts module\nIn December, will be scheduled officially on campusnet by academic offices."
  },
  {
    "objectID": "W9.html#how-many-observations-are-there-for-each-country-year-combination",
    "href": "W9.html#how-many-observations-are-there-for-each-country-year-combination",
    "title": "W#9 Logisitc Regression",
    "section": "How many observations are there for each country-year combination?",
    "text": "How many observations are there for each country-year combination?\nWeighting"
  },
  {
    "objectID": "W9.html#more-weights-from-the-survey",
    "href": "W9.html#more-weights-from-the-survey",
    "title": "W#9 Logisitc Regression",
    "section": "More weights from the survey",
    "text": "More weights from the survey"
  },
  {
    "objectID": "W9.html#mean-squared-error",
    "href": "W9.html#mean-squared-error",
    "title": "W#9 Logisitc Regression",
    "section": "Mean squared error",
    "text": "Mean squared error"
  },
  {
    "objectID": "W9.html#how-deaths-follow-cases",
    "href": "W9.html#how-deaths-follow-cases",
    "title": "W#9 Logisitc Regression",
    "section": "How Deaths Follow Cases",
    "text": "How Deaths Follow Cases"
  },
  {
    "objectID": "W9.html#next",
    "href": "W9.html#next",
    "title": "W#9 Logisitc Regression",
    "section": "Next",
    "text": "Next\nhttps://datasciencebox.org/course-materials/_slides/u4-d05-more-model-multiple-predictors/u4-d05-more-model-multiple-predictors.html#1"
  },
  {
    "objectID": "W9.html#test-und-training-set",
    "href": "W9.html#test-und-training-set",
    "title": "W#9 Logisitc Regression",
    "section": "Test und Training set",
    "text": "Test und Training set\nCross validation"
  },
  {
    "objectID": "W9.html#decision-trees",
    "href": "W9.html#decision-trees",
    "title": "W#9 Logisitc Regression",
    "section": "Decision Trees",
    "text": "Decision Trees"
  },
  {
    "objectID": "W4.html#programming-languages",
    "href": "W4.html#programming-languages",
    "title": "W#4 Data import, data wrangling",
    "section": "Programming languages",
    "text": "Programming languages\nSystems of rules which can process instructions to be executed by the computer.\nOur programming languages are:\n\n   \n\n\n\n\nIn R with function:\ndo_this(to_this)\ndo_that(to_this, with_those)\nto_this |> do_this() |> do_that(with_those) \nstore <- do_that(to_this)\n\nIn python:\nto_this.do_this()\nto_this.do_this(with_those)\nto_this.do_this().do_that(with_those)\nstore = do_that(to_this)\n\n\nWe can use R and python in a standard terminal (write R or python3) and write scripts with any editor (Wordpad)."
  },
  {
    "objectID": "W4.html#integrated-development-environment",
    "href": "W4.html#integrated-development-environment",
    "title": "W#4 Data import, data wrangling",
    "section": "Integrated development environment",
    "text": "Integrated development environment\nIDEs provide terminals, a source code editor, an object browser, output and help view, tools for rendering and version control, and more to help in the workflow. Our IDEs are:\n\n    VS Code\n\n\nEditors delight us with\n\nsyntax highlighting Then we see if code looks good\n\nc(1O, Text, true, 10,\"Text\",TRUE)\n\ncode completion Start writing, and press Tab to see options\nautomatic indentation, brace matching, keyboard shortcuts, …"
  },
  {
    "objectID": "W4.html#publishing-system",
    "href": "W4.html#publishing-system",
    "title": "W#4 Data import, data wrangling",
    "section": "Publishing system",
    "text": "Publishing system\nWeaves together text and code to produces good-looking formatted scientific or technical output.\n\n   \n\n\nA YAML header and Markdown text with code chunks is rendered to a document in several formats.\n notebook is a similar concept: text and executable code mixed together in a browser tab. Can be rendered by quarto. Popular in the python world."
  },
  {
    "objectID": "W4.html#publish-what",
    "href": "W4.html#publish-what",
    "title": "W#4 Data import, data wrangling",
    "section": "Publish what?",
    "text": "Publish what?\n\nyour project report as (html)\nmake a personal website (using GitHub pages)\nwrite your thesis (pdf)"
  },
  {
    "objectID": "W4.html#feedback-on-homework-01",
    "href": "W4.html#feedback-on-homework-01",
    "title": "W#4 Data import, data wrangling",
    "section": "Feedback on Homework 01",
    "text": "Feedback on Homework 01\nSome of you did not modify the line\n“The dimension with the most experience is … The dimension with the least experience is …”\nor did not replace the line\n“Remove this text and write you answer to Exercise 4.”\nwith your text.\nYou did the programming right, but forgot the textual part. Now, this doesn’t matter. However, the learning goal was not only programming, but also taking care that the rendered output communicates your work well."
  },
  {
    "objectID": "W4.html#version-control",
    "href": "W4.html#version-control",
    "title": "W#4 Data import, data wrangling",
    "section": "Version control",
    "text": "Version control\n   \ngit manages local versioning of files in a directory1 as repository2, and merging different versions of the repository.\nGitHub provides git server for repositories and collaborative tools.\nDirectory = FolderRepository = A directory including a subfolder .git which stores the history of as commits."
  },
  {
    "objectID": "W4.html#command-line-interfaces",
    "href": "W4.html#command-line-interfaces",
    "title": "W#4 Data import, data wrangling",
    "section": "Command line interfaces",
    "text": "Command line interfaces\nIn CLIs you communicate with your computer using the Read-Evaluate-Print-Loop (REPL). Terminal, Shell, Console all mostly synonym to CLI\n\nTerminal to access files and programs via commands in bash or zsh1. Also available in RStudio and VS Code.\nR console provided in RStudio2\npython3 console provided by VS Code3\n\nLanguages used in the Terminal with commands like cd=change directory, pwd=print working directory, or ls=list files.Can also be started in Terminal with RCan also be started in Terminal with python3"
  },
  {
    "objectID": "W4.html#feedback-on-version-control",
    "href": "W4.html#feedback-on-version-control",
    "title": "W#4 Data import, data wrangling",
    "section": "Feedback on version control",
    "text": "Feedback on version control\n\nUsing git and GitHub is one of the most common modes of collaboration involving coding\nIt is a learning goal to get used to it\nIt will probably not go away after this course\ngit problems are sometimes uncomfortable to solve and require concentration and grit (also for experienced people)"
  },
  {
    "objectID": "W4.html#quarto-and-git-only-use-the-cli",
    "href": "W4.html#quarto-and-git-only-use-the-cli",
    "title": "W#4 Data import, data wrangling",
    "section": "quarto and git only use the CLI",
    "text": "quarto and git only use the CLI\nFor example:\n\nquarto render MyFile.qmd --to docx renders the MyFile.qmd to a Word file  git add MyFile.qmd adds MyFile.qmd (or ots changes) to the staging area\ngit commit -m \"Update of code\" creates new commit with staged files\ngit push merge local commit into repository it was cloned from\n\n\nRStudio/VS Code provide buttons and shortcuts for the most common commands\nYou can also do it yourself.\nUse it to solve a problem in an “uncommon” situation, after research about the problem:\n\nread error message carefully (often they give a hint, but not always)\nsearching StackOverflow\nasking others\nfiling an issue in our General Discussion"
  },
  {
    "objectID": "W4.html#readr-and-readxl",
    "href": "W4.html#readr-and-readxl",
    "title": "W#4 Data import, data wrangling",
    "section": "readr and readxl",
    "text": "readr and readxl\n\n\n\n\nread_csv() - comma delimited files\nread_csv2() - semicolon delimited files (common where “,” is used as decimal place)\nread_tsv() - tab delimited files\nread_delim() - reads in files with any delimiter\n…\n\n\n\n\nread_excel() read xls or xlsx files from MS Excel\n…"
  },
  {
    "objectID": "W4.html#other-data-formats",
    "href": "W4.html#other-data-formats",
    "title": "W#4 Data import, data wrangling",
    "section": "Other data formats",
    "text": "Other data formats\nR packages, analog libraries will exist for python\n\ngooglesheets4: Google Sheets\nhaven: SPSS, Stata, and SAS files\nDBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc): allows you to run SQL queries against a database and return a data frame\njsonline: JSON\nxml2: xml\nrvest: web scraping\nhttr: web APIs\n…"
  },
  {
    "objectID": "W4.html#comma-separated-values-csv",
    "href": "W4.html#comma-separated-values-csv",
    "title": "W#4 Data import, data wrangling",
    "section": "Comma-separated values (CSV)",
    "text": "Comma-separated values (CSV)\nWe use this when there is no certain reason to do otherwise (it is not provided, or storage is an issue).\nCSV files are delimited text file\n\nCan be viewed with any text editor\nShow each row of the data frame in a line\nSeparates the content of columns by commas (or the delimiter character)\nEach cell could be surrounded by quotes (when long text with commas (!) is in cells)\nThe first line is interpreted as listing the variable names by default\n\nreadr tries to guess the data type of variables\nYou can also customize it yourself!"
  },
  {
    "objectID": "W4.html#data-import-workflow",
    "href": "W4.html#data-import-workflow",
    "title": "W#4 Data import, data wrangling",
    "section": "Data import workflow",
    "text": "Data import workflow\n\nYou download your CSV file to the data/ directory. You may use download.file() for this, but make sure you do not download large amounts of data each time you render your file! (Comment out # and use again only when needed.)\nRead the data with data <- read_csv(\"data/FILENAME.csv\") and read the report in the console.\nExplore if you are happy and iterate by customizing you data import line using specifications (see the function help) until the data is as you want it to be.\n\nUse this for Homework 02 for the ESS and corona projects.\nSelf-learning:  concepts similar for loading CSV in python."
  },
  {
    "objectID": "W4.html#columns-types",
    "href": "W4.html#columns-types",
    "title": "W#4 Data import, data wrangling",
    "section": "Columns types",
    "text": "Columns types\n\n\n\ntype function\ndata type\n\n\n\n\ncol_character()\ncharacter\n\n\ncol_date()\ndate\n\n\ncol_datetime()\nPOSIXct (date-time)\n\n\ncol_double()\ndouble (numeric)\n\n\ncol_factor()\nfactor\n\n\ncol_guess()\nlet readr guess (default)\n\n\ncol_integer()\ninteger\n\n\ncol_logical()\nlogical\n\n\ncol_number()\nnumbers mixed with non-number characters\n\n\ncol_numeric()\ndouble or integer\n\n\ncol_skip()\ndo not read\n\n\ncol_time()\ntime"
  },
  {
    "objectID": "W4.html#data-hotel-bookings",
    "href": "W4.html#data-hotel-bookings",
    "title": "W#4 Data import, data wrangling",
    "section": "Data: Hotel bookings",
    "text": "Data: Hotel bookings\n\nData from two hotels: one resort and one city hotel\nObservations: Each row represents a hotel booking\n\n\n\nhotels<- read_csv(\"data/hotels.csv\")\n\nRows: 119390 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): hotel, arrival_date_month, meal, country, market_segment, distrib...\ndbl  (18): is_canceled, lead_time, arrival_date_year, arrival_date_week_numb...\ndate  (1): reservation_status_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "W4.html#grammar-of-data-wrangling",
    "href": "W4.html#grammar-of-data-wrangling",
    "title": "W#4 Data import, data wrangling",
    "section": "Grammar of Data Wrangling",
    "text": "Grammar of Data Wrangling\n\n\n\n\n\nGrammar of data wrangling: Start with a dataset and pipe it through several manipulations with |>\nmpg |> \n  filter(cyl == 8) |> \n  select(manufacturer, hwy) |> \n  group_by(manufacturer) |> \n  summarize(mean_hwy = mean(hwy))\n\n\nIn python: Similar concept making a chain using . to apply pandas methods for data frames one after the other.\nCompare the grammar of graphics ggplot2: Start creating a ggplot object, specifying data, and mapping variables to aesthetics, add graphical layers (geom_ functions) with +\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = trans)) + \n  geom_point() + \n  geom_smooth()"
  },
  {
    "objectID": "W4.html#dplyr-uses-verbs-to-manipulate",
    "href": "W4.html#dplyr-uses-verbs-to-manipulate",
    "title": "W#4 Data import, data wrangling",
    "section": "dplyr uses verbs to manipulate",
    "text": "dplyr uses verbs to manipulate\n\nselect: pick columns by name\narrange: reorder rows\nslice: pick rows using index(es)\nfilter: pick rows matching criteria\ndistinct: filter for unique rows\nmutate: add new variables\nsummarise: reduce variables to values\ngroup_by: for grouped operations\n… (many more)\n\nWhy does piping with |> work?\n\nBecause every dplyr function takes a data frame as first argument and outputs a (manipulated) data frame."
  },
  {
    "objectID": "W4.html#back-to-hotel-data-first-look",
    "href": "W4.html#back-to-hotel-data-first-look",
    "title": "W#4 Data import, data wrangling",
    "section": "Back to hotel data: First look",
    "text": "Back to hotel data: First look\n\nhotels <- read_csv(\"data/hotels.csv\")\n\n\nFirst look on variables names\n\nnames(hotels)\n\n\n\n [1] \"hotel\"                          \"is_canceled\"                   \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\"       \n\n\n\nTo download the data you can use in R: download.file(\"https://raw.githubusercontent.com/rstudio-education/datascience-box/main/course-materials/_slides/u2-d06-grammar-wrangle/data/hotels.csv\", \"data/hotels.csv\")"
  },
  {
    "objectID": "W4.html#second-look-glimpse",
    "href": "W4.html#second-look-glimpse",
    "title": "W#4 Data import, data wrangling",
    "section": "Second look glimpse",
    "text": "Second look glimpse\n\nglimpse(hotels)\n\n\n\nRows: 119,390\nColumns: 32\n$ hotel                          <chr> \"Resort Hotel\", \"Resort Hotel\", \"Resort…\n$ is_canceled                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, …\n$ lead_time                      <dbl> 342, 737, 7, 13, 14, 14, 0, 9, 85, 75, …\n$ arrival_date_year              <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 201…\n$ arrival_date_month             <chr> \"July\", \"July\", \"July\", \"July\", \"July\",…\n$ arrival_date_week_number       <dbl> 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ arrival_date_day_of_month      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ stays_in_weekend_nights        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ stays_in_week_nights           <dbl> 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, …\n$ adults                         <dbl> 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ children                       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ babies                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ meal                           <chr> \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB…\n$ country                        <chr> \"PRT\", \"PRT\", \"GBR\", \"GBR\", \"GBR\", \"GBR…\n$ market_segment                 <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ distribution_channel           <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ is_repeated_guest              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_cancellations         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_bookings_not_canceled <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ reserved_room_type             <chr> \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ assigned_room_type             <chr> \"C\", \"C\", \"C\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ booking_changes                <dbl> 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ deposit_type                   <chr> \"No Deposit\", \"No Deposit\", \"No Deposit…\n$ agent                          <chr> \"NULL\", \"NULL\", \"NULL\", \"304\", \"240\", \"…\n$ company                        <chr> \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\",…\n$ days_in_waiting_list           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ customer_type                  <chr> \"Transient\", \"Transient\", \"Transient\", …\n$ adr                            <dbl> 0.00, 0.00, 75.00, 75.00, 98.00, 98.00,…\n$ required_car_parking_spaces    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_of_special_requests      <dbl> 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 3, …\n$ reservation_status             <chr> \"Check-Out\", \"Check-Out\", \"Check-Out\", …\n$ reservation_status_date        <date> 2015-07-01, 2015-07-01, 2015-07-02, 20…"
  },
  {
    "objectID": "W4.html#select-a-sinlge-colum",
    "href": "W4.html#select-a-sinlge-colum",
    "title": "W#4 Data import, data wrangling",
    "section": "Select a sinlge colum",
    "text": "Select a sinlge colum\n\nhotels |> select(lead_time)     \n\n\n\n# A tibble: 119,390 × 1\n   lead_time\n       <dbl>\n 1       342\n 2       737\n 3         7\n 4        13\n 5        14\n 6        14\n 7         0\n 8         9\n 9        85\n10        75\n# … with 119,380 more rows\n\n\nNote: select(hotels, lead_time) is identical.\n\n\nIn hotel business, lead time is the time betweeen booking and arrival."
  },
  {
    "objectID": "W4.html#select-more-columns",
    "href": "W4.html#select-more-columns",
    "title": "W#4 Data import, data wrangling",
    "section": "Select more columns",
    "text": "Select more columns\n\nhotels |> select(hotel, lead_time)     \n\n\n\n# A tibble: 119,390 × 2\n   hotel        lead_time\n   <chr>            <dbl>\n 1 Resort Hotel       342\n 2 Resort Hotel       737\n 3 Resort Hotel         7\n 4 Resort Hotel        13\n 5 Resort Hotel        14\n 6 Resort Hotel        14\n 7 Resort Hotel         0\n 8 Resort Hotel         9\n 9 Resort Hotel        85\n10 Resort Hotel        75\n# … with 119,380 more rows\n\n\nNote that hotel is a variable, but hotels the data frame object name"
  },
  {
    "objectID": "W4.html#select-helper-starts_with",
    "href": "W4.html#select-helper-starts_with",
    "title": "W#4 Data import, data wrangling",
    "section": "Select helper starts_with",
    "text": "Select helper starts_with\n\nhotels |> select(starts_with(\"arrival\"))\n\n\n\n# A tibble: 119,390 × 4\n   arrival_date_year arrival_date_month arrival_date_week_number arrival_date_…¹\n               <dbl> <chr>                                 <dbl>           <dbl>\n 1              2015 July                                     27               1\n 2              2015 July                                     27               1\n 3              2015 July                                     27               1\n 4              2015 July                                     27               1\n 5              2015 July                                     27               1\n 6              2015 July                                     27               1\n 7              2015 July                                     27               1\n 8              2015 July                                     27               1\n 9              2015 July                                     27               1\n10              2015 July                                     27               1\n# … with 119,380 more rows, and abbreviated variable name\n#   ¹​arrival_date_day_of_month"
  },
  {
    "objectID": "W4.html#bring-columns-to-the-front",
    "href": "W4.html#bring-columns-to-the-front",
    "title": "W#4 Data import, data wrangling",
    "section": "Bring columns to the front",
    "text": "Bring columns to the front\n\nhotels |> select(hotel, market_segment, children, everything())\n\n\n\n# A tibble: 119,390 × 32\n   hotel marke…¹ child…² is_ca…³ lead_…⁴ arriv…⁵ arriv…⁶ arriv…⁷ arriv…⁸ stays…⁹\n   <chr> <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>\n 1 Reso… Direct        0       0     342    2015 July         27       1       0\n 2 Reso… Direct        0       0     737    2015 July         27       1       0\n 3 Reso… Direct        0       0       7    2015 July         27       1       0\n 4 Reso… Corpor…       0       0      13    2015 July         27       1       0\n 5 Reso… Online…       0       0      14    2015 July         27       1       0\n 6 Reso… Online…       0       0      14    2015 July         27       1       0\n 7 Reso… Direct        0       0       0    2015 July         27       1       0\n 8 Reso… Direct        0       0       9    2015 July         27       1       0\n 9 Reso… Online…       0       1      85    2015 July         27       1       0\n10 Reso… Offlin…       0       1      75    2015 July         27       1       0\n# … with 119,380 more rows, 22 more variables: stays_in_week_nights <dbl>,\n#   adults <dbl>, babies <dbl>, meal <chr>, country <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#more-select-helpers",
    "href": "W4.html#more-select-helpers",
    "title": "W#4 Data import, data wrangling",
    "section": "More select helpers",
    "text": "More select helpers\n\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\ncontains(): Contains a literal string\nnum_range(): Matches a numerical range like x01, x02, x03\none_of(): Matches variable names in a character vector\neverything(): Matches all variables\nlast_col(): Select last variable, possibly with an offset\nmatches(): Matches a regular expression (a sequence of symbols/characters expressing a string/pattern to be searched for within text)\n\n\n\nCheck details with ?one_of"
  },
  {
    "objectID": "W4.html#slice-for-certain-rows",
    "href": "W4.html#slice-for-certain-rows",
    "title": "W#4 Data import, data wrangling",
    "section": "slice for certain rows",
    "text": "slice for certain rows\n\nhotels |> slice(2:4)\n\n\n\n# A tibble: 3 × 32\n  hotel   is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n  <chr>     <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n1 Resort…       0     737    2015 July         27       1       0       0      2\n2 Resort…       0       7    2015 July         27       1       0       1      1\n3 Resort…       0      13    2015 July         27       1       0       1      1\n# … with 22 more variables: children <dbl>, babies <dbl>, meal <chr>,\n#   country <chr>, market_segment <chr>, distribution_channel <chr>,\n#   is_repeated_guest <dbl>, previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>,\n#   assigned_room_type <chr>, booking_changes <dbl>, deposit_type <chr>,\n#   agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#filter-for-rows-with-certain-criteria",
    "href": "W4.html#filter-for-rows-with-certain-criteria",
    "title": "W#4 Data import, data wrangling",
    "section": "filter for rows with certain criteria",
    "text": "filter for rows with certain criteria\n\nhotels |> filter(hotel == \"City Hotel\")\n\n\n\n# A tibble: 79,330 × 32\n   hotel  is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n   <chr>    <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 City …       0       6    2015 July         27       1       0       2      1\n 2 City …       1      88    2015 July         27       1       0       4      2\n 3 City …       1      65    2015 July         27       1       0       4      1\n 4 City …       1      92    2015 July         27       1       2       4      2\n 5 City …       1     100    2015 July         27       2       0       2      2\n 6 City …       1      79    2015 July         27       2       0       3      2\n 7 City …       0       3    2015 July         27       2       0       3      1\n 8 City …       1      63    2015 July         27       2       1       3      1\n 9 City …       1      62    2015 July         27       2       2       3      2\n10 City …       1      62    2015 July         27       2       2       3      2\n# … with 79,320 more rows, 22 more variables: children <dbl>, babies <dbl>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#filter-for-multiple-criteria",
    "href": "W4.html#filter-for-multiple-criteria",
    "title": "W#4 Data import, data wrangling",
    "section": "filter for multiple criteria",
    "text": "filter for multiple criteria\n\nhotels |> filter(\n  babies >= 1,\n  children >= 1, \n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 175 × 4\n   hotel        adults babies children\n   <chr>         <dbl>  <dbl>    <dbl>\n 1 Resort Hotel      2      1        1\n 2 Resort Hotel      2      1        1\n 3 Resort Hotel      2      1        1\n 4 Resort Hotel      2      1        1\n 5 Resort Hotel      2      1        1\n 6 Resort Hotel      2      1        1\n 7 Resort Hotel      2      1        1\n 8 Resort Hotel      2      1        2\n 9 Resort Hotel      2      1        2\n10 Resort Hotel      1      1        2\n# … with 165 more rows\n\n\nComma-separated conditions are interpreted as all these should be fulfilled.\nThis is identical to the logical AND &.\nhotels |> filter(babies >= 1 & children >= 1)"
  },
  {
    "objectID": "W4.html#filter-for-complexer-criteria",
    "href": "W4.html#filter-for-complexer-criteria",
    "title": "W#4 Data import, data wrangling",
    "section": "filter for complexer criteria",
    "text": "filter for complexer criteria\n\nhotels |> filter(\n  babies >= 1 | children >= 1\n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 9,332 × 4\n   hotel        adults babies children\n   <chr>         <dbl>  <dbl>    <dbl>\n 1 Resort Hotel      2      0        1\n 2 Resort Hotel      2      0        2\n 3 Resort Hotel      2      0        2\n 4 Resort Hotel      2      0        2\n 5 Resort Hotel      2      0        1\n 6 Resort Hotel      2      0        1\n 7 Resort Hotel      1      0        2\n 8 Resort Hotel      2      0        2\n 9 Resort Hotel      2      1        0\n10 Resort Hotel      2      1        0\n# … with 9,322 more rows\n\n\n| is the logical OR. Only one criterion needs to be fulfilled."
  },
  {
    "objectID": "W4.html#logical-operators",
    "href": "W4.html#logical-operators",
    "title": "W#4 Data import, data wrangling",
    "section": "Logical operators1",
    "text": "Logical operators1\n\n\n\noperator\ndefinition\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\nx & y\nx AND y\n\n\nx | y\nx OR y\n\n\nis.na(x)\ntest if x is NA (misssing data)\n\n\n!is.na(x)\ntest if x is not NA (not missing data)\n\n\nx %in% y\ntest if x is in y (often used for strings)\n\n\n!(x %in% y)\ntest if x is not in y\n\n\n!x\nnot x\n\n\n\nLogical is sometimes called Boolean"
  },
  {
    "objectID": "W4.html#indexing",
    "href": "W4.html#indexing",
    "title": "W#4 Data import, data wrangling",
    "section": "Indexing",
    "text": "Indexing\nSelect and filter can also be achieved by indexing.\nIn R as well as in python.\nSelect ranges of rows and columns\n\nhotels[1:3,5:7]\n\n\n\n# A tibble: 3 × 3\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month\n  <chr>                                 <dbl>                     <dbl>\n1 July                                     27                         1\n2 July                                     27                         1\n3 July                                     27                         1\n\n\nYou can use any vector (with non-overshooting indexes)\n\nhotels[c(1:3,100232),c(5:7,1)]\n\n\n\n# A tibble: 4 × 4\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month hotel   \n  <chr>                                 <dbl>                     <dbl> <chr>   \n1 July                                     27                         1 Resort …\n2 July                                     27                         1 Resort …\n3 July                                     27                         1 Resort …\n4 October                                  44                        23 City Ho…"
  },
  {
    "objectID": "W4.html#python-is-0-indexed-r-is-1-indexed",
    "href": "W4.html#python-is-0-indexed-r-is-1-indexed",
    "title": "W#4 Data import, data wrangling",
    "section": "python is 0-indexed, R is 1-indexed!",
    "text": "python is 0-indexed, R is 1-indexed!\npython: indexes go from 0 to n-1\nR: indexes go from 1 to n\nBew aware!\nThere is no correct way. For some use cases one is more natural for others the other.\nAnalog: In mathematics there is an unsettled debate if 0 is the first natural number or 1"
  },
  {
    "objectID": "W4.html#logical-indexing-with-logical-vectors",
    "href": "W4.html#logical-indexing-with-logical-vectors",
    "title": "W#4 Data import, data wrangling",
    "section": "Logical indexing with logical vectors",
    "text": "Logical indexing with logical vectors\n\n\ndata <- tibble(x = LETTERS[1:5], y = letters[6:10])\ndata\n\n\n# A tibble: 5 × 2\n  x     y    \n  <chr> <chr>\n1 A     f    \n2 B     g    \n3 C     h    \n4 D     i    \n5 E     j    \n\n\n\n\n\n\ndata[c(TRUE,FALSE,TRUE,FALSE,TRUE),c(TRUE,FALSE)]\n\n\n# A tibble: 3 × 1\n  x    \n  <chr>\n1 A    \n2 C    \n3 E"
  },
  {
    "objectID": "W4.html#logical-vectors-from-conditional-statements",
    "href": "W4.html#logical-vectors-from-conditional-statements",
    "title": "W#4 Data import, data wrangling",
    "section": "Logical vectors from conditional statements",
    "text": "Logical vectors from conditional statements\n\n\ndata$x\n\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n\n\n\n\n\ndata$x %in% c(\"C\",\"E\")\n\n\n[1] FALSE FALSE  TRUE FALSE  TRUE\n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\"),]\n\n\n# A tibble: 2 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 E     j    \n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\") | \n       data$y %in% c(\"h\",\"i\"),]\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j    \n\n\n\n\n\n\n\ndata |> \n  filter(\n    x %in% c(\"C\",\"E\") | y %in% c(\"h\",\"i\")\n    )\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j"
  },
  {
    "objectID": "W4.html#unique-combinations-arranging",
    "href": "W4.html#unique-combinations-arranging",
    "title": "W#4 Data import, data wrangling",
    "section": "Unique combinations, arranging",
    "text": "Unique combinations, arranging\ndistinct and arrange\n\nhotels |> \n  distinct(hotel, market_segment) |> \n  arrange(hotel, market_segment)\n\n\n\n# A tibble: 14 × 2\n   hotel        market_segment\n   <chr>        <chr>         \n 1 City Hotel   Aviation      \n 2 City Hotel   Complementary \n 3 City Hotel   Corporate     \n 4 City Hotel   Direct        \n 5 City Hotel   Groups        \n 6 City Hotel   Offline TA/TO \n 7 City Hotel   Online TA     \n 8 City Hotel   Undefined     \n 9 Resort Hotel Complementary \n10 Resort Hotel Corporate     \n11 Resort Hotel Direct        \n12 Resort Hotel Groups        \n13 Resort Hotel Offline TA/TO \n14 Resort Hotel Online TA"
  },
  {
    "objectID": "W4.html#counting",
    "href": "W4.html#counting",
    "title": "W#4 Data import, data wrangling",
    "section": "Counting",
    "text": "Counting\ncount\n\nhotels |> \n  count(hotel, market_segment) |>      # This produces a new variable n\n  arrange(n)\n\n\n\n# A tibble: 14 × 3\n   hotel        market_segment     n\n   <chr>        <chr>          <int>\n 1 City Hotel   Undefined          2\n 2 Resort Hotel Complementary    201\n 3 City Hotel   Aviation         237\n 4 City Hotel   Complementary    542\n 5 Resort Hotel Corporate       2309\n 6 City Hotel   Corporate       2986\n 7 Resort Hotel Groups          5836\n 8 City Hotel   Direct          6093\n 9 Resort Hotel Direct          6513\n10 Resort Hotel Offline TA/TO   7472\n11 City Hotel   Groups         13975\n12 City Hotel   Offline TA/TO  16747\n13 Resort Hotel Online TA      17729\n14 City Hotel   Online TA      38748"
  },
  {
    "objectID": "W4.html#create-a-new-variable-with-mutate",
    "href": "W4.html#create-a-new-variable-with-mutate",
    "title": "W#4 Data import, data wrangling",
    "section": "Create a new variable with mutate",
    "text": "Create a new variable with mutate\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  select(children, babies, little_ones) |>\n  arrange(desc(little_ones)) # This sorts in descending order. See the big thing!\n\n\n\n# A tibble: 119,390 × 3\n   children babies little_ones\n      <dbl>  <dbl>       <dbl>\n 1       10      0          10\n 2        0     10          10\n 3        0      9           9\n 4        2      1           3\n 5        2      1           3\n 6        2      1           3\n 7        3      0           3\n 8        2      1           3\n 9        2      1           3\n10        3      0           3\n# … with 119,380 more rows"
  },
  {
    "objectID": "W4.html#more-mutating",
    "href": "W4.html#more-mutating",
    "title": "W#4 Data import, data wrangling",
    "section": "More mutating",
    "text": "More mutating\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  count(hotel, little_ones) |>\n  mutate(prop = n / sum(n))\n\n\n\n# A tibble: 12 × 4\n   hotel        little_ones     n       prop\n   <chr>              <dbl> <int>      <dbl>\n 1 City Hotel             0 73923 0.619     \n 2 City Hotel             1  3263 0.0273    \n 3 City Hotel             2  2056 0.0172    \n 4 City Hotel             3    82 0.000687  \n 5 City Hotel             9     1 0.00000838\n 6 City Hotel            10     1 0.00000838\n 7 City Hotel            NA     4 0.0000335 \n 8 Resort Hotel           0 36131 0.303     \n 9 Resort Hotel           1  2183 0.0183    \n10 Resort Hotel           2  1716 0.0144    \n11 Resort Hotel           3    29 0.000243  \n12 Resort Hotel          10     1 0.00000838"
  },
  {
    "objectID": "W4.html#summarizing",
    "href": "W4.html#summarizing",
    "title": "W#4 Data import, data wrangling",
    "section": "Summarizing",
    "text": "Summarizing\n\nhotels |>\n  summarize(mean_adr = mean(adr))\n\n\n\n# A tibble: 1 × 1\n  mean_adr\n     <dbl>\n1     102.\n\n\n\nThat shrinks the data frame to one row!\nDon’t forget to name the new variable (here mean_adr)\nYou can use any function you can apply to a vector!\n(Sometimes you may need to write your own one.)\n\n\n\nIn hoteling, ADR is the average daily rate, the average daily rental income per paid occupied room. A performce indicator."
  },
  {
    "objectID": "W4.html#grouped-operations",
    "href": "W4.html#grouped-operations",
    "title": "W#4 Data import, data wrangling",
    "section": "Grouped operations",
    "text": "Grouped operations\n\nhotels |>\n  group_by(hotel) |>\n  summarise(mean_adr = mean(adr))\n\n\n\n# A tibble: 2 × 2\n  hotel        mean_adr\n  <chr>           <dbl>\n1 City Hotel      105. \n2 Resort Hotel     95.0\n\n\nLook at the grouping attributes:\n\nhotels |>\n  group_by(hotel)\n\n\n\n# A tibble: 119,390 × 32\n# Groups:   hotel [2]\n   hotel  is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n   <chr>    <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Resor…       0     342    2015 July         27       1       0       0      2\n 2 Resor…       0     737    2015 July         27       1       0       0      2\n 3 Resor…       0       7    2015 July         27       1       0       1      1\n 4 Resor…       0      13    2015 July         27       1       0       1      1\n 5 Resor…       0      14    2015 July         27       1       0       2      2\n 6 Resor…       0      14    2015 July         27       1       0       2      2\n 7 Resor…       0       0    2015 July         27       1       0       2      2\n 8 Resor…       0       9    2015 July         27       1       0       2      2\n 9 Resor…       1      85    2015 July         27       1       0       3      2\n10 Resor…       1      75    2015 July         27       1       0       3      2\n# … with 119,380 more rows, 22 more variables: children <dbl>, babies <dbl>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#grouping-summarizing-visualizing",
    "href": "W4.html#grouping-summarizing-visualizing",
    "title": "W#4 Data import, data wrangling",
    "section": "Grouping, summarizing, visualizing",
    "text": "Grouping, summarizing, visualizing\n\nhotels |>\n  group_by(hotel, arrival_date_week_number) |>\n  summarise(mean_adr = mean(adr)) |> \n  ggplot(aes(x = arrival_date_week_number, y = mean_adr, color = hotel)) +\n  geom_line()"
  },
  {
    "objectID": "W4.html#resources",
    "href": "W4.html#resources",
    "title": "W#4 Data import, data wrangling",
    "section": "Resources",
    "text": "Resources\n\nFor systemic understanding: Learning resources linked in the syllabus\n\nR for Data Science\n\nChapters 3, 5, 9 (short), 10 (short), 11\n\nCorresponding chapters in Python Data Science Handbook\n\nFor quick overview to get inpiration\n\nCheatsheets (find some in RStudio -> Help, other by google)\n\nggplot2 Cheatsheet\ndplyr Cheatsheet\n\n\nFor detailed help with a function\n\nHelp file of the function ?FUNCTION-NAME, or search box in Help tab\nReference page on the package webpage"
  },
  {
    "objectID": "W4.html#questions-and-advice-for-the-homework-of-starting-projects",
    "href": "W4.html#questions-and-advice-for-the-homework-of-starting-projects",
    "title": "W#4 Data import, data wrangling",
    "section": "Questions and advice for the homework of starting projects",
    "text": "Questions and advice for the homework of starting projects\n\nData search\n\nAny insights about corona data?\n\nStarting a new quarto markdown document\n\nWhat to write into the YAML?\n\nData import\n\nExpect that some customization is needed!\n\nFirst graph\n\nYou are encouraged to play further!\nThese projects can be the seed for you Data Science Tools module project.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html",
    "href": "hw-instructions/hw-04-instr.html",
    "title": "Homework 04",
    "section": "",
    "text": "Important\n\n\n\nHomework 04 is due Sunday, Nov 6. When you get stuck, you are encouraged to push intermediate steps before you contact us.\nProject teams should be formed by Oct 30.\nIn this homework you are to continue to do data analysis of the data with the data about corona in R and the European Social Survey (ESS) in python."
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html#corona",
    "href": "hw-instructions/hw-04-instr.html#corona",
    "title": "Homework 04",
    "section": "1 Corona",
    "text": "1 Corona\nContinue to work on your repository corona-ind-USERNAME.\nContinue to work on the Quarto-document “Corona_Analysis.qmd” from Homework 03 and add new headlines and new code chunks for each new question.\n\n1.1 Question 5: Do the cumulative cases reported by the WHO for Germany, France, India and the country of your choice coincide with the cumulative sum of new cases?\n\nFilter the who dataset for the four countries. Group by these countries and compute a new variable called Total_cases as the cumulative sum (cumsum) of the new cases for each each country.\nNow, test if this variable coincides with the variable Cumulative_cases which is already present in the dataset. To that end, compute a new variable with the difference of the two time lines and check if the difference is zero in every time step. (You can use count on the the difference variable to count different values.)\nWrite down the answer to the question.\n\n\n\n1.2 Question 6: What can the visualization of the daily change of new cases in Germany tell us about the evolution of the pandemic?\n\n\n\n\n\n\nTip\n\n\n\nDownload the two datasets (who and owid) anew to get the most recent values.\n\n\n\nUse the dataset of the WHO, filter for Germany and all dates after August 15, 2022.\nMake a new variable New_cases_smoothed where you smooth New_cases with a 7-day lagged window.\nCreate another variable called Change which is the difference of in the smoothed new cases with the day before.\nDevelop two ggplots (see below), one for the smoothed new cases and one for the change. Once you have the code for each plot save each plot as a ggplot object (e.g., g1 and g2) and show them in your rendered document with new cases above the change. (Hint: Use the package patchwork and g1 / g2 to show the two plots.)\n\nUse a bar-chart for both plots. Use geom_col.\nFor the plot of the change make the bars for negative numbers filled with blue and the bars for positive numbers filled with red. The way to do it is to add two geom_cols to the same plot. Both have different y-aesthetics but both based on the Change variable. In one the negative values are set to zero in the other the positive values. For example you can create such a variable like ifelse(Change >= 0, Change, 0).\nLet us make these plots look nice. Use + labs() to create meaningful axis labels. Use scale_x_date to make the x-axis-ticks look nice. For example, use date_breaks = ... to specify the main labelled ticks, date_minor_breaks = ... for smaller unlabelled ticks, and use date_labels = ... to make the labels shorter by omitting the year in the dates. You have to look up the specification in the help of scale_x_date.\n\nDescribe what the blue and the red regions in the change plot tell us about the recent evolution of corona cases in Germany.\nFrom the total case, the new cases and the change of new cases. Which timeline is the derivative an the anti-derivative of which?\n\n\n\n1.3 Question 7: Which variable explains more variance of total deaths per million, the human development index (HDI) or the median age?\nUse the owid dataset and filter the rows with data Oct 15, 2022 only and with a valid continent (this should be 223 rows).\nEstimate two linear model with this dataset. Both have total_death_per_million as dependent variable.\n\nUse human_development_index as independent variable and save the model object as mod_hdi.\nUse median_age as independent variable and save the model object as mod_age.\n\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html. Answer the question.\n\n\n1.4 Question 8: How do the two models improve when the continent is added as an interaction effect?\nEstimate two more linear models, mod_hdicont and mod_agecont, by adding continent as an interaction effect to the two models from Question 8 (human_development_index * continent and median_age * continent).\n\nPrint the coefficients of mod_agecont in tidy form and interpret the coefficient of the intercept, the main effects, and the interaction effects.\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html.\n\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html. Answer the question. In which case is the improvement stronger? Why is the improvement different?\n\n\n1.5 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html#european-social-survey",
    "href": "hw-instructions/hw-04-instr.html#european-social-survey",
    "title": "Homework 04",
    "section": "2 European Social Survey",
    "text": "2 European Social Survey\nContinue to work on your repository ess-ind-USERNAME. You have to look ESS Data Documentation https://ess-search.nsd.no/.\nContinue to work on the Quarto-document “ESS-analysis.qmd” (alternatively “ESS-analysis.ipynb” if you can render html-files from it) from Homework 03 and add new headlines and new code chunks for each new question.\n\n2.1 Question 6: What are the correlations between life satisfaction, trust in the police, religiosity, emotional attachment to Europe, and social activity?\nFilter the ess dataset for essround 9 and select the variables stflife, trstplc, rlgdgr, atcherp, and sclact.\nCompute the correlation matrix and visualize correlation coefficients with color.\nFor calculating the correlation of a dataframe use .corr() which returns the correlation matrix. Visualizing a value with a certain color is called heatmap. Use your created correlation matrix in the seaborn function sns.heatmap(). Look up the function and vary the parameters for better visibility.\nExplain the two highest and two lowest correlations.\n\n\n2.2 Question 7: Which variables can explain the variance of life satisfaction in a linear model how well?\nWith the same dataset compute a linear model where life satisfaction is explained as a linear combination of all other four variables.\nPrint and interpret the coefficients. (Look up and report the variable descriptions and their scales at https://ess-search.nsd.no for this purpose!)\nThere are many python packages that include linear models. For similarity with R we use the (statsmodel package)[https://www.statsmodels.org/stable/index.html].\nimport statsmodel.api as sm\nThe default model within statsmodel does not have an intercept defined. Therefore, add a constant to the predictors with sm.add_constant(). Now apply the sm.OLS() to your data with the predictors and dependent variable inside the function. The results of your regression are summarized by applying and printing the results.summary() function.\nWhich percentage of the variance in life satisfaction can be explained with the model?\n\n\n2.3 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html#your-own-project",
    "href": "hw-instructions/hw-04-instr.html#your-own-project",
    "title": "Homework 04",
    "section": "3 Your own project",
    "text": "3 Your own project\nThe assessment of the Data Science Tools module will be based on a team project report.\n\n3.1 A project report in a nutshell\n\nYou pick a dataset,\ndo some interesting question-driven data analysis with it,\nwrite up well structured and nicely formatted report about the analysis, and\npresent it at the end of the semester.\n\nMore details will come soon.\n\n\n3.2 The tasks for this Homework\n\nTeam formation such that a repository at https://github.com/orgs/JU-F22-MDSSB-MET-01 can be provided.\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n\n\n\n\n\n\nTask 1: Form project teams by Sunday, Oct 30.\n\n\n\n\nTeams should have two members.\nSend your names to Jan Lorenz by email or via Teams.\nTeams of three (or more) or individual projects can be permitted on requests providing a reason.\n\n\n\n\n\n\n\n\n\nTask 2: Start a document and list data source/topic and draft questions\n\n\n\n\nYou need your repository in https://github.com/orgs/JU-F22-MDSSB-MET-01 to submit it. Please form your team early and inform us such that it can be created.\nStart a quarto markdown document in the repository. The text can be brief at this stage.\n\n\n\n\n\n3.3 Where to find data?\nThis up to you. Some entry points are:\n\nGoogle Dataset Search https://datasetsearch.research.google.com/\n\nTidy Tuesday https://github.com/rfordatascience/tidytuesday\nAwesome public datasets https://github.com/awesomedata/awesome-public-datasets\nKaggle datasets https://www.kaggle.com/datasets\nJan and Martin can provide ideas for data about polarization in Europe (based on ESS), a voting advice application for German elections, census data about Bremen’s districts.\n\nIf you have a specific topic in mind which can be approached with data but haven’t found a good dataset yet, you can also provide the topic and the questions which interest you only at this stage."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html",
    "href": "hw-instructions/hw-01-instr.html",
    "title": "Homework 01",
    "section": "",
    "text": "Note\n\n\n\nUpdated Sep 7, 2022 to prevent problems with cloning your repository.\nSep 9, 2022: Updated the individual repositories on with an updated dataset. All which have already cloned it should do a “Pull” in the Git tab of RStudio.\nThe goal of this assignment is to introduce you to R, RStudio, Git, and GitHub, which you’ll be using throughout the course both to learn the data science concepts discussed in the course and to analyze real data and come to informed conclusions."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#prerequisites",
    "href": "hw-instructions/hw-01-instr.html#prerequisites",
    "title": "Homework 01",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\nThis assignment assumes that you have reviewed the lectures of week 1 and checked all boxes on the Prerequisites Checklist."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#terminology",
    "href": "hw-instructions/hw-01-instr.html#terminology",
    "title": "Homework 01",
    "section": "1.2 Terminology",
    "text": "1.2 Terminology\nWe’ve already thrown around a few new terms, so let’s define them before we proceed.\n\nR and python: Names of the programming language we will be using throughout the course.\nRStudio: An integrated development environment developed for R, which can also work with python. In other words, a convenient interface for writing and running code.\nGit: A version control system.\nGitHub: A web platform for hosting version controlled files and facilitating collaboration among users.\nRepository: A Git repository contains all of your project’s files and stores each file’s revision history.\n\nIt’s common to refer to a repository as a repo.\n\nIn this course, each assignment you work on will be contained in a Git repo.\nFor individual assignments, only you will have access to the repo. For team assignments, all team members will have access to a single repo where they work collaboratively.\nAll repos associated with this course are housed in the course GitHub organization. The organization is set up such that students can only see repos they have access to, but the course instructors can see all of them."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#starting-slowly-step-by-step",
    "href": "hw-instructions/hw-01-instr.html#starting-slowly-step-by-step",
    "title": "Homework 01",
    "section": "1.3 Starting slowly step by step",
    "text": "1.3 Starting slowly step by step\nAs the course progresses, you are encouraged to explore beyond what the assignments dictate; a willingness to experiment will make you a much better programmer! Before we get to that stage, however, you need to build some basic fluency in the tools and the workflow we use. First, we will explore the fundamental building blocks of all of these tools.\nBefore you can get started with the analysis, you need to make sure you:\n\nhave a GitHub account\nare a member of the course GitHub organization https://github.com/JU-F22-MDSSB-MET-01\nhave the needed software stack installation on your local machine (see the Prerequisites Checklist in the slides of the Week 1 lectures.)\n\nIf you failed to confirm any of these, it means you have not yet completed the prerequisites for this assignment. Please go back to Prerequisites and complete them before continuing the assignment."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-0.-authenticate-git-to-access-your-github-content",
    "href": "hw-instructions/hw-01-instr.html#step-0.-authenticate-git-to-access-your-github-content",
    "title": "Homework 01",
    "section": "2.1 Step 0. Authenticate git to access your GitHub content",
    "text": "2.1 Step 0. Authenticate git to access your GitHub content\nBefore you can clone your repository you need to tell GitHub that you are authorized to do this, and to that end you need to make a Personal Access Token (PAT) in your GitHub account and make this available to git and RStudio on your local machine.\nThere are several ways to do this (e.g. from the command line) but as we will use RStudio anyway, we can use a convenient way provided there.\nRead more about PATs and how to use them in “Happy Git with R” Chapter 9 https://happygitwithr.com/https-pat.html (in particular the TL;DR which describes what we use).\nOpen RStudio and install the packages usethis and gitcreds if you haven’t done already: Go to the “Console” pane at the bottom left. Type in\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nand hit Enter. Now the packages should be installed.\nNow, use two commands. Copy them to the console and click Enter:\nusethis::create_github_token()\nThis opens http://github.com and you may need to log in. Then you can make the PAT (read more details in “Happy Git with R”). For today, you can go the fast way and do not think about the options and click “Generate token”. Use the clipboard icon 📋 to copy the PAT. Go back to RStudio and do in the console:\ngitcreds::gitcreds_set()\nIn the dialog in the console paste your PAT from the clipboard and press Enter. That should be it and you do not need to repeat these steps until the PAT expires. (If the PAT expires you have to make a new one in the same way.)"
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-1.-get-url-of-repo-to-be-cloned",
    "href": "hw-instructions/hw-01-instr.html#step-1.-get-url-of-repo-to-be-cloned",
    "title": "Homework 01",
    "section": "2.2 Step 1. Get URL of repo to be cloned",
    "text": "2.2 Step 1. Get URL of repo to be cloned\nOn GitHub https://github.com/JU-F22-MDSSB-MET-01, open your repository for Homework 1.\n\nOn GitHub, click on the green Code button, select HTTPS (this might already be selected by default, and if it is, you’ll see the text Use Git or checkout with SVN using the web URL). Click on the clipboard icon 📋 to copy the repo URL."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-2.-go-to-rstudio",
    "href": "hw-instructions/hw-01-instr.html#step-2.-go-to-rstudio",
    "title": "Homework 01",
    "section": "2.3 Step 2. Go to RStudio",
    "text": "2.3 Step 2. Go to RStudio\nGo to your RStudio. Select “New Project…” either from the File menu or from the special project menu on the top right of the RStudio window.\n\nIn the New Project Wizard, click on “Version Control” and then “Git”.\n\n\n\n\n\n\nImportant\n\n\n\nIf “Version Control” or “Git” is not available in your RStudio, then either you haven’t installed git on you computer or your RStudio installation has not recognized it properly. In a correct installation RStudio would recognize git on your machine when started and makes the options available automatically. You have to solve this issue first to continue.\n\n\nThen paste the repositories URL (which should still be in your clipboard) into the “Repository URL:” field. Leave directory name as it is automatically chosen, but make sure that you create the directory in a the folder where you want to store the course material on your computer via the “Browse …” button.\nWhen you click “Create Project”\n\ngit will create a new directory in the folder, copies all the files from github to it, and initializes your git repository locally\nRStudio will switch to that the new project"
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-1.-update-the-yaml",
    "href": "hw-instructions/hw-01-instr.html#step-1.-update-the-yaml",
    "title": "Homework 01",
    "section": "4.1 Step 1. Update the YAML",
    "text": "4.1 Step 1. Update the YAML\nOpen the quarto (qmd) file in your project, change the author name to your name, and “Render” the document.\n This calls quarto, and quarto renders the document. In this case, that means, quarto creates a new file “hw-01-R.md” in the gfm format as specified in the YAML. GFM stands for GitHub Flavored Markdown which is a markdown document which can be nicely shown on https://github.com.\nWhen the file was rendered successfully, RStudio shows it in the “Viewer” pane at the bottom right. At the same place you can look in the “Files” pane you can check if the file is there.\n\nIf you do not find the “Render” button in your RStudio installation, then either quarto is not installed or RStudio has not recognized. You have to fix this issue first before you can continue. Another source of error while rendering could be that you haven’t installed the tidyverse package."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-2-commit",
    "href": "hw-instructions/hw-01-instr.html#step-2-commit",
    "title": "Homework 01",
    "section": "4.2 Step 2: Commit",
    "text": "4.2 Step 2: Commit\nGo to the Git pane in your RStudio (top right corner).\nYou should see that your qmd (quarto) file and its output, your md file (Markdown), are listed there as recently changed files.\nNext, click on Diff. This will pop open a new window that shows you the difference between the last committed state of the document and its current state that includes your changes. (Click on the file “hw-01-R.qmd”.) If you’re happy with these changes, click on the checkboxes of all files in the list, and type “Update author name” in the Commit message box and hit Commit and then close the window.\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we may tell you exactly when to commit and what commit message to use. As the semester progresses we will let you make these decisions."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-3.-push",
    "href": "hw-instructions/hw-01-instr.html#step-3.-push",
    "title": "Homework 01",
    "section": "4.3 Step 3. Push",
    "text": "4.3 Step 3. Push\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course instructors (your repos in this course are private to you and us, only).\nGo the Git pane and click on Push.\nThought exercise: Which of the steps “updating the YAML”, “committing”, and “pushing” involves talking to GitHub?1"
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#check-what-you-did",
    "href": "hw-instructions/hw-01-instr.html#check-what-you-did",
    "title": "Homework 01",
    "section": "4.4 Check what you did",
    "text": "4.4 Check what you did\nGo to your repository on https://github.com/JU-F22-MDSSB-MET-01 and click on the file “hw-01-R.md”. You should see a nicely rendered version with your name in the header."
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html",
    "href": "hw-instructions/hw-02-instr.html",
    "title": "Homework 02",
    "section": "",
    "text": "The goals of this assignment are\nTo that end, we provide three repositories with starter documents which you should clone and push your solutions to:"
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html#data-overview",
    "href": "hw-instructions/hw-02-instr.html#data-overview",
    "title": "Homework 02",
    "section": "1.1 Data overview",
    "text": "1.1 Data overview\n\n\n\n\n\n\nNote\n\n\n\nFirst do the following exercises in R in hw-02-R.qmd. Afterwards go through hw-02-py.qmd. This documents contains some tips how to do the same operations in python. Exercises which are not about coding are omitted there.\n\n\n\nRun the first chunk packages-data such that the lines are executed in the console to load the tidyverse functions and the ny3cflight13 data. See which dataframes are available in the Environment tab. The environment should be empty, but you can select “package:nycflights13” instead of “Global Environment” and you see values markes as <Promise>. Once you click on one or call it in the Console you see basic information there.\nReplace the “??” in the text with the actual numbers. Don’t write the numbers your self, but write inline code in which you output the number of rows of each data frame. In the document this already done for the airlines. Render the document to see if it works. More information https://quarto.org/docs/get-started/computations/rstudio.html#inline-code\nExplore the datasets. Write View(flights) in the Console to see the data as a spreadsheet. Write glimpse(flight) to see an overview of all variables, their types and first values. As these dataframes come from a package there is also a Help page for each dataframe which you access with ?flights. There you find short descriptions about each variable.\n\nWrite a nicely formatted short description about the variables origin, arr_delay, and dep_delay from flights and engine and seat from planes."
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html#data-visualizations",
    "href": "hw-instructions/hw-02-instr.html#data-visualizations",
    "title": "Homework 02",
    "section": "1.2 Data visualizations",
    "text": "1.2 Data visualizations\n\nWe first want to know the distribution of values of the categorical variable origin in flights. To that end, make a bar chart. Read the Help ?geom_bar and decide if you need to use geom_bar or geom_col. You can use the template below. Write your solution in the chunk flightsorigin. Test your line by sending it to the Console (with Ctrl + Enter). Once you are satisfied, render the document, commit the changes with message “First Visualization!” and push it. In the following, you can commit and push when you want. (Note, that we can provide help directly in your repo when you commit and push before.) This is the template.\n\nggplot(data = __________, mapping = aes(x = ________)) + \n  geom_TOSELECT()\n\nNow, we want to know the distribution of values of the numerical variable distance in flights. A common visualization is a histogram. Use geom-histogram with the same template, write the solution in the chunk flightsdistance, and test it. Notice, the red comment in the console. It advises to specify a binwidth. Test binwidth = 5, binwidth = 50, and binwidth = 500 in geom_histogram, notice the difference (consult ?geom_histogram) for details, and decide which shows the distribution best.\nIn chunk distributions you see two ways to visualize the distribution of the number of seats in airplanes - points for each observation and a boxplot. (Read ?geom_boxplot for more information). Note, that there are three ggplot objects (g1, g2, g3) which are shown combined with g1 + g2 + g3 (using the patchwork package). Make the empty g3 into a vertical histogram for the same data following the exercise before. Hint: For the vertical histogram assign distance to the y aesthetic and leave out the x aesthetic. Think about the advantages and disadvantages of each visualization.\nNow, we make the first plot which visualizes two variables, the categorical variable engine, and the numerical variable seats in airplanes. Use the template and with aesthetics x and y and the geom_boxplot and put the solution into the chunk engine-seats\nTwo numerical variables can be visualized with a scatter plot using geom_point and the aesthetics x and y. Let us look at dep_delay and arr_delay of flights. Warning: The flights is very large! So, do not use flights in data = _____ but a random sample of 10,000 flights sample_n(flights, 10000). Now, let us add information about the categorical variable origin and assign it to the color aesthetic. Put your solution into the chunk delays. Test your solution several times and observe the changes in the visualization because of the random sampling. In which region are the changes most substantial? (This is a qualitative judgment.)\nFinally, let us visualize the location of airports as points at their longitude and latitude (look up the variable names) and color them with the timezone tzone they are in. Put the solution into the chunk airportlocations."
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html#data-wrangling-pipes-and-visualization",
    "href": "hw-instructions/hw-02-instr.html#data-wrangling-pipes-and-visualization",
    "title": "Homework 02",
    "section": "1.3 Data wrangling, pipes, and visualization",
    "text": "1.3 Data wrangling, pipes, and visualization\nIn the following, you have to solve some data wrangling tasks. For data wrangling, the usage of the pipe, or a chain of pipes, is convenient. You can also use the pipe to finish with a visualization.\n\nPut the code snippet below into the chunk flightsaveragespeed and test it. The mutate line makes a new variable called speed which is the distance of the flight divided by the time in the air. The select line selects variables from the dataset. In this case, it selects air_time as the first, distance as the second, and the new speed as the third variable. All other variables are dropped.\nThe values in the new speed variable do not look like speeds of airplanes in km/h. Why? Because they are in miles/minute which we know from the variable descriptions. Modify the equation in the mutate command such that the values are in km/h. To that end, you have to divide air time by 60 and multiply distance by a certain factor. Look up the factor. Be careful with the order of mathematical operations and maybe use brackets (). Test your computation. Are the speed values reasonable?\nNow, make a histogram of speed. Add another pipe after the select statement and write ggplot(mapping = aes()) in the next line. Note, that you should not put data = flights into the argument of ggplot()! It is the mission of the pipe to do this. Fill out the aes() command accordingly, and add the geom for a histogram.\n\nflights |> \n  mutate(speed = distance / air_time ) |> \n  select(air_time, distance, speed)\n\nPractice filter operations, which subsets certain observations of a dataframe. Write a line which filters the flights which\n\n\nhad an arrival delay of two or more hours\nflew to Houston (IAH or HOU)\narrived more than two hours late, but didn’t leave late\nstarted with a delay of at least an hour, but made up over 30 minutes in flight. Put all four lines into the chunk filtering.\n\n\nAnother common operation is summarizing data. Put the code below into the chunk summarizing and test it. You see the average delay at departure. (See ?mean to learn what na.rm = TRUE is doing). Now, we want to know the average delay at departue for each of the three airports of origin. This is done with a group_by applied to the flights. Find out how and modify the code in the chunk accordingly.\n\nflights |> \n  summarize(mean_dep_deplay = mean(dep_delay, na.rm = TRUE))\n\nFinally, look at the plot of airportlocations. The airport locations show the shape of the United States of America, but there are four airports on the right hand side which do not fit that pattern. Filter airports such that you only see these four airport. Write your solution into the chunk stangeairportlocations. Check with internet research where these airports are located. Why are the locations from the data as they are? List your hypotheses for each airport under the chunk."
  },
  {
    "objectID": "hw-instructions/hw-02-py-instruction.html",
    "href": "hw-instructions/hw-02-py-instruction.html",
    "title": "Additional python instruction Homework 02",
    "section": "",
    "text": "Read and do the homework in R first. There you find more instructions and exercises.\nYou can also copy each chunk into a notebook cell within jupyter notebook template file and work with the .ipynb file\nquarto can also render .ipynb files. (Actually quarto converts .qmd files into .ipynb files and renders them afterwards)\nThere is also a .ipynb Template in the python Codebase provided.\n\nThis analysis works with datasets in the package nycflights13 about flights, specifically a sample of domestic flights that departed from the three major New York City airport in 2013.\n\n1 Data overview\n#| label: packages-data\n#| message: false  # We do not want to see the common tidyverse message in our document\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nycflights13 import flights, planes,airports\nRun the first chunk to import the packages, by running the command as:\n\nfrom nycflights import …\n\nYou can deliberately choose which dataset you want to import from nycflights13. The nameing convension seems to be identical with the dataset in R.\n#| output: false\nflights.info()\nExplore the datasets. If you are running the code in VSCode you can see the variable in the “JUPYTER: VARIABLES” window. Write flights.info() in the Console or in a chunk and remove it afterwards to see an overview of all variables, their types. (Write a short note about the differences to the R glimpse command)\n\n\n2 Whats the differences between R:glimpse and pandas:info\n…\n\n\n3 Data visualization\n\nWe first want to know the distribution of values of the categorical variable origin in flights. To that end, make a bar chart. Use the seaborn command displot.\n\n\nsns.displot(data=flights,x = .. y= ..,height=.., aspect=..)\ngeom_bar or geom_col is just a variation between the usage of variable x and y. Use height=.., aspect=.. to adjust the size of the plot.\n\nNow, we want to know the distribution of values of the numerical variable distance in flights. A common visualization is a histogram. Use sns.histplot with the same template, write the solution in the chunk flightsdistance, and test it. The command binwidth is equivalent to bin-width in R. Use it inside the seaborn function.\n\nfig,ax = plt.subplots(figsize=(12,6)) is used to vary the size of the plot.\nfig,ax = plt.subplots(figsize=(12,6))\nsns.histplot(data=flights,x='distance',ax=ax, ...)\nIn chunk distributions you see two ways to visualize the distribution of the number of seats in airplanes - points for each observation and a boxplot. Different to ggplot you define the output first. With ncols= .. ,nrows =.. you specify the layout and access it with  ax  similar to an array.\n#| label: distributions\nfig,ax = plt.subplots(nrows=1,ncols=3,figsize=(8,6))\n#fig,[ax1,ax2,ax3] = plt.subplots(nrows=1,ncols=3,figsize=(24,12)) ## Or define the array elements\nax[0].plot(np.zeros(len(planes)),planes['seats'],'k.')\nsns.boxplot(data=planes,y='seats',\n    boxprops={'facecolor':'None'},\n    ax=ax[1])\n#sns.boxplot(data=planes\n    #boxprops={'facecolor':'None'},\n    #ax=..)\n    # )\nNow, we make the first plot in python which visualizes two variables, the categorical variable engine, and the numerical variable seats in airplanes\n#| label: engine-seats\nfig,ax = plt.subplots(figsize=(24,12))\n\nsns.boxplot(data=planes,x = .. , y = ..\n    boxprops={'facecolor':'None'},\n    ax=ax)\nSo, do not use flights in data = .. but a random sample of 10,000 flights flights.sample(n=10000). To visualize a scatterplot use sns.scatterplot(). To add an information about the categorical variable origin and assign it to the color aesthetica variation, include  ..,hue= .. , in the scatterplot command.\n#| label: delays\nfig,ax = plt.subplots(figsize=(12,10))\nsns.scatterplot(data= flights.., x='dep_delay',y='arr_delay',..,ax=ax)\nFinally, let us visualize the location of airports as points at their longitude and latitude (look up the variable names) and color them with the timezone tzone they are in. Put the solution into the chunk airportlocations.\n#| label: airportlocations\nfig,ax = plt.subplots(figsize=(12,10))\nsns.scatterplot()\n\n\n4 Data Wrangling\nIn the following, you have to solve some data wrangling tasks. For data wrangling, you use different commands onto a DataFrame devided by a “.” . There is not equivalent to “pipes” in the pandas or python programming language. Here the easiest way is to creat a new column in our DataFrame with the average speed in it. The pandas specific .div() command allows for a fast division of two columns. Also modify the equation such that the values are in km/h.\n#| label: flightsaveragespeed\nflights['speed'] = flights['distance'].div(flights['air_time'])\nThe filter function is not available in python. IMHO the .loc function is the most comprehensive function the filter a DataFrame by using logical expressions.\nPractice .loc operations, which subsets certain observations of a dataframe. Write a line which filters the flights which\n\nhad an arrival delay of two or more hours\nflew to Houston (IAH or HOU)\narrived more than two hours late, but didn’t leave late\nstarted with a delay of at least an hour, but made up over 30 minutes in flight.\n\nuse individual lines for each filter. In the end use .describe() on the flights DataFrame. Now, we want to know the average delay at departue for each of the three airports of origin. This is done with a groupby(..) applied to the filtered DataFrame. additionally apply mean() and select the correct column. Find out how and modify the code in the chunk accordingly.\n#| label: flightsaveragespeed\nflights = flights.loc[flights['arr_delay']>=120] # 120 minutes\nNOTE: If you do not choose a new variable name for the DataFrame, data is lost, until you reload the complete DataFrame\nFinally, look at the plot of airportlocations. The airport locations show the shape of the United States of America, but there are four airports on the right hand side which do not fit that pattern. Filter airports such that you only see these four airport. Write your solution into the chunk stangeairportlocations. Check with internet research where these airports are located. Why are the locations from the data as they are? List your hypotheses for each airport under the chunk.\n#| label: strangeairportlocations\nplt.figure(figsize=(20,6))\nplt.plot( airports[..],airports[..] ,'.')"
  },
  {
    "objectID": "hw-instructions/hw-03-py-instruction.html",
    "href": "hw-instructions/hw-03-py-instruction.html",
    "title": "Additional Python Instruction Homework 03",
    "section": "",
    "text": "First we import all the relevant packages. We also need the pycountry package which we can simply install via pip in the command line.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pycountry\nThe next step is to import the Data.\ndf = pd.read_csv(...)\nIf we take a look at the countries within the dataset and we try to convert it via the pycounty package we run into an error. This error is a result of the Countrycode 'XK which refers to Kosovo but is not officially included as ISO alpha_2 code. Therefore we have to do it manually or use the following function:\ndef CountryConversion(x):\n    if x =='XK':  ## Special Case\n        return 'Kosovo'\n    else: ## otherwise use pycountry package\n        return pycountry.countries.get(alpha_2=x).name  \n        # the name is the country name within the database\n\n\n## Python function can also have multiple returns inside a function\n## if logical expressions are used.\nInstead of using the name you can also use the pycountry.countries.get(alpha_2=x).official_name.\nApply (.aaply()) this function to the cntry column and create a new column in the same step. We also need to create the ['Year'] column by using the essround column. Here round 1 is equivalent to 2002, round 2 to 2004, round 3 to 2006 and so on.\n\n\nInstead of recoding number 77, 88 and 99, you can also remove them from the dataset by utilizing loc in conjunction with a logical expression. After that we have to use the groupby function but with our newly created columns. Additionally, we apply .mean() to this pandas Series. To get a DataFrame out of the Series, we have to reset the index by reset_index() You may remember we utilized melt() in our first homework to create a long format of the DataFrame. In this case we want a wide format with the Country as index and the columns referring to the Year. The correct function is pivot(...) which you have to apply. Last but not least you sort the values by .sort_values(by=...)\n\n\n\n\nTo count the number of individuals each combination of valid values of atchctr and atcherp, you have to create a crosstable by applying the following function\n\nvaluetable = pd.crosstab(df['atchctr'],df['atcherp']).reset_index()[['atchctr',  0.0,  1.0,  2.0,  3.0,  4.0, 5.0,  6.0,  7.0,  8.0,  9.0, 10.0]].loc[0:10]\nThe table that is created looks exactly like the plot we want to recreate. But for plotting python and most other programming languages prefer a long data format. To save time and not wrangle with this table we choose a different way. Therefore we simply use the function value_counts() on both columns\nvaluedf = df[['ColA', 'ColB']].value_counts().reset_index()\nvaluedf = df.rename(columns={0:\"NumIndividuals\"})\nTake a look at the data you created.\nFinally we can plot our data with sns.relplot, define the Dataset with data= , the x and y variable x =  ,y = and use size and hue within relplot\n\nFor this we need the orignal data set. To create a lowess plot in python we simply toggle to lowess inside the regplot function with lowess=True. And don’t show the underlying data by setting scatter=False. Beware of the 77 88 and 99 that may still exist within your data\n\nsns.regplot(data=...,....)\nIf you want to have a seperate plot for each country define a facetgrid:\ng = sns.FacetGrid(Dataframe, col=\"Country\", col_wrap=5 )\n## now we can map a function to each plot in this grid\ng.map_dataframe(sns.regplot,x= .. , y= ,.., ..)\nEverything you want to have inside the regplot function you have to write individually after the function inside the map funciton.\n\n\n\nRepeat everything again for euftf\n\n\n\nYou have to again group the data by country-year combination. To get to number of observations the .size() function should be helpful. Again .reset_index() to create a DataFrame. Now you can rename the column 0 to a more suitable name by using a dictionary. Define a dictionary and apply it to rename the column.\nRenameDict = dict({SizeColumnName: 'MoreSuitableName'})\nYourDataFrame = YourDataFrame.rename(columns=RenameDict)\nNow we have to pivot() again to get wide data format. The index= should be the country and the columns= should be the Year."
  },
  {
    "objectID": "hw-instructions/hw-03-instr.html",
    "href": "hw-instructions/hw-03-instr.html",
    "title": "Homework 03",
    "section": "",
    "text": "Important\n\n\n\nHomework 03 is due Oct 16.\nThe goal of this assignment is to do steps of exploratory data analysis (EDA) with the data about corona and the European Social Survey (ESS) from Homework 02."
  },
  {
    "objectID": "hw-instructions/hw-03-instr.html#corona",
    "href": "hw-instructions/hw-03-instr.html#corona",
    "title": "Homework 03",
    "section": "1 Corona",
    "text": "1 Corona\nContinue to work on your repository corona-ind-USERNAME and make the analysis in R-chunks.\nCreate a new Quarto-document and save it as “Corona_Analysis.qmd”.\n\nCustomize the YAML with the title “Corona Analysis”, your name, and output format being a standalone html-file with embedded resources. Enable code-folding in the output html file.\nWrite a chunk labeled data where you load the tidyverse library and import the data from Our World in Data (OWiD) into a tibble owid and the data from the World Health Organization (WHO) into a tibble called who (use read_csv for both).\nDocument what you did: Write a headline ## Data before the chunk data and briefly describe the two data sources and link the data files.\n\n\n\n\n\n\n\nTip\n\n\n\n\nIn https://github.com/JU-F22-MDSSB-MET-01/corona-ind-janlorenz/blob/main/Download_corona_data.R you find a script which downloads the corona datasets from OWiD and WHO. You can copy this.\nIn https://github.com/JU-F22-MDSSB-MET-01/corona-ind-janlorenz/blob/main/Corona.qmd you find an example for the YAML and the data import chunk.\n\n\n\n\n1.1 Question 1: Is the data of OWiD and WHO the same?\nWrite a new headline ## Differences between OWiD and WHO data, make the following analysis in a chunk, and write an answer to the question. Guide for the analysis:\n\nCreate a joined tibble. This can be done with a chain of pipes |>.\n\nMake a new tibble who_owid where you left join owid to who. Use the combination of the data and the country as the key. To that end you have to identify the corresponding columns in who and owid with the named vector c(\"Date_reported\" = \"date\", \"Country\" = \"location\").\nUse select to select and rename columns in who_owid such that you have columns for Date, Country, New_cases_who, New_cases_owid, Total_cases_who, and Total_cases_owid.\n\nMake a visualization where the new cases in WHO and OWiD appear as two lines. Filter for the time before 2020-07-01 and four countries: Germany, France, India, and a country of your choice (Hint: use %in%). Then facet your visualization by country.\nRepeat the visualization for the comparison of WHO and OWiD data for the total cases.\nBelow the visualizations, explain what is visualizaed and then answer the question based on the visualizations. (Some guiding questions: What are the differences? How severe are the differences on a daily basis and in the long run? What data shows more fluctuations?) Bonus: What is the reason for the difference?\n\n\nFor comparing New_cases_who and New_cases_owid you need a to make the tibble longer (pivot_longer) such that you have one column with a categroical variable specifying the data source and one column with the actual value of the new cases in each data set. Then you can make a ggplot with geom_line where you make two lines by assigning the data source to the color aesthetic.\nFor faceting, use facet_wrap. It is useful to specify that there should be only one column and that scales are specified such that each facet has its own y-axis scale (called free_y).\nThe same advice holds for the visualization of total cases.\n\n\n\n1.2 Question 2: Are there patterns in the timeline of new cases which seem unrelated to the potential spread of the virus in the population? How could we smooth them best?\nWrite a new headline ## Smoothing new cases, make a chunk where you compute three additional columns with smoothed case numbers (see below), make a visualization and explain which is most appropriate to use and why.\n\nComputed a new tibble who_smoothed from who for this question. (There would be no essential difference with owid.) For each day compute the average of the actual day and some days directly before such that each new value is an average of 3, 7, or 10 days. Call the three new columns New_cases_smooth3, New_cases_smooth7, and New_cases_smooth10. The average is the sum of all values divided by the total number of days. Use mutate. For each column use lag several times for each previous day (see ?lag). Don’t forget to group by countries!\nVisualize the timeline for Germany using a time span between 6 and 12 month. Make one panel for New_cases_smooth3, New_cases_smooth7, and New_cases_smooth10 respectively. (Hint: Either use pivot_longer and faceting, or three ggplot objects combined using the package patchwork.)\nAnswer the question. What is the pattern which seems unrelated to the spread of the virus? Which is the most appropriate smoothing, 3, 7, or 10 days and why?\n\n\n\n1.3 Question 3: How do deaths follow cases?\nMake a new headline about this question in your report.\nIn owid, there are variables new_cases_smoothed and new_deaths_smoothed which you can use for convenience to explore this question.\nTo explore the connection between COVID-19 cases and deaths focus on the first wave in Germany. In owid, filter the data for Germany and dates before 2020-07-01. Plot new_cases_smoothed and new_deaths_smoothed in one line plot using color to distinguish both. (Hint: Use pivot_longer.) Now, produce a new variable scaled_lagged_cases where you do a shift-and-scale transformation of new_cases_smoothed: That means you scale down the number of cases by multiplying a factor \\(y\\)(< 1) and you lag the number of cases along the time axis by \\(x\\) days using y * lag(new_cases_smoothed, x).\n\nPlay with numerical values of \\(x\\) and \\(y\\) by plotting scaled_lagged_cases together with new_death_smoothed. Find values for such that both lines overlap as good as possible. Put this visualization into the report.\nDescribe what these “visually calibrated” values of \\(x\\) and \\(y\\) tell us about the relation of cases and deaths of COVID-19?\nRedo the line plot with your calibrated values for all dates in Germany. Describe how the connection between new cases and deaths changes in the course of the pandemic. List potential reasons.\nRepeat this analysis for another country of your choice. To that end find out the time range of the first wave in the country before. Do you get the same \\(x\\) and \\(y\\) by visually calibrating for this country?\n\n\n\n1.4 Question 4: How is the severity of the corona pandemic in countries related to human development?\nMake a new headline about this question in your report.\nHuman development is meant to be measured by the human development index of the United Nations Development Programme. Briefly, describe in the report how the Human Development Index (HDI) is composed.\nUse owid. Make a figure which visualizes each country as a point with human_development_index on the horizontal and total_deaths_per_million on the vertical axis. Filter the values for the date 2022-08-31. Filter out all rows which do not represent countries. (Hint: You can take all rows which have a valid value for continent.) Make the size of dots such that the area is proportional to the population. (Hint: Use the scale_size_area() as an additional layer.) Color the dots by continent.\nDescribe the visible relation of HDI and total corona related deaths per million. Emphasize what you find interesting, and hypothesize about potential reasons for the finding.\n\n\n1.5 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file and push it to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-03-instr.html#european-social-survey",
    "href": "hw-instructions/hw-03-instr.html#european-social-survey",
    "title": "Homework 03",
    "section": "2 European Social Survey",
    "text": "2 European Social Survey\nContinue to work on your repository ess-ind-USERNAME and make the analysis in python-chunks. You have to look ESS Data Documentation https://ess-search.nsd.no/.\n\nCreate a new Quarto-document and save it as “ESS-analysis.qmd” (alternatively as “ESS-analysis.ipynb” if you can render html-files similarly from it).\n\n\nCustomize the YAML with the title “European Social Survey Analysis”, your name, and output format being a standalone html-file with embedded resources. Enable code-folding in the output html file.\nWrite a chunk labeled data where you load the tidyverse library and import the dataset you downloaded for Homework 02.\n\nDocument what you did: Write a headline ## Data before the chunk data and briefly describe the data source.\nFor each of the following questions write a fitting headline to structure your report.\n\n\n2.1 Question 1: What is the ranking of European countries with respect to of average satisfaction with life?\n\nCompute the average of stflife for each country-year combination. Make a table with four columns: the country name and the average life satisfaction for the years 2006, 2012, and 2018. Order the table by the values in 2018 and print it nicely in your report.\nTo that end, you must create new variables countryname from the two-character ISO-codes in cntyr (use the library pycountry) and a new variable year.\nBeware: The ESS variables have coded missing values as large numbers like 77 or 88. Find out these values and recode them as NA before computing averages!\n\nDescribe what you find remarkable in the table. (Who is on top? who at the bottom? Any interesting trends? Any geographical patterns?)\n\n\n\n2.2 Question 2: What is the relation of the emotional attachment of Europeans to their own country and to Europe?\nEmotional attachment to Europe may diminish emotional attachment to the country. Test if this is true with the variables atchctr and atcherp. First, look up the wording of the two questions and the answering options and write it into the report.\nBuild two visualizations and put them in the report. Beware: First recode the missing values as NA! (See above.)\n\nA visualization where you first count the number of individuals each combination of valid values of atchctr and atcherp. Then plot each combination as a dot with atchctr on the x-axis, atcherp on the y-axis, and the number of individuals represented by size and color and color of the dot.\nMake a lowess (locally weighted scatterplot smoothing) line of atchctr on the x-axis, atcherp on the y-axis using the function regplot from seaborn.\n\nAnswer the question based on the visualization.\n\n\n2.3 Question 3: What is the relation of the emotional attachment to the own country and the opinion about further of European integration?\nThis is analog to Question 2 with euftf instead of atcherp. First, look up the wording of euftf and the answering options and write it into the report. Beware: First recode the missing values as NA! (See above.)\nBuild two visualizations as above.\nAnswer the question.\n\n\n2.4 Question 4: How many observations are there for each country-year combination?\nCompute the numbers and list them nicely in a table in your report.\nAre the numbers roughly proportional to the population in these countries? Discuss if the answer creates some shortcomings about the answers to Questions 2 and 3. How could these be mitigated?\n\n\n2.5 Question 5: Answer your own exploratory question\nLook up questions behind the variables in the ESS data set, formulate a question, and answer it with summarizing, and visualization."
  },
  {
    "objectID": "W9.html#precision-and-recall",
    "href": "W9.html#precision-and-recall",
    "title": "W#9 Logisitc Regression",
    "section": "Precision and Recall",
    "text": "Precision and Recall"
  },
  {
    "objectID": "W9.html#receiver-operating-characteristic",
    "href": "W9.html#receiver-operating-characteristic",
    "title": "W#9 Logisitc Regression",
    "section": "Receiver operating characteristic",
    "text": "Receiver operating characteristic"
  },
  {
    "objectID": "W9.html#recap-linear-models",
    "href": "W9.html#recap-linear-models",
    "title": "W#9 Logisitc Regression",
    "section": "Recap linear models",
    "text": "Recap linear models\nWe had linear models \\(y_i = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_n\\) with\nResponse (dependent) variable \\(y\\): Numeric\nPredictor (independent) variables \\(x_1,\\dots,x_n\\): Numeric or Binary (0 or 1)\n\nWhen a variable is categorical: We dummify it to \\(m-1\\) binary (dummy) variables (\\(m\\) is the number of categories)\n\nNote: In computer science dummifying is called one-hot encoding.\n\nWhen a variable has ordered categories (ordinal level of measurement): We may transform to a numerical variable assuming comparison of numerical distances between categories are interpretable."
  },
  {
    "objectID": "W9.html#interaction-effects",
    "href": "W9.html#interaction-effects",
    "title": "W#9 Logisitc Regression",
    "section": "Interaction effects",
    "text": "Interaction effects\nAdding products of variables in the linear model \\(y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{3}x_1x_2 + \\dots\\).\nFor \\(x_1\\) and \\(x_2\\) being dummy variables this is for example"
  },
  {
    "objectID": "W9.html#recap-interaction-effects",
    "href": "W9.html#recap-interaction-effects",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Recap: Interaction effects",
    "text": "Recap: Interaction effects\nAdding products of variables in the linear model \\(y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{3}x_1x_2 + \\dots\\).\nFor \\(x_1\\) and \\(x_2\\) being dummy variables this is for example\n\n\n\n\n\ngndr_f\nhas_kids\ngndr_f_x_has_kids\n\n\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\nCheck:\n\nWhat are the reference categories? Being male without kids.\nEstimating model on life satisfaction. How would we see if being a mother/father increases life satisfaction more? positiv/negative coefficient gndr_f_x_has_kids"
  },
  {
    "objectID": "W9.html#what-if-response-is-binary",
    "href": "W9.html#what-if-response-is-binary",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "What if response is binary?",
    "text": "What if response is binary?\n\nExample: Spam filter for emails\n\n\nlibrary(openintro)\nlibrary(tidyverse)\nglimpse(email)\n\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2012-01-01 07:16:41, 2012-01-01 08:03:59, 2012-01-01 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …"
  },
  {
    "objectID": "W9.html#variables",
    "href": "W9.html#variables",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Variables",
    "text": "Variables\n?email shows all variable descriptions. For example:\n\nspam Indicator for whether the email was spam.\nfrom Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\ncc Number of people cc’ed.\ntime Time at which email was sent.\nattach The number of attached files.\ndollar The number of times a dollar sign or the word “dollar” appeared in the email.\nnum_char The number of characters in the email, in thousands.\nre_subj Whether the subject started with “Re:”, “RE:”, “re:”, or “rE:”\n\n\n\nThe development, extraction, or discovery of such variables is called feature engineering, feature extraction or feature discovery. Usually, a combination of domain knowledge and data science skill is needed to do this."
  },
  {
    "objectID": "W9.html#multinomial",
    "href": "W9.html#multinomial",
    "title": "W#9 Logisitc Regression",
    "section": "Multinomial",
    "text": "Multinomial\n\n\nWe will not cover other categorical variables than binary ones here. However, many of the probabilistic concepts transfer."
  },
  {
    "objectID": "W9.html#does",
    "href": "W9.html#does",
    "title": "W#9 Logisitc Regression",
    "section": "Does",
    "text": "Does\n\nemail |> ggplot(aes(x = num_char, y = spam)) + geom_boxplot()"
  },
  {
    "objectID": "W9.html#data-exploration",
    "href": "W9.html#data-exploration",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Data exploration",
    "text": "Data exploration\nWould you expect spam to be longer or shorter?\n\n\nemail |> ggplot(aes(x = num_char, y = spam)) + geom_boxplot()\n\n\n\n\n\n\nWould you expect spam subject to start with “Re:” or the like?\n\n\n\nemail |> ggplot(aes(y = re_subj, fill = spam)) + geom_bar()"
  },
  {
    "objectID": "W9.html#modeling",
    "href": "W9.html#modeling",
    "title": "W#9 Logisitc Regression",
    "section": "Modeling",
    "text": "Modeling\nBoth seem to give some signal.\nHow can we model the relationship?\nWe focus first on just num_char:\n\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\nlibrary(tidymodels) \nlinear_reg() |> fit(as.numeric(spam)-1 ~ num_char, data = email)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = as.numeric(spam) - 1 ~ num_char, data = data)\n\nCoefficients:\n(Intercept)     num_char  \n   0.118214    -0.002299"
  },
  {
    "objectID": "W9.html#linear-models",
    "href": "W9.html#linear-models",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Linear models?",
    "text": "Linear models?\nBoth seem to give some signal. How can we model the relationship?\nWe focus first on just num_char:\n\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + geom_point(alpha = 0.2) + geom_smooth(method = \"lm\")\n\n\n\nlibrary(tidymodels) \nlinear_reg() |> fit(as.numeric(spam)-1 ~ num_char, data = email)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = as.numeric(spam) - 1 ~ num_char, data = data)\n\nCoefficients:\n(Intercept)     num_char  \n   0.118214    -0.002299  \n\n\nWe would like to have a better concept!"
  },
  {
    "objectID": "W9.html#a-probabilistic-concept",
    "href": "W9.html#a-probabilistic-concept",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "A probabilistic concept",
    "text": "A probabilistic concept\n\nWe treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials\n\nBernoulli trial: a random experiment with exactly two possible outcomes, success and failure, in which the probability of success is the same every time the experiment is conducted\n\n\n\n\nEach email is treated as Bernoulli trial with separate probability of success\n\n\\[ y_i ∼ \\text{Bernoulli}(p_i) \\]\n\n\n\nWe use the predictor variables to model the Bernoulli parameter \\(p_i\\)\n\n\n\n\nNow we conceptualized a continuous response, but still a linear model does not fit perfectly for \\(p_i\\) (since a probability is between 0 and 1).\nHowever, we can transform the linear model to have the appropriate range."
  },
  {
    "objectID": "W9.html#generalized-linear-models",
    "href": "W9.html#generalized-linear-models",
    "title": "W#9 Logisitc Regression",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nThis is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs)\n\n. . . - Logistic regression is just one example"
  },
  {
    "objectID": "W9.html#three-characteristics-of-glms",
    "href": "W9.html#three-characteristics-of-glms",
    "title": "W#9 Logisitc Regression",
    "section": "Three characteristics of GLMs",
    "text": "Three characteristics of GLMs\nAll GLMs have the following three characteristics:\n\nA probability distribution describing a generative model for the outcome variable\n\n. . . 2. A linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\]\n. . . 3. A link function that relates the linear model to the parameter of the outcome distribution\nclass: middle"
  },
  {
    "objectID": "W9.html#logistic-regression-1",
    "href": "W9.html#logistic-regression-1",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLogistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors.\nThe distribution is the Bernoulli distribution.\nAs link function connecting \\(\\eta_i\\) to \\(p_i\\) we use the logit function.\n\n\n\nLogit function: \\(\\text{logit}: [0,1] \\to \\mathbb{R}\\)\n\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\n\\(\\frac{p}{1-p}\\) is called the odds of a success which happens with probability \\(p\\).\nExample: Roll a six with a die has \\(p=1/6\\). Thus, the odds are \\(\\frac{1/6}{5/6} = 1/5\\). Sometimes written as 1:5. “The odds of success are one to five.”"
  },
  {
    "objectID": "W9.html#logit-function-visualised",
    "href": "W9.html#logit-function-visualised",
    "title": "W#9 Logisitc Regression",
    "section": "Logit function, visualised",
    "text": "Logit function, visualised\n\nggplot() + \n geom_function(fun = function(x) log(x/(1-x)), xlim=c(0.001,0.999), n = 500) + \n geom_function(fun = function(x) 1/(1 + exp(-x)), color = \"red\") +\n scale_x_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n scale_y_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n coord_fixed() + theme_minimal(base_size = 24) + labs(x = \"x\")"
  },
  {
    "objectID": "W9.html#properties-of-the-logit",
    "href": "W9.html#properties-of-the-logit",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Properties of the logit",
    "text": "Properties of the logit\n\nLogit takes values between 0 and 1 and returns values between \\(-\\infty\\) and \\(\\infty\\)\nThe inverse of the logit function if the logistic function (mapping values from \\(-\\infty\\) and \\(\\infty\\) to values between 0 and 1): \\[\\text{logit}^{-1}(x) = \\text{logistic}(x) = \\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}}\\]\nLogit can be interpreted as the log odds of a success – more on this later.\n\n\n\n\nThe logistic function is the solution of the differential equation \\(\\frac{d}{dx}f(x) = f(x)(1-f(x))\\) which also appears in the SI-model of epidemics (and other models of exponential growth with saturation).\nGood exercises to check your math skills:\n\nShow that \\(\\text{logit}^{-1}(x) = \\text{logistic}(x)\\) or the other way round.\nTransform \\(\\frac{e^x}{1+e^x}\\) into \\(\\frac{1}{1+e^{-x}}\\).\nCheck that \\(\\text{logistic}(x)\\) is a solution to \\(\\frac{d}{dx}f(x) = f(x)(1-f(x))\\).\n\nYou can request hints from me when you get stuck."
  },
  {
    "objectID": "W9.html#the-logistic-regression-model",
    "href": "W9.html#the-logistic-regression-model",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nBased on the three GLM criteria we have\n\n\\(y_i \\sim \\text{Bernoulli}(p_i)\\)\n\\(\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\n\\(\\text{logit}(p_i) = \\eta_i\\)\n\n\n\n\nFrom which we get\n\n\\[p_i = \\frac{e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}{1 + e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}\\]"
  },
  {
    "objectID": "W9.html#characterising-glms",
    "href": "W9.html#characterising-glms",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Characterising GLMs",
    "text": "Characterising GLMs\n\nGeneralized linear models (GLMs) are a way of addressing many problems in regression\nLogistic regression is one example\n\nAll GLMs have the following three characteristics:\n\nA probability distribution as a generative model for the outcome variable \\(y_i \\sim \\text{Distribution}(\\text{parameter})\\)\n\n\n\nA linear model \\(\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\)\nwhere \\(\\eta\\) is related to a mean parameter of the distribution by the …\n\n\n\n\nLink function that relates the linear model to the parameter of the outcome distribution."
  },
  {
    "objectID": "W9.html#logit-and-logistic-function",
    "href": "W9.html#logit-and-logistic-function",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Logit and logistic function",
    "text": "Logit and logistic function\n\nggplot() + \n geom_function(fun = function(x) log(x/(1-x)), xlim=c(0.001,0.999), n = 500) + \n geom_function(fun = function(x) 1/(1 + exp(-x)), color = \"red\") +\n scale_x_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n scale_y_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n coord_fixed() + theme_minimal(base_size = 24) + labs(x = \"x\")"
  },
  {
    "objectID": "W9.html#recap-linear-models-1",
    "href": "W9.html#recap-linear-models-1",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Recap linear models",
    "text": "Recap linear models\nWe had linear models \\(y_i = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_n\\) with\nResponse (dependent) variable \\(y\\): Numeric\nPredictor (independent) variables \\(x_1,\\dots,x_n\\): Numeric or Binary (0 or 1)\n\nWhen a variable is categorical: We dummify it to \\(m-1\\) binary (dummy) variables (\\(m\\) is the number of categories)\n\nNote: In computer science dummifying is called one-hot encoding.\n\nWhen a variable has ordered categories (ordinal level of measurement): We may transform to a numerical variable assuming comparison of numerical distances between categories are interpretable."
  },
  {
    "objectID": "W9.html#modeling-spam",
    "href": "W9.html#modeling-spam",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Modeling spam",
    "text": "Modeling spam\nWith tidymodels we fit a GLM in the same way as a linear model except we\n\nspecify the model with logistic_reg()\nuse \"glm\" instead of \"lm\" as the engine\ndefine family = \"binomial\" for the link function to be used in the model\n\n\nspam_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char, data = email, family = \"binomial\")\ntidy(spam_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\n\n\n\nThe family is binomial because the Bernoulli distribution is a special case of the binomial distribution \\(\\text{Binomial}(n,p)\\) with \\(n=1\\).\nThe binomial distribution specifies the probability to have \\(k\\) successes in \\(n\\) Bernoulli trials with the same success probability \\(p\\)."
  },
  {
    "objectID": "W9.html#spam-model",
    "href": "W9.html#spam-model",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Spam model",
    "text": "Spam model\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\nModel: \\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}\\]"
  },
  {
    "objectID": "W9.html#predicted-probability-examples",
    "href": "W9.html#predicted-probability-examples",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Predicted probability: Examples",
    "text": "Predicted probability: Examples\nWe can compute the predicted probability that an email with 2000 character is spam as follows:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot 2 = -1.9242\\]\n(Note: num_char is in thousands.)\n\\[\\frac{p}{1-p} = e^{-1.9242} = 0.15 \\Rightarrow p = 0.15 \\cdot (1 - p)\\]\n\\[p = 0.15 - 0.15\\cdot p \\Rightarrow 1.15\\cdot p = 0.15\\]\n\\[p = 0.15 / 1.15 = 0.13\\]"
  },
  {
    "objectID": "W9.html#predicted-probability",
    "href": "W9.html#predicted-probability",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Predicted probability",
    "text": "Predicted probability\n\nlogistic <- function(t) 1/(1+exp(-t))\npreds <- tibble(x=c(2,15,40), y = logistic(-1.80-0.0621*x))\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-1.80-0.0621*x),color=\"red\") +\n geom_point(data = preds, mapping = aes(x,y), color = \"blue\", size = 3)\n\n\nSpam probability 2,000 characters: 0.1273939\nSpam probability 15,000 characters: 0.06114\nSpam probability 40,000 characters: 0.0135999"
  },
  {
    "objectID": "W9.html#false-positive-and-negative",
    "href": "W9.html#false-positive-and-negative",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail labelled spam\nEmail labelled not spam\n\n\n\n\nEmail is spam\nTrue positive\nFalse negative (Type 2 error)\n\n\nEmail is not spam\nFalse positive (Type 1 error)\nTrue negative"
  },
  {
    "objectID": "W9.html#sensitivity-and-specificity-1",
    "href": "W9.html#sensitivity-and-specificity-1",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)\n\n\nFor spam:\nSensitivity: Fraction of emails labelled as spam among all emails which are spam.\nLow sensitivity \\(\\to\\) More false negatives \\(\\to\\) More spam in you inbox!\nSpecificity: Fraction of emails labelled as not spam among all emails which are not spam.\nLow specificity \\(\\to\\) More false positives \\(\\to\\) More relevant emails in spam folder!\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "W9.html#another-example",
    "href": "W9.html#another-example",
    "title": "W#9 Logisitc Regression",
    "section": "Another example",
    "text": "Another example\ndata(“PimaIndiansDiabetes2”)"
  },
  {
    "objectID": "W9.html#maximum-likelihood",
    "href": "W9.html#maximum-likelihood",
    "title": "W#9 Logisitc Regression",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nSee “More probability” for Logistic Regression."
  },
  {
    "objectID": "W9.html#another-example-penguins",
    "href": "W9.html#another-example-penguins",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Another example: Penguins",
    "text": "Another example: Penguins\n\nlibrary(palmerpenguins)\nsex_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(sex ~ body_mass_g, data = na.omit(penguins), family = \"binomial\")\ntidy(sex_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) -5.16     0.724        -7.13 1.03e-12\n2 body_mass_g  0.00124  0.000173      7.18 7.10e-13\n\n\n\nna.omit(penguins) |> ggplot(aes(x = body_mass_g, y = sex)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-5.16+0.00124*x) + 1,color=\"red\") +\n xlim(c(2000,7000))"
  },
  {
    "objectID": "W9.html#another-view-sensitivity-and-specifity",
    "href": "W9.html#another-view-sensitivity-and-specifity",
    "title": "W#9 Logisitc Regression",
    "section": "Another view: Sensitivity and specifity",
    "text": "Another view: Sensitivity and specifity"
  },
  {
    "objectID": "W9.html#another-view",
    "href": "W9.html#another-view",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Another view",
    "text": "Another view"
  },
  {
    "objectID": "W9.html#covid-19-tests",
    "href": "W9.html#covid-19-tests",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "COVID-19 tests",
    "text": "COVID-19 tests\nWhat is the sensitivity of a test?\n\nProbability to have COVID-19 when the test is positive.\n\n\nWhat is the specificity of a test?\n\n\nProbability to not have COVID-19 when the test is negative.\nOften the sensitivity is around 90% and the specificity is around 99%. What does that mean?\n\n\n\nWhen you test negative you can be more sure that you don’t have it, than you can be sure that you have it when your test is positive.\nHowever, in a larger population of testing individuals with high prevalence also 99% specificity implies a large fraction of false negatives!"
  },
  {
    "objectID": "W9.html#goal-building-a-spam-filter",
    "href": "W9.html#goal-building-a-spam-filter",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Goal: Building a spam filter",
    "text": "Goal: Building a spam filter\n\nData: Set of emails and we know if each email is spam/not and other features\nUse logistic regression to predict the probability that an incoming email is spam\nUse model selection to pick the model with the best predictive performance\n\n\n\nBuilding a model to predict the probability that an email is spam is only half of the battle! We also need a decision rule about which emails get flagged as spam (e.g. what probability should we use as out cutoff?)\n\n\n\n\nA simple approach: choose a single threshold probability and any email that exceeds that probability is flagged as spam"
  },
  {
    "objectID": "W9.html#emails-use-all-predictors",
    "href": "W9.html#emails-use-all-predictors",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Emails: Use all predictors",
    "text": "Emails: Use all predictors\n\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = email, family = \"binomial\") |>\n  tidy() |> print(n = 22)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n# A tibble: 22 × 5\n   term         estimate std.error statistic  p.value\n   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)  -9.09e+1   9.80e+3  -0.00928 9.93e- 1\n 2 to_multiple1 -2.68e+0   3.27e-1  -8.21    2.25e-16\n 3 from1        -2.19e+1   9.80e+3  -0.00224 9.98e- 1\n 4 cc            1.88e-2   2.20e-2   0.855   3.93e- 1\n 5 sent_email1  -2.07e+1   3.87e+2  -0.0536  9.57e- 1\n 6 time          8.48e-8   2.85e-8   2.98    2.92e- 3\n 7 image        -1.78e+0   5.95e-1  -3.00    2.73e- 3\n 8 attach        7.35e-1   1.44e-1   5.09    3.61e- 7\n 9 dollar       -6.85e-2   2.64e-2  -2.59    9.64e- 3\n10 winneryes     2.07e+0   3.65e-1   5.67    1.41e- 8\n11 inherit       3.15e-1   1.56e-1   2.02    4.32e- 2\n12 viagra        2.84e+0   2.22e+3   0.00128 9.99e- 1\n13 password     -8.54e-1   2.97e-1  -2.88    4.03e- 3\n14 num_char      5.06e-2   2.38e-2   2.13    3.35e- 2\n15 line_breaks  -5.49e-3   1.35e-3  -4.06    4.91e- 5\n16 format1      -6.14e-1   1.49e-1  -4.14    3.53e- 5\n17 re_subj1     -1.64e+0   3.86e-1  -4.25    2.16e- 5\n18 exclaim_subj  1.42e-1   2.43e-1   0.585   5.58e- 1\n19 urgent_subj1  3.88e+0   1.32e+0   2.95    3.18e- 3\n20 exclaim_mess  1.08e-2   1.81e-3   5.98    2.23e- 9\n21 numbersmall  -1.19e+0   1.54e-1  -7.74    9.62e-15\n22 numberbig    -2.95e-1   2.20e-1  -1.34    1.79e- 1\n\n\n\n\nWe treat the warning later."
  },
  {
    "objectID": "W9.html#the-prediction-task",
    "href": "W9.html#the-prediction-task",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "The prediction task",
    "text": "The prediction task\n\nThe mechanics of prediction is easy:\n\nPlug in values of predictors to the model equation\nCalculate the predicted value of the response variable, \\(\\hat{y}\\)\n\n\n\n\nGetting it right is harder\n\nThere is no guarantee the model estimates you have are correct\nOr that your model will perform as well with new data as it did with your sample data"
  },
  {
    "objectID": "W9.html#overfitting",
    "href": "W9.html#overfitting",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Overfitting",
    "text": "Overfitting\n\n\n\n\n\n\n\nThis is simulated data."
  },
  {
    "objectID": "W9.html#spending-our-data",
    "href": "W9.html#spending-our-data",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Spending our data",
    "text": "Spending our data\n\nSeveral steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available can lead to overfitting.\n\nSolution: We subsets our data for different tasks, as opposed to allocating all data to parameter estimation (as we have done so far)."
  },
  {
    "objectID": "W9.html#splitting-data-1",
    "href": "W9.html#splitting-data-1",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Splitting data",
    "text": "Splitting data\n\nTraining set:\n\nSandbox for model building\nSpend most of your time using the training set to develop the model\nMajority of the data (usually 80%)\n\nTesting set:\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once, otherwise it becomes part of the modeling process\nRemainder of the data (usually 20%)"
  },
  {
    "objectID": "W9.html#performing-the-split",
    "href": "W9.html#performing-the-split",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Performing the split",
    "text": "Performing the split\n\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1116)\n\n# Put 80% of the data into the training set \nemail_split <- initial_split(email, prop = 0.80)\n\n# Create data frames for the two sets:\ntrain_data <- training(email_split)\ntest_data  <- testing(email_split)"
  },
  {
    "objectID": "W9.html#peek-at-the-split",
    "href": "W9.html#peek-at-the-split",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Peek at the split",
    "text": "Peek at the split\n\n\n\nglimpse(train_data)\n\nRows: 3,136\nColumns: 21\n$ spam         <fct> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0,…\n$ sent_email   <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ time         <dttm> 2012-01-25 23:46:55, 2012-01-03 06:28:28, 2012-02-04 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 10, 0, 0, 0, 0, 0, 13, 0, 0, 0, 2, 0, 0, 0, 14, 0, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, yes, no, no, no, no, no, no, …\n$ inherit      <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, …\n$ num_char     <dbl> 23.308, 1.162, 4.732, 42.238, 1.228, 25.599, 16.764, 10.7…\n$ line_breaks  <int> 477, 2, 127, 712, 30, 674, 367, 226, 98, 671, 46, 192, 67…\n$ format       <fct> 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 12, 0, 2, 2, 2, 31, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 11, 1, …\n$ number       <fct> small, none, big, big, small, small, small, small, small,…\n\n\n\n\nglimpse(test_data)\n\nRows: 785\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, …\n$ sent_email   <fct> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ time         <dttm> 2012-01-01 18:55:06, 2012-01-01 20:38:32, 2012-01-02 06:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, …\n$ dollar       <dbl> 0, 0, 5, 0, 0, 0, 0, 5, 4, 0, 0, 0, 21, 0, 0, 2, 9, 0, 0,…\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, …\n$ num_char     <dbl> 4.837, 15.075, 18.037, 45.842, 11.438, 1.482, 14.431, 0.9…\n$ line_breaks  <int> 193, 354, 345, 881, 125, 24, 296, 13, 192, 14, 32, 30, 55…\n$ format       <fct> 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, …\n$ exclaim_subj <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 1, 10, 20, 5, 2, 0, 0, 0, 6, 0, 0, 1, 3, 0, 4, 0, 1, 0, 1…\n$ number       <fct> big, small, small, big, small, none, small, small, small,…"
  },
  {
    "objectID": "W9.html#interpretation-of-coefficients",
    "href": "W9.html#interpretation-of-coefficients",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}\\]\nWhat does an increase by thousand characters (num_char + 1) imply?\n\nLet us assume the predicted probability of an email is \\(p_0\\). Then an increase of num_char by one implied that the log-odds become\n\\[\\log\\left(\\frac{p_0}{1-p_0}\\right) - 0.0621 = \\log\\left(\\frac{p_0}{1-p_0}\\right) - \\log(e^{0.0621})\\]\n\\[ = \\log\\left(\\frac{p_0}{1-p_0}\\right) + \\log(\\frac{1}{e^{0.0621}}) = \\log\\left(\\frac{p_0}{1-p_0} \\frac{1}{e^{0.0621}}\\right) = \\log\\left(\\frac{p_0}{1-p_0} 0.94\\right)\\]\nThat means the odds of being spam decrease by 6%."
  },
  {
    "objectID": "W9.html#confusion-matrix",
    "href": "W9.html#confusion-matrix",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nMore general: Confusion matrix of statistical classification:"
  },
  {
    "objectID": "W9.html#fit-a-model-to-the-training-dataset",
    "href": "W9.html#fit-a-model-to-the-training-dataset",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Fit a model to the training dataset",
    "text": "Fit a model to the training dataset\n\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = train_data, family = \"binomial\")\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWe get a warning and should explore the reasons for 0 or 1 probability.\n\n\n\nA deeper looking into the predicted probabilities (not shown here) shows that 4 cases are predicted to be spam with 100% probability, as well as 864 cases are predicted to be not spam with 100% probability.\nNote: The dplyr function near was used to assess if predicted probabilities were one.\nThis is usually undesirable. Hence the warning."
  },
  {
    "objectID": "W9.html#look-at-categorical-predictors",
    "href": "W9.html#look-at-categorical-predictors",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Look at categorical predictors",
    "text": "Look at categorical predictors\n\nCloser look at from and sent_email."
  },
  {
    "objectID": "W9.html#counting-cases",
    "href": "W9.html#counting-cases",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Counting cases",
    "text": "Counting cases\n\n\nfrom: Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\n\ntrain_data |> count(spam, from)\n\n# A tibble: 3 × 3\n  spam  from      n\n  <fct> <fct> <int>\n1 0     1      2837\n2 1     0         3\n3 1     1       296\n\n\nNo non-spam mails without from.\n\nsent_mail: Indicator for whether the sender had been sent an email from the receiver in the last 30 days.\n\ntrain_data |> count(spam, sent_email)\n\n# A tibble: 3 × 3\n  spam  sent_email     n\n  <fct> <fct>      <int>\n1 0     0           1972\n2 0     1            865\n3 1     0            299\n\n\nNo spam mails with sent_email.\n\n\n\nThere is incomplete separation in the data for those variables.\nThat mean we have a sure prediction probabilities (0 or 1). (That is the warning. Also, these variables have the highest coefficients.)\nThis is not what we assume about reality. Maybe our sample is too small to see it.\nTherefore we exclude these variables."
  },
  {
    "objectID": "W9.html#look-at-numerical-variables",
    "href": "W9.html#look-at-numerical-variables",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Look at numerical variables",
    "text": "Look at numerical variables\n\n\ntrain_data |>\n group_by(spam) |>\n select(where(is.numeric)) |> \n pivot_longer(-spam) |> \n group_by(name, spam) |> \n summarize(mean = mean(value), sd = sd(value)) |> \n print(n = 22)\n\n\n# A tibble: 22 × 4\n# Groups:   name [11]\n   name         spam       mean       sd\n   <chr>        <fct>     <dbl>    <dbl>\n 1 attach       0       0.124     0.775 \n 2 attach       1       0.227     0.620 \n 3 cc           0       0.393     2.62  \n 4 cc           1       0.388     3.25  \n 5 dollar       0       1.56      5.33  \n 6 dollar       1       0.779     3.01  \n 7 exclaim_mess 0       6.68     50.2   \n 8 exclaim_mess 1       8.75     88.4   \n 9 exclaim_subj 0       0.0783    0.269 \n10 exclaim_subj 1       0.0769    0.267 \n11 image        0       0.0536    0.503 \n12 image        1       0.00334   0.0578\n13 inherit      0       0.0352    0.216 \n14 inherit      1       0.0702    0.554 \n15 line_breaks  0     247.      326.    \n16 line_breaks  1     108.      321.    \n17 num_char     0      11.4      14.9   \n18 num_char     1       5.63     15.7   \n19 password     0       0.112     0.938 \n20 password     1       0.0201    0.182 \n21 viagra       0       0         0     \n22 viagra       1       0.0268    0.463 \n\n\n\nviagra has no mentions in non-spam emails.\n\nWe should exclude this variable for the same reason."
  },
  {
    "objectID": "W9.html#fit-a-model-to-the-training-dataset-1",
    "href": "W9.html#fit-a-model-to-the-training-dataset-1",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Fit a model to the training dataset",
    "text": "Fit a model to the training dataset\n\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ . - from - sent_email - viagra, data = train_data, family = \"binomial\") \n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nemail_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = spam ~ . - from - sent_email - viagra, family = stats::binomial, \n    data = data)\n\nCoefficients:\n (Intercept)  to_multiple1            cc          time         image  \n  -9.867e+01    -2.505e+00     1.944e-02     7.396e-08    -2.854e+00  \n      attach        dollar     winneryes       inherit      password  \n   5.070e-01    -6.440e-02     2.170e+00     4.499e-01    -7.065e-01  \n    num_char   line_breaks       format1      re_subj1  exclaim_subj  \n   5.870e-02    -5.420e-03    -9.017e-01    -2.995e+00     1.002e-01  \nurgent_subj1  exclaim_mess   numbersmall     numberbig  \n   3.572e+00     1.009e-02    -8.518e-01    -1.329e-01  \n\nDegrees of Freedom: 3135 Total (i.e. Null);  3117 Residual\nNull Deviance:      1974 \nResidual Deviance: 1447     AIC: 1485\n\n\nWe still get a warning, but without very high coefficients.\n\n\nA deeper analysis shows that now only two cases are predicted not spam with 100% probability."
  },
  {
    "objectID": "W9.html#predict-outcome-on-the-testing-dataset",
    "href": "W9.html#predict-outcome-on-the-testing-dataset",
    "title": "W#9 Logisitc Regression",
    "section": "Predict outcome on the testing dataset",
    "text": "Predict outcome on the testing dataset\nPredicting the raw values (log-odds)\n\npredict(email_fit, test_data, type = \"raw\") |> head()\n\n        1         2         3         4         5         6 \n-4.942500 -6.312226 -3.938487 -6.688992 -4.399541 -1.587700 \n\n\nPredicting probabilities\n\npredict(email_fit, test_data, type = \"prob\") |> head()\n\n# A tibble: 6 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.993 0.00709\n2   0.998 0.00181\n3   0.981 0.0191 \n4   0.999 0.00124\n5   0.988 0.0121 \n6   0.830 0.170  \n\n\nPredicting spam\n\npredict(email_fit, test_data) # That is the default with type = \"class\"\n\n# A tibble: 785 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#predict-probabilities-on-the-testing-dataset",
    "href": "W9.html#predict-probabilities-on-the-testing-dataset",
    "title": "W#9 Logisitc Regression",
    "section": "Predict probabilities on the testing dataset",
    "text": "Predict probabilities on the testing dataset\n\n\n# A tibble: 785 × 4\n   .pred_0 .pred_1 spam  time               \n     <dbl>   <dbl> <fct> <dttm>             \n 1   0.993 0.00709 0     2012-01-01 18:55:06\n 2   0.998 0.00181 0     2012-01-01 20:38:32\n 3   0.981 0.0191  0     2012-01-02 06:42:16\n 4   0.999 0.00124 0     2012-01-02 16:12:51\n 5   0.988 0.0121  0     2012-01-02 17:45:36\n 6   0.830 0.170   0     2012-01-02 22:55:03\n 7   0.959 0.0410  0     2012-01-03 02:07:17\n 8   0.861 0.139   0     2012-01-03 06:41:35\n 9   0.938 0.0617  0     2012-01-03 17:02:35\n10   0.902 0.0983  0     2012-01-03 12:14:51\n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#predict-with-the-testing-dataset",
    "href": "W9.html#predict-with-the-testing-dataset",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Predict with the testing dataset",
    "text": "Predict with the testing dataset\nPredicting the raw values (log-odds)\n\npredict(email_fit, test_data, type = \"raw\") |> head() # head prints the first values\n\n        1         2         3         4         5         6 \n-4.942500 -6.312226 -3.938487 -6.688992 -4.399541 -1.587700 \n\n\n\n\nPredicting probabilities\n\npredict(email_fit, test_data, type = \"prob\") |> head()\n\n# A tibble: 6 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.993 0.00709\n2   0.998 0.00181\n3   0.981 0.0191 \n4   0.999 0.00124\n5   0.988 0.0121 \n6   0.830 0.170  \n\n\n\nPredicting spam (default)\n\npredict(email_fit, test_data) # Would be type = \"class\"\n\n# A tibble: 785 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#relate-back-to-the-model-concept",
    "href": "W9.html#relate-back-to-the-model-concept",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Relate back to the model concept",
    "text": "Relate back to the model concept\n\n\nemail_pred <- \n predict(email_fit, test_data, type = \"prob\") |>\n select(spam_prob = .pred_1) |> \n mutate(spam_logodds = \n         predict(email_fit, test_data, type = \"raw\"), \n        spam_odds = exp(spam_logodds)) |> \n bind_cols(predict(email_fit, test_data)) |> \n # Append real data\n bind_cols(test_data |> select(spam)) \nemail_pred\n\n\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1   0.00709        -4.94   0.00714 0           0    \n 2   0.00181        -6.31   0.00181 0           0    \n 3   0.0191         -3.94   0.0195  0           0    \n 4   0.00124        -6.69   0.00124 0           0    \n 5   0.0121         -4.40   0.0123  0           0    \n 6   0.170          -1.59   0.204   0           0    \n 7   0.0410         -3.15   0.0427  0           0    \n 8   0.139          -1.83   0.161   0           0    \n 9   0.0617         -2.72   0.0657  0           0    \n10   0.0983         -2.22   0.109   0           0    \n# … with 775 more rows\n\n\n\n\nThe raw predictions are the log-odds.\nFrom which we can compute the odds.\nFrom which the probability is computed. Here it is done by predict.\nThe .pred_class prediction is when the probability > 0.5.\n\nWhat does it mean for the odds and the log-odds?\n\n\n\nAnswers: odds > 1, log-odds > 0"
  },
  {
    "objectID": "W9.html#another-look",
    "href": "W9.html#another-look",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Another look",
    "text": "Another look\n\nemail_pred |> arrange(desc(spam_prob)) |> print(n = 20)\n\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1     0.903       2.23       9.29  1           1    \n 2     0.833       1.60       4.98  1           0    \n 3     0.825       1.55       4.71  1           1    \n 4     0.733       1.01       2.75  1           1    \n 5     0.683       0.766      2.15  1           1    \n 6     0.626       0.517      1.68  1           1    \n 7     0.614       0.464      1.59  1           0    \n 8     0.597       0.392      1.48  1           1    \n 9     0.538       0.153      1.17  1           1    \n10     0.537       0.148      1.16  1           0    \n11     0.510       0.0404     1.04  1           0    \n12     0.491      -0.0345     0.966 0           0    \n13     0.490      -0.0407     0.960 0           0    \n14     0.489      -0.0453     0.956 0           1    \n15     0.483      -0.0698     0.933 0           1    \n16     0.473      -0.107      0.899 0           0    \n17     0.463      -0.150      0.861 0           0    \n18     0.457      -0.174      0.840 0           0    \n19     0.447      -0.212      0.809 0           0    \n20     0.447      -0.214      0.808 0           1    \n# … with 765 more rows\n\n\nWe see false positives and false negatives."
  },
  {
    "objectID": "W9.html#evaluate-the-performance",
    "href": "W9.html#evaluate-the-performance",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nReceiver operating characteristic (ROC) curve1 which plots true positive rate (sensitivity) vs. false positive rate (1 - specificity)\n\nemail_pred |> roc_curve(\n    truth = spam, estimate = spam_prob,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n\n\n\n\nOriginally developed for operators of military radar receivers, hence the odd name."
  },
  {
    "objectID": "W9.html#evaluate-the-performance-1",
    "href": "W9.html#evaluate-the-performance-1",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nFind the area under the curve.\nIn calculus language: \\(\\int_0^1 \\text{TPR}(\\text{FPR}) d\\text{FPR}\\) where TPR = True Positive Rate and FPR = False Positive Rate.\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.857"
  },
  {
    "objectID": "W9.html#feature-engineering-1",
    "href": "W9.html#feature-engineering-1",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are critical to the success of the model\nFeature engineering is getting creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "W9.html#modeling-workflow-revisited",
    "href": "W9.html#modeling-workflow-revisited",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Modeling workflow, revisited",
    "text": "Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\n\nThe tidymodels way (similar to ways in python).\n\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "W9.html#initiate-a-recipe",
    "href": "W9.html#initiate-a-recipe",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsummary(email_rec) |> print(n = 21)\n\n# A tibble: 21 × 4\n   variable     type    role      source  \n   <chr>        <chr>   <chr>     <chr>   \n 1 to_multiple  nominal predictor original\n 2 from         nominal predictor original\n 3 cc           numeric predictor original\n 4 sent_email   nominal predictor original\n 5 time         date    predictor original\n 6 image        numeric predictor original\n 7 attach       numeric predictor original\n 8 dollar       numeric predictor original\n 9 winner       nominal predictor original\n10 inherit      numeric predictor original\n11 viagra       numeric predictor original\n12 password     numeric predictor original\n13 num_char     numeric predictor original\n14 line_breaks  numeric predictor original\n15 format       nominal predictor original\n16 re_subj      nominal predictor original\n17 exclaim_subj numeric predictor original\n18 urgent_subj  nominal predictor original\n19 exclaim_mess numeric predictor original\n20 number       nominal predictor original\n21 spam         nominal outcome   original\n\n\nThe object email_rec only includes meta-data (columns names and types)!"
  },
  {
    "objectID": "W9.html#remove-certain-variables",
    "href": "W9.html#remove-certain-variables",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Remove certain variables",
    "text": "Remove certain variables\n\nemail_rec |>\n  step_rm(from, sent_email, viagra)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra"
  },
  {
    "objectID": "W9.html#feature-engineer-date",
    "href": "W9.html#feature-engineer-date",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Feature engineer date",
    "text": "Feature engineer date\n\nThe date-time may not be such an interesting predictor.\n\nIt could only bring in a general trend over time\n\nOften decomposing the date to the month or the day of the week (dow) is more interesting.\n\nstep_date can easily extract these\n\n\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time"
  },
  {
    "objectID": "W9.html#create-dummy-variables",
    "href": "W9.html#create-dummy-variables",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Create dummy variables",
    "text": "Create dummy variables\n\nUse helper functions like all_nominal or all_outcomes from tidymodels for column selection.\n\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) \n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time\nDummy variables from all_nominal(), -all_outcomes()"
  },
  {
    "objectID": "W9.html#remove-zero-variance-variables",
    "href": "W9.html#remove-zero-variance-variables",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Remove zero variance variables",
    "text": "Remove zero variance variables\nVariables that contain only a single value.\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time\nDummy variables from all_nominal(), -all_outcomes()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "W9.html#full-recipe",
    "href": "W9.html#full-recipe",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Full recipe",
    "text": "Full recipe\n\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n) |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\nemail_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time\nDummy variables from all_nominal(), -all_outcomes()\nZero variance filter on all_predictors()\n\n\nThe object email_rec only includes meta-data of the data frame it shall work on (a formula, columns names and types)!"
  },
  {
    "objectID": "W9.html#define-model",
    "href": "W9.html#define-model",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Define model",
    "text": "Define model\n\nemail_mod <- logistic_reg() |> \n  set_engine(\"glm\")\n\nemail_mod\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "W9.html#define-workflow",
    "href": "W9.html#define-workflow",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Define workflow",
    "text": "Define workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nemail_wflow <- workflow() |> \n  add_model(email_mod) |> \n  add_recipe(email_rec)\nemail_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_date()\n• step_rm()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "W9.html#fit-model-to-training-data",
    "href": "W9.html#fit-model-to-training-data",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nemail_fit <- email_wflow |> \n  fit(data = train_data)\n\ntidy(email_fit) |> print(n = 27)\n\n# A tibble: 27 × 5\n   term            estimate std.error statistic  p.value\n   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)     -0.651     0.254     -2.57   1.03e- 2\n 2 cc               0.0214    0.0229     0.936  3.49e- 1\n 3 image           -2.99      1.31      -2.28   2.28e- 2\n 4 attach           0.512     0.116      4.41   1.03e- 5\n 5 dollar          -0.0651    0.0307    -2.12   3.40e- 2\n 6 inherit          0.440     0.205      2.15   3.14e- 2\n 7 password        -0.723     0.302     -2.39   1.67e- 2\n 8 num_char         0.0585    0.0240     2.43   1.50e- 2\n 9 line_breaks     -0.00548   0.00139   -3.94   8.24e- 5\n10 exclaim_subj     0.0998    0.268      0.373  7.09e- 1\n11 exclaim_mess     0.0103    0.00198    5.20   2.02e- 7\n12 to_multiple_X1  -2.56      0.339     -7.56   4.11e-14\n13 winner_yes       2.24      0.430      5.21   1.90e- 7\n14 format_X1       -0.953     0.157     -6.06   1.38e- 9\n15 re_subj_X1      -3.00      0.444     -6.76   1.39e-11\n16 urgent_subj_X1   3.69      1.15       3.20   1.37e- 3\n17 number_small    -0.840     0.162     -5.20   1.98e- 7\n18 number_big      -0.0915    0.244     -0.375  7.07e- 1\n19 time_dow_Mo     -0.326     0.303     -1.08   2.82e- 1\n20 time_dow_Di      0.0813    0.275      0.296  7.67e- 1\n21 time_dow_Mi     -0.260     0.275     -0.946  3.44e- 1\n22 time_dow_Do     -0.220     0.279     -0.788  4.31e- 1\n23 time_dow_Fr     -0.0612    0.275     -0.223  8.24e- 1\n24 time_dow_Sa      0.0646    0.292      0.221  8.25e- 1\n25 time_month_Feb   0.760     0.178      4.26   2.03e- 5\n26 time_month_Mär   0.506     0.178      2.85   4.40e- 3\n27 time_month_Apr -12.0     394.        -0.0306 9.76e- 1"
  },
  {
    "objectID": "W9.html#make-predictions-for-test-data",
    "href": "W9.html#make-predictions-for-test-data",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nemail_pred <- predict(email_fit, test_data, type = \"prob\") |> \n  bind_cols(test_data) \n\nemail_pred\n\n# A tibble: 785 × 23\n   .pred_0  .pred_1 spam  to_mul…¹ from     cc sent_…² time                image\n     <dbl>    <dbl> <fct> <fct>    <fct> <int> <fct>   <dttm>              <dbl>\n 1   0.993 0.00653  0     1        1         0 1       2012-01-01 18:55:06     0\n 2   0.998 0.00169  0     0        1         1 1       2012-01-01 20:38:32     0\n 3   0.987 0.0127   0     0        1         0 0       2012-01-02 06:42:16     0\n 4   0.999 0.000825 0     0        1         1 0       2012-01-02 16:12:51     0\n 5   0.991 0.00876  0     0        1         4 0       2012-01-02 17:45:36     0\n 6   0.878 0.122    0     0        1         0 0       2012-01-02 22:55:03     0\n 7   0.959 0.0414   0     0        1         0 0       2012-01-03 02:07:17     0\n 8   0.852 0.148    0     0        1         0 0       2012-01-03 06:41:35     0\n 9   0.938 0.0619   0     0        1         0 0       2012-01-03 17:02:35     0\n10   0.896 0.104    0     0        1         0 0       2012-01-03 12:14:51     0\n# … with 775 more rows, 14 more variables: attach <dbl>, dollar <dbl>,\n#   winner <fct>, inherit <dbl>, viagra <dbl>, password <dbl>, num_char <dbl>,\n#   line_breaks <int>, format <fct>, re_subj <fct>, exclaim_subj <dbl>,\n#   urgent_subj <fct>, exclaim_mess <dbl>, number <fct>, and abbreviated\n#   variable names ¹​to_multiple, ²​sent_email"
  },
  {
    "objectID": "W9.html#evaluate-the-performance-2",
    "href": "W9.html#evaluate-the-performance-2",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nemail_pred |>\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) |>\n  autoplot()"
  },
  {
    "objectID": "W9.html#evaluate-the-performance-3",
    "href": "W9.html#evaluate-the-performance-3",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nemail_pred |>\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.860\n\n\nThis is at least slightly better than our former model (without the feature engineering workflow), which had AUC = 0.857."
  },
  {
    "objectID": "W9.html#cutoff-probability-0.5",
    "href": "W9.html#cutoff-probability-0.5",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Cutoff probability: 0.5",
    "text": "Cutoff probability: 0.5\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5. (That is the default.)\nConfusion matrix:\n\ncutoff_prob <- 0.5\nemail_pred |>\n  mutate(\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(.pred_1 > cutoff_prob, \"Email labelled spam\", \"Email labelled not spam\")\n    ) |>\n  count(spam_pred, spam) |>\n  pivot_wider(names_from = spam, values_from = n) |>\n  knitr::kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail labelled not spam\n707\n54\n\n\nEmail labelled spam\n10\n14\n\n\n\n\n\nSensitivity: 14/(14+54) = 0.206\nSpecificity: 707/(707+10) = 0.986"
  },
  {
    "objectID": "W9.html#cutoff-probability-0.25",
    "href": "W9.html#cutoff-probability-0.25",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Cutoff probability: 0.25",
    "text": "Cutoff probability: 0.25\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25.\nConfusion matrix:\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail labelled not spam\n656\n36\n\n\nEmail labelled spam\n61\n32\n\n\n\n\n\nSensitivity: 32/(32+36) = 0.471\nSpecificity: 656/(656 + 61) = 0.915"
  },
  {
    "objectID": "W9.html#cutoff-probability-0.75",
    "href": "W9.html#cutoff-probability-0.75",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Cutoff probability: 0.75",
    "text": "Cutoff probability: 0.75\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75.\nConfusion matrix:\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail labelled not spam\n716\n65\n\n\nEmail labelled spam\n1\n3\n\n\n\n\n\nSensitivity: 3/(3+65) = 0.044\nSpecificity: 716/(716+1) = 0.999"
  },
  {
    "objectID": "W9.html#check-our-very-first-model",
    "href": "W9.html#check-our-very-first-model",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Check our very first model",
    "text": "Check our very first model\nWe make a new simple recipe and draw workflow and fitting re-using the same specified logisitc regression model email_mod.\n\nsimple_email_rec <- recipe(\n spam ~ num_char,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsimple_email_pred <- \n workflow() |> \n add_model(email_mod) |> \n add_recipe(simple_email_rec) |> \n fit(data = train_data) |> \n predict(test_data, type = \"prob\") |> \n bind_cols(test_data |> select(spam,num_char,time)) \nsimple_email_pred \n\n# A tibble: 785 × 5\n   .pred_0 .pred_1 spam  num_char time               \n     <dbl>   <dbl> <fct>    <dbl> <dttm>             \n 1   0.889  0.111  0        4.84  2012-01-01 18:55:06\n 2   0.936  0.0644 0       15.1   2012-01-01 20:38:32\n 3   0.945  0.0547 0       18.0   2012-01-02 06:42:16\n 4   0.989  0.0113 0       45.8   2012-01-02 16:12:51\n 5   0.922  0.0784 0       11.4   2012-01-02 17:45:36\n 6   0.868  0.132  0        1.48  2012-01-02 22:55:03\n 7   0.933  0.0667 0       14.4   2012-01-03 02:07:17\n 8   0.864  0.136  0        0.978 2012-01-03 06:41:35\n 9   0.905  0.0953 0        7.79  2012-01-03 17:02:35\n10   0.864  0.136  0        0.978 2012-01-03 12:14:51\n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#evaluate-the-performance-4",
    "href": "W9.html#evaluate-the-performance-4",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nsimple_email_pred |> roc_curve(\n    truth = spam, estimate = .pred_1,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.753\n\n\nConclusion: It is not as good compared to AUC 0.86\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W9.html#multinomial-response-variable",
    "href": "W9.html#multinomial-response-variable",
    "title": "W#9 Classification Problems, Logisitc Regression, Prediction",
    "section": "Multinomial response variable?",
    "text": "Multinomial response variable?\n\nWe will not cover other categorical variables than binary ones here.\nHowever, many of the probabilistic concepts transfer."
  },
  {
    "objectID": "index.html#week-9-oct-27-classification-problems-logisitc-regression-prediction",
    "href": "index.html#week-9-oct-27-classification-problems-logisitc-regression-prediction",
    "title": "Data Science Concepts / Tools",
    "section": "Week 9, Oct 27: Classification Problems, Logisitc Regression, Prediction",
    "text": "Week 9, Oct 27: Classification Problems, Logisitc Regression, Prediction\nSlides Week 9\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 due in 10 days."
  },
  {
    "objectID": "W10.html#model-1-predict-eu-attitudes",
    "href": "W10.html#model-1-predict-eu-attitudes",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Model 1: Predict EU attitudes",
    "text": "Model 1: Predict EU attitudes\n\ness <- ess_raw |> filter(essround == 9) |> \n select(cntry, euftf, atchctr, atcherp, imueclt, lrscale) |> \n mutate(euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99), \n        atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),\n        atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),\n        imueclt = imueclt |> na_if(77) |> na_if(88) |> na_if(99),\n        lrscale = lrscale |> na_if(77) |> na_if(88) |> na_if(99))\n\nFor the ESS dataset\n\nwe filter for people from round 9 (2018)\nselect 5 attitude variabels and ntry with 29 countries: AT, BE, BG, CH, CY, CZ, DE, DK, EE, ES, FI, FR, GB, HR, HU, IE, IS, IT, LT, LV, ME, NL, NO, PL, PT, RS, SE, SI, SK\nrecode NA’s properly for five variables:\neuftf: European Union: European unification go further (=10) or gone too far (=0)\natchctr: How emotionally attached to [country] (0 to 10)\natcherp: How emotionally attached to Europe (0 to 10)\nimueclt: Country’s cultural life undermined (=0) or enriched (=10) by immigrants\nlrscale: Placement on left (=0) right (=10) scale"
  },
  {
    "objectID": "W10.html#model-1.a-1.b-and-1.c",
    "href": "W10.html#model-1.a-1.b-and-1.c",
    "title": "W#10: Cross validation",
    "section": "Model 1.a, 1.b, and 1.c",
    "text": "Model 1.a, 1.b, and 1.c\n\nCreate an initial split with 80% training data\nCreate three recipes\n\ness_rec1 without using the country variable\ness_rec2 with main effects for all 29 countries\ness_rec3 with additional interaction effects for all 29 countries\n\n\n\ness_split <- initial_split(ess, prop = 0.80)\ness_train <- training(ess_split)\ness_test <- testing(ess_split)\n\ness_model <- linear_reg() |> set_engine(\"lm\")\ness_rec1 <- ess_train |> \n recipe(euftf ~ .) |> \n step_rm(cntry)\ness_rec2 <- ess_train |> \n recipe(euftf ~ .)\ness_rec3 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(all_nominal()) |> \n step_interact(~starts_with(\"cntry\"):c(atchctr,atcherp,imueclt,lrscale))\n\ness_wflow <- workflow() |> \n add_model(ess_model)\n\ness_fit1 <- ess_wflow |> \n add_recipe(ess_rec1) |> \n fit(ess_train)\ness_fit2 <- ess_wflow |> \n add_recipe(ess_rec2) |> \n fit(ess_train)\ness_fit3 <- ess_wflow |> \n add_recipe(ess_rec3) |> \n fit(ess_train)"
  },
  {
    "objectID": "W10.html#model-1-fits",
    "href": "W10.html#model-1-fits",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Model 1: Fits",
    "text": "Model 1: Fits\n\n\n\ntidy(ess_fit1) |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 5 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   2.92  \n2 atchctr      -0.0952\n3 atcherp       0.285 \n4 imueclt       0.287 \n5 lrscale      -0.0401\n\n\n\n\ntidy(ess_fit2)  |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 33 × 2\n   term        estimate\n   <chr>          <dbl>\n 1 (Intercept)  2.10   \n 2 atchctr     -0.101  \n 3 atcherp      0.294  \n 4 imueclt      0.299  \n 5 lrscale     -0.0196 \n 6 cntry_BE     0.747  \n 7 cntry_BG     1.20   \n 8 cntry_CH    -0.0476 \n 9 cntry_CY     1.39   \n10 cntry_CZ     0.184  \n11 cntry_DE     1.32   \n12 cntry_DK     1.01   \n13 cntry_EE     0.532  \n14 cntry_ES     1.14   \n15 cntry_FI     0.00996\n16 cntry_FR     0.743  \n17 cntry_GB    -0.0705 \n18 cntry_HR     0.729  \n19 cntry_HU     0.216  \n20 cntry_IE     0.294  \n21 cntry_IS     0.0371 \n22 cntry_IT     0.593  \n23 cntry_LT     2.15   \n24 cntry_LV     0.426  \n25 cntry_ME     2.41   \n26 cntry_NL     0.600  \n27 cntry_NO    -0.170  \n28 cntry_PL     1.19   \n29 cntry_PT     1.42   \n30 cntry_RS     1.21   \n31 cntry_SE    -0.0761 \n32 cntry_SI     1.53   \n33 cntry_SK     0.178  \n\n\n\n\ntidy(ess_fit3)  |> select(term, estimate) |> print(n = 145)\n\n# A tibble: 145 × 2\n    term               estimate\n    <chr>                 <dbl>\n  1 (Intercept)         2.65   \n  2 atchctr            -0.135  \n  3 atcherp             0.259  \n  4 imueclt             0.412  \n  5 lrscale            -0.141  \n  6 cntry_BE            0.0202 \n  7 cntry_BG           -0.264  \n  8 cntry_CH            1.70   \n  9 cntry_CY            0.242  \n 10 cntry_CZ           -0.914  \n 11 cntry_DE            0.248  \n 12 cntry_DK            0.637  \n 13 cntry_EE           -0.709  \n 14 cntry_ES            1.15   \n 15 cntry_FI            0.143  \n 16 cntry_FR            0.136  \n 17 cntry_GB            0.486  \n 18 cntry_HR            0.177  \n 19 cntry_HU           -0.190  \n 20 cntry_IE            0.186  \n 21 cntry_IS            0.881  \n 22 cntry_IT           -0.582  \n 23 cntry_LT            0.925  \n 24 cntry_LV           -1.60   \n 25 cntry_ME            0.542  \n 26 cntry_NL           -0.804  \n 27 cntry_NO            0.763  \n 28 cntry_PL            2.24   \n 29 cntry_PT            2.18   \n 30 cntry_RS            0.220  \n 31 cntry_SE           -0.507  \n 32 cntry_SI            1.19   \n 33 cntry_SK           -0.604  \n 34 cntry_BE_x_atchctr -0.0249 \n 35 cntry_BE_x_atcherp  0.128  \n 36 cntry_BE_x_imueclt -0.130  \n 37 cntry_BE_x_lrscale  0.148  \n 38 cntry_BG_x_atchctr  0.220  \n 39 cntry_BG_x_atcherp -0.0614 \n 40 cntry_BG_x_imueclt -0.159  \n 41 cntry_BG_x_lrscale  0.151  \n 42 cntry_CH_x_atchctr -0.0286 \n 43 cntry_CH_x_atcherp  0.0194 \n 44 cntry_CH_x_imueclt -0.197  \n 45 cntry_CH_x_lrscale -0.106  \n 46 cntry_CY_x_atchctr  0.0956 \n 47 cntry_CY_x_atcherp  0.0496 \n 48 cntry_CY_x_imueclt -0.336  \n 49 cntry_CY_x_lrscale  0.300  \n 50 cntry_CZ_x_atchctr -0.0375 \n 51 cntry_CZ_x_atcherp -0.0267 \n 52 cntry_CZ_x_imueclt  0.0735 \n 53 cntry_CZ_x_lrscale  0.273  \n 54 cntry_DE_x_atchctr  0.0299 \n 55 cntry_DE_x_atcherp  0.159  \n 56 cntry_DE_x_imueclt -0.127  \n 57 cntry_DE_x_lrscale  0.0836 \n 58 cntry_DK_x_atchctr -0.0884 \n 59 cntry_DK_x_atcherp  0.0752 \n 60 cntry_DK_x_imueclt  0.0109 \n 61 cntry_DK_x_lrscale  0.0970 \n 62 cntry_EE_x_atchctr  0.0895 \n 63 cntry_EE_x_atcherp  0.0464 \n 64 cntry_EE_x_imueclt -0.137  \n 65 cntry_EE_x_lrscale  0.180  \n 66 cntry_ES_x_atchctr  0.0735 \n 67 cntry_ES_x_atcherp -0.0706 \n 68 cntry_ES_x_imueclt -0.0704 \n 69 cntry_ES_x_lrscale  0.0210 \n 70 cntry_FI_x_atchctr -0.106  \n 71 cntry_FI_x_atcherp  0.0602 \n 72 cntry_FI_x_imueclt -0.104  \n 73 cntry_FI_x_lrscale  0.176  \n 74 cntry_FR_x_atchctr -0.0254 \n 75 cntry_FR_x_atcherp  0.185  \n 76 cntry_FR_x_imueclt -0.195  \n 77 cntry_FR_x_lrscale  0.139  \n 78 cntry_GB_x_atchctr -0.0968 \n 79 cntry_GB_x_atcherp  0.161  \n 80 cntry_GB_x_imueclt -0.196  \n 81 cntry_GB_x_lrscale  0.0630 \n 82 cntry_HR_x_atchctr  0.145  \n 83 cntry_HR_x_atcherp -0.0138 \n 84 cntry_HR_x_imueclt -0.213  \n 85 cntry_HR_x_lrscale  0.120  \n 86 cntry_HU_x_atchctr  0.0531 \n 87 cntry_HU_x_atcherp -0.0656 \n 88 cntry_HU_x_imueclt -0.0588 \n 89 cntry_HU_x_lrscale  0.160  \n 90 cntry_IE_x_atchctr  0.0630 \n 91 cntry_IE_x_atcherp -0.0230 \n 92 cntry_IE_x_imueclt -0.138  \n 93 cntry_IE_x_lrscale  0.0874 \n 94 cntry_IS_x_atchctr -0.186  \n 95 cntry_IS_x_atcherp  0.111  \n 96 cntry_IS_x_imueclt -0.0756 \n 97 cntry_IS_x_lrscale  0.0596 \n 98 cntry_IT_x_atchctr  0.0381 \n 99 cntry_IT_x_atcherp -0.0405 \n100 cntry_IT_x_imueclt  0.114  \n101 cntry_IT_x_lrscale  0.111  \n102 cntry_LT_x_atchctr  0.145  \n103 cntry_LT_x_atcherp -0.113  \n104 cntry_LT_x_imueclt -0.0917 \n105 cntry_LT_x_lrscale  0.219  \n106 cntry_LV_x_atchctr  0.130  \n107 cntry_LV_x_atcherp -0.0431 \n108 cntry_LV_x_imueclt -0.161  \n109 cntry_LV_x_lrscale  0.365  \n110 cntry_ME_x_atchctr  0.198  \n111 cntry_ME_x_atcherp  0.136  \n112 cntry_ME_x_imueclt -0.179  \n113 cntry_ME_x_lrscale  0.0701 \n114 cntry_NL_x_atchctr  0.0416 \n115 cntry_NL_x_atcherp  0.138  \n116 cntry_NL_x_imueclt -0.130  \n117 cntry_NL_x_lrscale  0.182  \n118 cntry_NO_x_atchctr -0.0643 \n119 cntry_NO_x_atcherp -0.0283 \n120 cntry_NO_x_imueclt -0.202  \n121 cntry_NO_x_lrscale  0.187  \n122 cntry_PL_x_atchctr -0.00472\n123 cntry_PL_x_atcherp  0.0737 \n124 cntry_PL_x_imueclt -0.231  \n125 cntry_PL_x_lrscale -0.0290 \n126 cntry_PT_x_atchctr  0.0377 \n127 cntry_PT_x_atcherp -0.114  \n128 cntry_PT_x_imueclt -0.178  \n129 cntry_PT_x_lrscale  0.126  \n130 cntry_RS_x_atchctr  0.139  \n131 cntry_RS_x_atcherp  0.0996 \n132 cntry_RS_x_imueclt -0.285  \n133 cntry_RS_x_lrscale  0.167  \n134 cntry_SE_x_atchctr  0.0493 \n135 cntry_SE_x_atcherp -0.00505\n136 cntry_SE_x_imueclt -0.138  \n137 cntry_SE_x_lrscale  0.160  \n138 cntry_SI_x_atchctr  0.199  \n139 cntry_SI_x_atcherp -0.165  \n140 cntry_SI_x_imueclt -0.256  \n141 cntry_SI_x_lrscale  0.204  \n142 cntry_SK_x_atchctr  0.0955 \n143 cntry_SK_x_atcherp -0.0549 \n144 cntry_SK_x_imueclt -0.0424 \n145 cntry_SK_x_lrscale  0.137  \n\n\n\n\nNote: We omit std.error, p-values and so on in the display here because they are usually small in this large dataset, and we will not look at them now."
  },
  {
    "objectID": "W10.html#recap-interpreting-with-interaction-effects",
    "href": "W10.html#recap-interpreting-with-interaction-effects",
    "title": "W#10: Cross validation",
    "section": "Recap interpreting with interaction effects",
    "text": "Recap interpreting with interaction effects\n\ness_train |> \n filter(cntry==\"AT\") |> \n select(-cntry) |> \n lm(formula = euftf ~ . ) |> \n tidy()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.92     0.377       7.74 1.68e-14\n2 atchctr       -0.176    0.0369     -4.77 1.98e- 6\n3 atcherp        0.276    0.0309      8.95 9.16e-19\n4 imueclt        0.402    0.0274     14.6  8.05e-46\n5 lrscale       -0.135    0.0354     -3.82 1.36e- 4"
  },
  {
    "objectID": "W10.html#ne",
    "href": "W10.html#ne",
    "title": "W#10: Cross validation",
    "section": "Ne",
    "text": "Ne\n\n\n# A tibble: 785 × 2\n   .pred_0   .pred_1\n     <dbl>     <dbl>\n 1   0.931 0.0688   \n 2   0.996 0.00444  \n 3   0.994 0.00555  \n 4   0.914 0.0856   \n 5   0.775 0.225    \n 6   0.670 0.330    \n 7   1.00  0.000330 \n 8   1.00  0.0000793\n 9   0.997 0.00339  \n10   0.900 0.0995   \n# … with 775 more rows\n\n\n\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [169/19]> Fold01\n 2 <split [169/19]> Fold02\n 3 <split [169/19]> Fold03\n 4 <split [169/19]> Fold04\n 5 <split [169/19]> Fold05\n 6 <split [169/19]> Fold06\n 7 <split [169/19]> Fold07\n 8 <split [169/19]> Fold08\n 9 <split [170/18]> Fold09\n10 <split [170/18]> Fold10\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#model-1.1-1.2-and-1.3",
    "href": "W10.html#model-1.1-1.2-and-1.3",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Model 1.1, 1.2, and 1.3",
    "text": "Model 1.1, 1.2, and 1.3\n\n\n\nCreate an initial split with 80% training data\nCreate a linear model ess_mod\nCreate three recipes\n\ness_rec1 without using the country variable\ness_rec2 with main effects for all 29 countries\ness_rec3 with additional interaction effects for all 29 countries\n\nCreate the workflow\nFit 3 models by adding the 3 recipes\n\n\n\nset.seed(7)\ness_split <- initial_split(ess, prop = 0.80)\ness_train <- training(ess_split)\ness_test <- testing(ess_split)\n\ness_model <- linear_reg() |> set_engine(\"lm\")\ness_rec1 <- ess_train |> \n recipe(euftf ~ .) |> \n step_rm(cntry)\ness_rec2 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry)\ness_rec3 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry) |> \n step_interact(~starts_with(\"cntry\"):c(atchctr,atcherp,imueclt,lrscale))\n\ness_wflow <- workflow() |> \n add_model(ess_model)\n\ness_fit1 <- ess_wflow |> \n add_recipe(ess_rec1) |> \n fit(ess_train)\ness_fit2 <- ess_wflow |> \n add_recipe(ess_rec2) |> \n fit(ess_train)\ness_fit3 <- ess_wflow |> \n add_recipe(ess_rec3) |> \n fit(ess_train)"
  },
  {
    "objectID": "W10.html#recap-interpreting-interaction-effects",
    "href": "W10.html#recap-interpreting-interaction-effects",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Recap: Interpreting interaction effects",
    "text": "Recap: Interpreting interaction effects\nIn Model 3 the reference country is Austria (AT) therefore the intercept and main coefficient are valid for Austria and all interaction coefficients have to be added to these to be interpreted.\nCross check: A linear model with the data filtered for Austria only without a country effect:\n\ness_train |> \n filter(cntry==\"AT\") |> \n select(-cntry) |> \n lm(formula = euftf ~ . ) |> \n tidy()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.65     0.364       7.28 5.06e-13\n2 atchctr       -0.135    0.0353     -3.83 1.34e- 4\n3 atcherp        0.259    0.0296      8.76 4.51e-18\n4 imueclt        0.412    0.0268     15.3  6.49e-50\n5 lrscale       -0.141    0.0338     -4.16 3.34e- 5\n\n\nThe coefficients are identical to the full model with all interaction effects."
  },
  {
    "objectID": "W10.html#make-predictions-for-test-data",
    "href": "W10.html#make-predictions-for-test-data",
    "title": "W#10: Cross validation",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#make-predictions-for-training-data",
    "href": "W10.html#make-predictions-for-training-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\ness_train_pred1 <- predict(ess_fit1, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred1\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.27     3 HR          8       7       5       8\n 2  4.93     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.20    NA BG         10       7       5       5\n 6  5.67     8 BG          9       6       8      10\n 7  5.52     3 AT         10      10       3       4\n 8  7.22     3 FR          9       8      10       0\n 9  4.17     1 FI          8       5       3       7\n10  4.47     4 BG         10       7       3       9\n# … with 39,605 more rows\n\n\nNote:\n\nWe can make predictions when the response in NA\nWe cannot make predictions when on predictor is NA"
  },
  {
    "objectID": "W10.html#make-predictions-for-training-data-1",
    "href": "W10.html#make-predictions-for-training-data-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\n\n\ness_train_pred2 <- predict(ess_fit2, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred2\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.42     3 HR          8       7       5       8\n 2  5.53     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.75    NA BG         10       7       5       5\n 6  6.36     8 BG          9       6       8      10\n 7  4.86     3 AT         10      10       3       4\n 8  7.28     3 FR          9       8      10       0\n 9  3.54     1 FI          8       5       3       7\n10  5.08     4 BG         10       7       3       9\n# … with 39,605 more rows\n\n\n\n\ness_train_pred3 <- predict(ess_fit3, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred3\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.45     3 HR          8       7       5       8\n 2  5.22     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.94    NA BG         10       7       5       5\n 6  6.47     8 BG          9       6       8      10\n 7  4.56     3 AT         10      10       3       4\n 8  7.06     3 FR          9       8      10       0\n 9  3.63     1 FI          8       5       3       7\n10  5.47     4 BG         10       7       3       9\n# … with 39,605 more rows"
  },
  {
    "objectID": "W10.html#r-squared",
    "href": "W10.html#r-squared",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "R-squared",
    "text": "R-squared\nRecap R-squared: Percentage of variability in euftf explained by the model\n\n\nrsq(ess_train_pred1, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.178\n\n\n\n\n\nrsq(ess_train_pred2, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.230\n\n\n\n\n\nrsq(ess_train_pred3, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.253\n\n\n\nWhich model is better in prediction?"
  },
  {
    "objectID": "W10.html#root-mean-squared-error-rmse",
    "href": "W10.html#root-mean-squared-error-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Root mean squared error (RMSE)",
    "text": "Root mean squared error (RMSE)\nRMSE is an alternative measure of performance.\n\\[\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}\\]\nwhere \\(\\hat{y}_i\\) is the predicted value and \\(y_i\\) the true value.\n(The name RMSE pretty much describes what the measure does.)\n\n\nrmse(ess_train_pred1, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.44\n\n\n\n\n\nrmse(ess_train_pred2, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.36\n\n\n\n\n\nrmse(ess_train_pred3, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.32\n\n\n\nShould we prefer larger or lower RMSE?"
  },
  {
    "objectID": "W10.html#interpreting-rmse",
    "href": "W10.html#interpreting-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\nIn contrast to R-squared, RMSE can only be interpreted with knowledge about the range and of the response variable.\nThe values of euftf range from 0 to 10\n\nThe RMSE of 2.3244529 shows how much predicted values deviate from the true value on average."
  },
  {
    "objectID": "W10.html#make-predictions-for-testing-data",
    "href": "W10.html#make-predictions-for-testing-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\ness_test_pred3 <- predict(ess_fit3, ess_test) |> \n bind_cols(ess_test |> select(euftf, everything()))\ness_test_pred3\n\n# A tibble: 9,904 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  3.93     5 AT          9       6       4       5\n 2 NA        2 AT         10       3       7      NA\n 3  4.49     4 AT          5       5       4       3\n 4  2.56     3 AT          8       6       0       4\n 5  7.73    10 AT         10      10      10       2\n 6  6.66     8 AT          5       7       8       3\n 7  2.87     2 AT         10       4       3       5\n 8  3.43     5 AT         10       3       5       5\n 9  7.19    10 AT          7       9       8       1\n10  3.24     8 AT         10       6       3       6\n# … with 9,894 more rows"
  },
  {
    "objectID": "W10.html#training-vs.-testing-data-prediction",
    "href": "W10.html#training-vs.-testing-data-prediction",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Training vs. testing data prediction",
    "text": "Training vs. testing data prediction\n\n\n\n\n\nModel\nMetric\nTrain\nTest\n\n\n\n\n1\nR-squared\n0.178\n0.169\n\n\n1\nRMSE\n2.440\n2.451\n\n\n2\nR-squared\n0.230\n0.216\n\n\n2\nRMSE\n2.360\n2.381\n\n\n3\nR-squared\n0.253\n0.242\n\n\n3\nRMSE\n2.324\n2.340\n\n\n\n\n\n\nR-squared is a little lower in the test data, RMSE a bit higher (both mean lower performance)\nOften, metrics are worse for the testing data, as here.\nHowever, in it can also be the other way round by chance."
  },
  {
    "objectID": "W10.html#how-to-evaluate-performance-on-training-data",
    "href": "W10.html#how-to-evaluate-performance-on-training-data",
    "title": "W#10: Cross validation",
    "section": "How to evaluate performance on training data?",
    "text": "How to evaluate performance on training data?\n\nModel performance changes with the random selection of the training data. How can we then reliably compare models?\nAnyway, the training data is not a good source for model performance. It is not an independent piece of information. Predicting the training data only reveals what the model already “knows”.\nAlso, we should save the testing data only for the final validation, so we should not use it systematically to compare models.\n\nA solution: Cross validation"
  },
  {
    "objectID": "W10.html#cross-validation-1",
    "href": "W10.html#cross-validation-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, \\(v\\)-fold cross validation:\n\nShuffle your data and make a partition with \\(v\\) parts\n\nRecall from set theory: A partition is a division of a set into mutually disjoint parts which union cover the whole set. Here applied to observations (rows) in a data frame.\n\nUse 1 part for validation, and the remaining \\(v-1\\) parts for training\nRepeat \\(v\\) times"
  },
  {
    "objectID": "W10.html#split-data-into-folds",
    "href": "W10.html#split-data-into-folds",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Split data into folds",
    "text": "Split data into folds\nWe split the ess data into ten parts.\n\n\n\nfolds <- vfold_cv(ess_train, v = 10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits               id    \n   <list>               <chr> \n 1 <split [35653/3962]> Fold01\n 2 <split [35653/3962]> Fold02\n 3 <split [35653/3962]> Fold03\n 4 <split [35653/3962]> Fold04\n 5 <split [35653/3962]> Fold05\n 6 <split [35654/3961]> Fold06\n 7 <split [35654/3961]> Fold07\n 8 <split [35654/3961]> Fold08\n 9 <split [35654/3961]> Fold09\n10 <split [35654/3961]> Fold10"
  },
  {
    "objectID": "W10.html#cross-validation-2",
    "href": "W10.html#cross-validation-2",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "W10.html#fit-resamples",
    "href": "W10.html#fit-resamples",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Fit resamples",
    "text": "Fit resamples\nWe use the workflow (model plus formula and recipe) we have on the folds with fit_resamples.\n\ness_fit3_rs <- ess_wflow |> add_recipe(ess_rec3) |> \n fit_resamples(folds)\ness_fit3_rs\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits               id     .metrics         .notes          \n   <list>               <chr>  <list>           <list>          \n 1 <split [35653/3962]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]>\n 2 <split [35653/3962]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]>\n 3 <split [35653/3962]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]>\n 4 <split [35653/3962]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]>\n 5 <split [35653/3962]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]>\n 6 <split [35654/3961]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]>\n 7 <split [35654/3961]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]>\n 8 <split [35654/3961]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]>\n 9 <split [35654/3961]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]>\n10 <split [35654/3961]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]>\n\n\nThis computes a set of performance metrics for each folds. For linear models the defaults are R-squared and RMSE."
  },
  {
    "objectID": "W10.html#collect-the-metrics",
    "href": "W10.html#collect-the-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Collect the metrics",
    "text": "Collect the metrics\n\ness_fit3_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   2.34     10 0.00794 Preprocessor1_Model1\n2 rsq     standard   0.245    10 0.00667 Preprocessor1_Model1\n\n\nThese values are indeed closer to the values we got for the test data."
  },
  {
    "objectID": "W10.html#deeper-look-into-the-metrics",
    "href": "W10.html#deeper-look-into-the-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Deeper look into the metrics",
    "text": "Deeper look into the metrics\n\n\n\ness_fit3_rs |> collect_metrics(summarize = FALSE)\n\n# A tibble: 20 × 5\n   id     .metric .estimator .estimate .config             \n   <chr>  <chr>   <chr>          <dbl> <chr>               \n 1 Fold01 rmse    standard       2.29  Preprocessor1_Model1\n 2 Fold01 rsq     standard       0.278 Preprocessor1_Model1\n 3 Fold02 rmse    standard       2.35  Preprocessor1_Model1\n 4 Fold02 rsq     standard       0.238 Preprocessor1_Model1\n 5 Fold03 rmse    standard       2.33  Preprocessor1_Model1\n 6 Fold03 rsq     standard       0.258 Preprocessor1_Model1\n 7 Fold04 rmse    standard       2.38  Preprocessor1_Model1\n 8 Fold04 rsq     standard       0.249 Preprocessor1_Model1\n 9 Fold05 rmse    standard       2.34  Preprocessor1_Model1\n10 Fold05 rsq     standard       0.250 Preprocessor1_Model1\n11 Fold06 rmse    standard       2.32  Preprocessor1_Model1\n12 Fold06 rsq     standard       0.261 Preprocessor1_Model1\n13 Fold07 rmse    standard       2.36  Preprocessor1_Model1\n14 Fold07 rsq     standard       0.215 Preprocessor1_Model1\n15 Fold08 rmse    standard       2.35  Preprocessor1_Model1\n16 Fold08 rsq     standard       0.214 Preprocessor1_Model1\n17 Fold09 rmse    standard       2.33  Preprocessor1_Model1\n18 Fold09 rsq     standard       0.257 Preprocessor1_Model1\n19 Fold10 rmse    standard       2.35  Preprocessor1_Model1\n20 Fold10 rsq     standard       0.226 Preprocessor1_Model1\n\n\n\n\n\n\n\n\nid\nrmse\nrsq\n\n\n\n\nFold01\n2.286402\n0.2780252\n\n\nFold02\n2.352520\n0.2378150\n\n\nFold03\n2.334960\n0.2579689\n\n\nFold04\n2.376705\n0.2491427\n\n\nFold05\n2.338995\n0.2503009\n\n\nFold06\n2.315747\n0.2607719\n\n\nFold07\n2.362725\n0.2147322\n\n\nFold08\n2.347958\n0.2137093\n\n\nFold09\n2.331361\n0.2568661\n\n\nFold10\n2.345771\n0.2258870"
  },
  {
    "objectID": "W10.html#cross-validation-for-logistic-regression",
    "href": "W10.html#cross-validation-for-logistic-regression",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation for logistic regression",
    "text": "Cross validation for logistic regression\nTake 2 simple models predicting the sex of penguins.\n\nlibrary(palmerpenguins)\npenguins <- na.omit(penguins)\nset.seed(9999)\npeng_split <- initial_split(penguins, prob = 0.8)\npeng_train <- training(peng_split)\npeng_test <- testing(peng_split)\npeng_folds <- vfold_cv(peng_train, v = 5)\n\npeng_rec1 <- peng_train |> \n recipe(sex ~ flipper_length_mm + body_mass_g, family = \"binomial\")\npeng_rec2 <- peng_train |> \n recipe(sex ~ bill_depth_mm + bill_length_mm, family = \"binomial\")  \npeng_mod <- logistic_reg() |> set_engine(\"glm\")\npeng_wflow1 <- workflow() |> add_model(peng_mod) |> add_recipe(peng_rec1)\npeng_wflow2 <- workflow() |> add_model(peng_mod) |> add_recipe(peng_rec2)\n\npeng_fit1 <- peng_wflow1 |> fit(peng_train)\npeng_fit2 <- peng_wflow2 |> fit(peng_train)\n\n\n\n\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        8.72     3.02          2.89 3.90e- 3\n2 flipper_length_mm -0.0988   0.0225       -4.38 1.16e- 5\n3 body_mass_g        0.00267  0.000437      6.12 9.59e-10\n\n\n\n\n\n# A tibble: 3 × 5\n  term           estimate std.error statistic  p.value\n  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     -24.4      3.29       -7.40 1.33e-13\n2 bill_depth_mm     0.791    0.111       7.13 1.01e-12\n3 bill_length_mm    0.249    0.0407      6.13 8.97e-10"
  },
  {
    "objectID": "W10.html#accuracy",
    "href": "W10.html#accuracy",
    "title": "W#10: Cross validation",
    "section": "Accuracy",
    "text": "Accuracy\n\nAccuracy is the fraction of correct predictions: (TP + TN) / (TP + FP + FN + TN)\nRecall:\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)"
  },
  {
    "objectID": "W10.html#how-to-evaluate-performance-on-training-data-only",
    "href": "W10.html#how-to-evaluate-performance-on-training-data-only",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "How to evaluate performance on training data only?",
    "text": "How to evaluate performance on training data only?\n\nModel performance changes with the random selection of the training data. How can we then reliably compare models?\nAnyway, the training data is not a good source for model performance. It is not an independent piece of information. Predicting the training data only reveals what the model already “knows”.\nAlso, we should save the testing data only for the final validation, so we should not use it systematically to compare models.\n\nA solution: Cross validation"
  },
  {
    "objectID": "W10.html#cross-validation-for-logistic-regression-1",
    "href": "W10.html#cross-validation-for-logistic-regression-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation for logistic regression",
    "text": "Cross validation for logistic regression\n\npeng_fit1_rs <- peng_wflow1 |> fit_resamples(peng_folds)\npeng_fit2_rs <- peng_wflow2 |> fit_resamples(peng_folds)\npeng_fit1_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.654     5  0.0298 Preprocessor1_Model1\n2 roc_auc  binary     0.755     5  0.0268 Preprocessor1_Model1\n\npeng_fit2_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.783     5  0.0240 Preprocessor1_Model1\n2 roc_auc  binary     0.857     5  0.0123 Preprocessor1_Model1\n\n\n\nFor the logistic regression fit_resamples has two performance measures per default: AUC (area under the ROC-curve) and accuracy.\nThe model using the two variables about penguin bills performs better than the model using body mass and flipper length."
  },
  {
    "objectID": "W10.html#accuracy-for-classifiers",
    "href": "W10.html#accuracy-for-classifiers",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Accuracy for classifiers",
    "text": "Accuracy for classifiers\n\n\nAccuracy is the fraction of correct predictions:\n(TP + TN) / (TP + FP + FN + TN)\n\nAccuracy is a good overall performance measure but it does not specify if errors are false positives or false negatives.\n\nFor logistic regression:\n\n\n\n\n\nAccuracy relies a decision threshold (default 50%). It has an easy interpretation.\nAUC is the area under the ROC-curve sweeping over all possible thresholds.\n\nRecall:\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)"
  },
  {
    "objectID": "W10.html#comparison-of-metrics",
    "href": "W10.html#comparison-of-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\n\n\nFlipper length, body mass\n\npeng_fit1_rs |> collect_metrics(summarize = FALSE) |> select(-.config) |> arrange(.metric)\n\n# A tibble: 10 × 4\n   id    .metric  .estimator .estimate\n   <chr> <chr>    <chr>          <dbl>\n 1 Fold1 accuracy binary         0.76 \n 2 Fold2 accuracy binary         0.6  \n 3 Fold3 accuracy binary         0.62 \n 4 Fold4 accuracy binary         0.68 \n 5 Fold5 accuracy binary         0.612\n 6 Fold1 roc_auc  binary         0.851\n 7 Fold2 roc_auc  binary         0.689\n 8 Fold3 roc_auc  binary         0.745\n 9 Fold4 roc_auc  binary         0.728\n10 Fold5 roc_auc  binary         0.761\n\npeng_test_pred1 <- predict(peng_fit1, new_data = peng_test, type = \"prob\") |> \n mutate(.pred_class_0.5 = if_else(.pred_female > 0.5, \"female\", \"male\") |> factor(),\n        .pred_class_0.45 = if_else(.pred_female > 0.45, \"female\", \"male\") |> factor(),\n        .pred_class_0.55 = if_else(.pred_female > 0.55, \"female\", \"male\") |> factor()\n        ) |> \n bind_cols(peng_test |> select(sex, everything())) \npeng_test_pred1 |> roc_auc(truth = sex, estimate = .pred_female)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.830\n\n\n\nBill length and depth\n\npeng_fit2_rs |> collect_metrics(summarize = FALSE) |> select(-.config) |> arrange(.metric)\n\n# A tibble: 10 × 4\n   id    .metric  .estimator .estimate\n   <chr> <chr>    <chr>          <dbl>\n 1 Fold1 accuracy binary         0.8  \n 2 Fold2 accuracy binary         0.86 \n 3 Fold3 accuracy binary         0.78 \n 4 Fold4 accuracy binary         0.76 \n 5 Fold5 accuracy binary         0.714\n 6 Fold1 roc_auc  binary         0.851\n 7 Fold2 roc_auc  binary         0.889\n 8 Fold3 roc_auc  binary         0.881\n 9 Fold4 roc_auc  binary         0.840\n10 Fold5 roc_auc  binary         0.825\n\npeng_test_pred2 <- predict(peng_fit2, new_data = peng_test, type = \"prob\") |> \n mutate(.pred_class_0.5 = if_else(.pred_female > 0.5, \"female\", \"male\") |> factor(),\n        .pred_class_0.45 = if_else(.pred_female > 0.45, \"female\", \"male\") |> factor(),\n        .pred_class_0.55 = if_else(.pred_female > 0.55, \"female\", \"male\") |> factor()\n        ) |> \n bind_cols(peng_test |> select(sex, everything())) \npeng_test_pred2 |> roc_auc(truth = sex, estimate = .pred_female)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.839"
  },
  {
    "objectID": "W10.html#galtons-data",
    "href": "W10.html#galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Galton’s data",
    "text": "Galton’s data\nWhat is the weight of the meat of this ox?\n\nlibrary(readxl)\ngalton <- read_excel(\"data/galton_data.xlsx\")\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\")\n\n\n\n\n787 estimates, true value 1198, mean 1196.7\n\n\nWe focus on the arithmetic mean as aggregation function for the wisdom of the crowd here."
  },
  {
    "objectID": "W10.html#rmse-galtons-data",
    "href": "W10.html#rmse-galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "RMSE Galton’s data",
    "text": "RMSE Galton’s data\nDescribe the estimation game as a predictive model:\n\nAll estimates are made to predict the same value: the truth.\nThe truth is the same for all here.\n\n\nrmse_galton <- galton |> \n mutate(truth = 1198) |>\n rmse(truth = truth, estimate = Estimate)\nrmse_galton\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        73.6"
  },
  {
    "objectID": "W10.html#the-diversity-prediction-theorem",
    "href": "W10.html#the-diversity-prediction-theorem",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "The diversity prediction theorem1",
    "text": "The diversity prediction theorem1\n\nMSE is a measure the average individuals error\nBias-squared is a measure the collective error\nVariance is a measure for the diversity of estimates around the mean\n\nThe mathematical relation \\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\] can be formulated as\nCollective error = Individual error - Diversity\nInterpretation: The higher the diversity the lower the collective error!\nNotion from: Page, S. E. (2007). The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press."
  },
  {
    "objectID": "W10.html#mse-variance-and-bias-of-estimates",
    "href": "W10.html#mse-variance-and-bias-of-estimates",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "MSE, Variance, and Bias of estimates",
    "text": "MSE, Variance, and Bias of estimates\nLet us look at the following measures\n\n\\(\\text{MSE} = \\text{RMSE}^2 = \\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\) where \\(y_i = \\text{truth}\\) here for all \\(i\\)\n\\(\\text{Variance} = \\frac{1}{n}\\sum_{i = 1}^n (y_i - \\bar{y})^2\\) where \\(\\bar{y}\\) is the mean, so the aggregated Wisdom of Crowd estimate here\n\\(\\text{Bias-squared} = (\\bar{y} - \\hat{y})^2\\) which is the square difference between truth and mean estimate.\n\nThere a mathematical relation (good math excerise to check):\n\\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\]"
  },
  {
    "objectID": "W10.html#testing-for-galtons-data",
    "href": "W10.html#testing-for-galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Testing for Galton’s data",
    "text": "Testing for Galton’s data\n\\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\]\n\nMSE <- (rmse_galton$.estimate)^2 \nMSE\n\n[1] 5409.795\n\nVariance <- var(galton$Estimate)*(nrow(galton)-1)/nrow(galton)\nVariance\n\n[1] 5408.132\n\n# Note, we had to correct for the divisor (n-1) in the classical statistical definition\n# to get the sample variance instead of the estimate for the population variance\nBias_squared <- (mean(galton$Estimate) - 1198)^2\nBias_squared\n\n[1] 1.663346\n\nBias_squared + Variance\n\n[1] 5409.795\n\n\n\n\nSuch nice mathematical properties are probably one reason why these squared measures are so popular."
  },
  {
    "objectID": "W10.html#why-is-this-message-a-bit-suggestive",
    "href": "W10.html#why-is-this-message-a-bit-suggestive",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Why is this message a bit suggestive?",
    "text": "Why is this message a bit suggestive?\nThe mathematical relation \\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\] can be formulated as\nCollective error = Individual error - Diversity\nInterpretation: The higher the diversity the lower the collective error!\n\n\n\\(\\text{MSE}\\) and \\(\\text{Variance}\\) are not independent!\nActivities to increase diversity (Variance) typically also increase the average individual error (MSE).\nFor example, if we just add more random estimates with same mean but wild variance to our sample we increase both and do not gain any decrease of the collective error."
  },
  {
    "objectID": "W10.html#accuracy-for-numerical-estimate",
    "href": "W10.html#accuracy-for-numerical-estimate",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Accuracy for numerical estimate",
    "text": "Accuracy for numerical estimate\n\nFor binary classifiers accuracy has a simple definition: Fraction of correct classifications.\n\nIt can be further informed by other more specific measures taken from the confusion matrix (sensitivity, specificity)\n\n\nHow about numerical estimators?\nFor example outcomes of estimation games, or linear regression models.\n\nAccuracy is for example measured by (R)MSE\n\\(\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\) shows us that we can make a\nbias-variance decomposition\nThat means some part of the error is a systematic (the bias) and another part due to random variation (the variance).\nLearn more about the bias-variance tradeoff in statistical learning independently!"
  },
  {
    "objectID": "W10.html#d-accuracy-trueness-and-precision",
    "href": "W10.html#d-accuracy-trueness-and-precision",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "2-d Accuracy: Trueness and Precision",
    "text": "2-d Accuracy: Trueness and Precision\nAccording to ISO 5725-1 Standard: Accuracy (trueness and precision) of measurement methods and results - Part 1: General principles and definitions. there are two dimension of accuracy of numerical measurement."
  },
  {
    "objectID": "W10.html#section",
    "href": "W10.html#section",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "",
    "text": "JU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#what-is-a-wise-crowd",
    "href": "W10.html#what-is-a-wise-crowd",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "What is a wise crowd?",
    "text": "What is a wise crowd?\nAssume the dots are estimates. Which is a wise crowd?\n\n\n\nOf course, high trueness and high precision! But, …\nFocusing on the crowd being wise instead of its individuals: High trueness, low precision."
  },
  {
    "objectID": "W10.html#organ-donors",
    "href": "W10.html#organ-donors",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Organ donors",
    "text": "Organ donors\nPeople providing an organ for donation sometimes seek the help of a special “medical consultant”. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients.\nOne consultant tried to attract patients by noting that the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!)."
  },
  {
    "objectID": "W10.html#data",
    "href": "W10.html#data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Data",
    "text": "Data\n\norgan_donor <- tibble(\n  outcome = c(rep(\"complication\", 3), rep(\"no complication\", 59))\n)\n\n\norgan_donor |>\n  count(outcome)\n\n# A tibble: 2 × 2\n  outcome             n\n  <chr>           <int>\n1 complication        3\n2 no complication    59"
  },
  {
    "objectID": "W10.html#parameter-vs.-statistic",
    "href": "W10.html#parameter-vs.-statistic",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Parameter vs. statistic",
    "text": "Parameter vs. statistic\nA parameter for a hypothesis test is the “true” value of interest. We typically estimate the parameter using a sample statistic as a point estimate.\n\\(p\\): true rate of complication, here 0.1 (10% complication rate in US)\n\\(\\hat{p}\\): rate of complication in the sample = \\(\\frac{3}{62}\\) = 0.048"
  },
  {
    "objectID": "W10.html#correlation-vs.-causation",
    "href": "W10.html#correlation-vs.-causation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Correlation vs. causation",
    "text": "Correlation vs. causation\nIs it possible to assess the consultant’s claim using the data?\nNo. The claim is: There is a causal connection, but the data are observational. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate (for example).\nWhile it is not possible to assess the causal claim, it is still possible to test for an association using these data. For this question we ask, could the low complication rate of \\(\\hat{p}\\) = 0.048 be due to chance?"
  },
  {
    "objectID": "W10.html#two-claims",
    "href": "W10.html#two-claims",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Two claims",
    "text": "Two claims\n\nNull hypothesis: “There is nothing going on”\n\nComplication rate for this consultant is no different than the US average of 10%\n\nAlternative hypothesis: “There is something going on”\n\nComplication rate for this consultant is lower than the US average of 10%"
  },
  {
    "objectID": "W10.html#hypothesis-testing-as-a-court-trial",
    "href": "W10.html#hypothesis-testing-as-a-court-trial",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "W10.html#hypothesis-testing-framework",
    "href": "W10.html#hypothesis-testing-framework",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\), that represents the status quo\nSet an alternative hypothesis, \\(H_A\\), that represents the research question, i.e. what we are testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value.\nDefinition: Probability of observed or more extreme outcome given that the null hypothesis is true.\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "W10.html#setting-the-hypotheses",
    "href": "W10.html#setting-the-hypotheses",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Setting the hypotheses",
    "text": "Setting the hypotheses\nWhich of the following is the correct set of hypotheses for the claim that the consultant has lower complication rates?\n\n\\(H_0: p = 0.10\\); \\(H_A: p \\ne 0.10\\)\n\\(H_0: p = 0.10\\); \\(H_A: p > 0.10\\)\n\\(H_0: p = 0.10\\); \\(H_A: p < 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} \\ne 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} > 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} < 0.10\\)\n\n\nCorrect is c. Hypotheses are be about the true rate of complication \\(p\\) not the observed ones \\(\\hat{p}\\)"
  },
  {
    "objectID": "W10.html#simulating-the-null-distribution",
    "href": "W10.html#simulating-the-null-distribution",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Simulating the null distribution",
    "text": "Simulating the null distribution\nSince \\(H_0: p = 0.10\\), we need to simulate a null distribution where the probability of success (complication) for each trial (patient) is 0.10.\nHow should we simulate the null distribution for this study using a bag of chips?\n\nHow many chips? For example 10 which makes 10% choices possible\nHow many colors? 2\nWhat should colors represent? “complication”, “no complication”\nHow many draws? 62 as the data\nWith replacement or without replacement? With replacement\n\nWhen sampling from the null distribution, what would be the expected proportion of “complications”? 0.1"
  },
  {
    "objectID": "W10.html#what-do-we-expect",
    "href": "W10.html#what-do-we-expect",
    "title": "W#10: Cross validation",
    "section": "What do we expect?",
    "text": "What do we expect?\n.question[ When sampling from the null distribution, what is the expected proportion of success (complications)?]"
  },
  {
    "objectID": "W10.html#simulation",
    "href": "W10.html#simulation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Simulation!",
    "text": "Simulation!\n\nset.seed(1234)\noutcomes <- c(\"complication\", \"no complication\")\nsim1 <- sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\nsim1\n\n [1] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [5] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [9] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[13] \"no complication\" \"complication\"    \"no complication\" \"no complication\"\n[17] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[21] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[25] \"no complication\" \"no complication\" \"no complication\" \"complication\"   \n[29] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[33] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[37] \"no complication\" \"no complication\" \"complication\"    \"no complication\"\n[41] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[45] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[49] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[53] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[57] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[61] \"no complication\" \"no complication\"\n\nsum(sim1 == \"complication\")/62\n\n[1] 0.0483871\n\n\nOh OK, this was is pretty close to the consultant’s rate. But maybe it was a rare event?"
  },
  {
    "objectID": "W10.html#more-simulation",
    "href": "W10.html#more-simulation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "More simulation!",
    "text": "More simulation!\n\none_sim <- function() sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1290323\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1290323\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.09677419\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.09677419\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1774194\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1129032\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1129032"
  },
  {
    "objectID": "W10.html#automating-with-tidymodels",
    "href": "W10.html#automating-with-tidymodels",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Automating with tidymodels1",
    "text": "Automating with tidymodels1\n\n\n\norgan_donor\n\n# A tibble: 62 × 1\n   outcome        \n   <chr>          \n 1 complication   \n 2 complication   \n 3 complication   \n 4 no complication\n 5 no complication\n 6 no complication\n 7 no complication\n 8 no complication\n 9 no complication\n10 no complication\n# … with 52 more rows\n\n\n\n\nset.seed(10)\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 100, type = \"draw\") |> \n  calculate(stat = \"prop\")\nnull_dist\n\nResponse: outcome (factor)\nNull Hypothesis: point\n# A tibble: 100 × 2\n   replicate   stat\n   <fct>      <dbl>\n 1 1         0.0323\n 2 2         0.0645\n 3 3         0.0968\n 4 4         0.0161\n 5 5         0.161 \n 6 6         0.0968\n 7 7         0.0645\n 8 8         0.129 \n 9 9         0.161 \n10 10        0.0968\n# … with 90 more rows\n\n\n\n\nOf course, you can also do it in your own way without packages."
  },
  {
    "objectID": "W10.html#visualizing-the-null-distribution",
    "href": "W10.html#visualizing-the-null-distribution",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Visualizing the null distribution",
    "text": "Visualizing the null distribution\n\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Null distribution\")"
  },
  {
    "objectID": "W10.html#calculating-the-p-value-visually",
    "href": "W10.html#calculating-the-p-value-visually",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Calculating the p-value, visually",
    "text": "Calculating the p-value, visually\nWhat is the p-value: How often was the simulated sample proportion at least as extreme as the observed sample proportion?"
  },
  {
    "objectID": "W10.html#calculating-the-p-value-directly",
    "href": "W10.html#calculating-the-p-value-directly",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Calculating the p-value, directly",
    "text": "Calculating the p-value, directly\n\nnull_dist |>\n summarise(p_value = sum(stat <= 3/62)/n())\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1    0.13\n\n\nThis is the fraction of simulations where complications was equal or below 0.0483871."
  },
  {
    "objectID": "W10.html#significance-level",
    "href": "W10.html#significance-level",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Significance level",
    "text": "Significance level\n\nA significance level \\(\\alpha\\) is a threshold we make up to make our judgment about the plausibility of the null hypothesis being true given the observed data.\nWe often use \\(\\alpha = 0.05 = 5\\%\\) as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model.\nIf p-value < \\(\\alpha\\), reject \\(H_0\\) in favor of \\(H_A\\): The data provide convincing evidence for the alternative hypothesis.\nIf p-value > \\(\\alpha\\), fail to reject \\(H_0\\) in favor of \\(H_A\\): The data do not provide convincing evidence for the alternative hypothesis.\n\nWhat is the conclusion of the hypothesis test?\nSince the p-value is greater than the significance level, we fail to reject the null hypothesis. These data do not provide convincing evidence that this consultant incurs a lower complication rate than the 10% overall US complication rate."
  },
  {
    "objectID": "W10.html#conclusion",
    "href": "W10.html#conclusion",
    "title": "W#10: Cross validation",
    "section": "Conclusion",
    "text": "Conclusion\n.question[ What is the conclusion of the hypothesis test?]\n–\nSince the p-value is greater than the significance level, we fail to reject the null hypothesis. These data do not provide convincing evidence that this consultant incurs a lower complication rate than 10% (overall US complication rate)."
  },
  {
    "objectID": "W10.html#lets-get-real",
    "href": "W10.html#lets-get-real",
    "title": "W#10: Cross validation",
    "section": "Let’s get real",
    "text": "Let’s get real\n\n100 simulations is not sufficient\nWe usually simulate around 15,000 times to get an accurate distribution, but we’ll do 1,000 here for efficiency."
  },
  {
    "objectID": "W10.html#run-the-test",
    "href": "W10.html#run-the-test",
    "title": "W#10: Cross validation",
    "section": "Run the test",
    "text": "Run the test\n.small[\n\n\n\n]"
  },
  {
    "objectID": "W10.html#visualize-and-calculate",
    "href": "W10.html#visualize-and-calculate",
    "title": "W#10: Cross validation",
    "section": "Visualize and calculate",
    "text": "Visualize and calculate\n.small[\n\n\n\n]\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#simulations-is-not-sufficient",
    "href": "W10.html#simulations-is-not-sufficient",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "100 simulations is not sufficient",
    "text": "100 simulations is not sufficient\n\nWe simulate 15,000 times to get an accurate distribution.\n\n\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 15000, type = \"simulate\") |> \n  calculate(stat = \"prop\")\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  geom_vline(xintercept = 3/62, color = \"red\")"
  },
  {
    "objectID": "W10.html#xkcd-on-p-values",
    "href": "W10.html#xkcd-on-p-values",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "xkcd on p-values",
    "text": "xkcd on p-values\n\n\n \n\n\n\nSignificance levels are fairly arbitrary. Sometimes they are used (wrongly) as definitive judgments\nThey can even be used to do p-hacking: Searching for “significant” effects in observational data\nIn parts of science it has become a “gamed” performance metric.\nThe p-value syas nothing about effect size!"
  },
  {
    "objectID": "W10.html#our-more-robust-p-value",
    "href": "W10.html#our-more-robust-p-value",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Our more robust p-value",
    "text": "Our more robust p-value\nFor the null distribution with 15,000 simulations\n\nnull_dist |>\n  filter(stat <= 3/62) |>\n  summarise(p_value = n()/nrow(null_dist))\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.125\n\n\nOh OK, our fist p-value was much more borderline in favor of the alternative hypothesis."
  },
  {
    "objectID": "W10.html#model-purpose-predict-eu-attitudes",
    "href": "W10.html#model-purpose-predict-eu-attitudes",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model purpose: Predict EU attitudes",
    "text": "Model purpose: Predict EU attitudes\n\ness <- ess_raw |> filter(essround == 9) |> \n select(cntry, euftf, atchctr, atcherp, imueclt, lrscale) |> \n mutate(euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99), \n        atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),\n        atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),\n        imueclt = imueclt |> na_if(77) |> na_if(88) |> na_if(99),\n        lrscale = lrscale |> na_if(77) |> na_if(88) |> na_if(99))\n\nFor the ESS dataset\n\nwe filter for people from round 9 (2018)\nselect 5 attitude variables and cntry with 29 countries: AT, BE, BG, CH, CY, CZ, DE, DK, EE, ES, FI, FR, GB, HR, HU, IE, IS, IT, LT, LV, ME, NL, NO, PL, PT, RS, SE, SI, SK\nrecode NA’s properly for five variables:\neuftf: European Union: European unification go further (=10) or gone too far (=0)\natchctr: How emotionally attached to [country] (0 to 10)\natcherp: How emotionally attached to Europe (0 to 10)\nimueclt: Country’s cultural life undermined (=0) or enriched (=10) by immigrants\nlrscale: Placement on left (=0) right (=10) scale"
  },
  {
    "objectID": "W10.html#model-1-2-and-3",
    "href": "W10.html#model-1-2-and-3",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model 1, 2, and 3",
    "text": "Model 1, 2, and 3\n\n\n\nCreate an initial split with 80% training data\nCreate a linear model ess_mod\nCreate three recipes\n\ness_rec1 without using the country variable\ness_rec2 with main effects for all 29 countries\ness_rec3 with additional interaction effects for all 29 countries\n\nCreate the workflow\nFit 3 models by adding the 3 recipes\n\n\n\nset.seed(7)\ness_split <- initial_split(ess, prop = 0.80)\ness_train <- training(ess_split)\ness_test <- testing(ess_split)\n\ness_model <- linear_reg() |> set_engine(\"lm\")\ness_rec1 <- ess_train |> \n recipe(euftf ~ .) |> \n step_rm(cntry)\ness_rec2 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry)\ness_rec3 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry) |> \n step_interact(~starts_with(\"cntry\"):c(atchctr,atcherp,imueclt,lrscale))\n\ness_wflow <- workflow() |> \n add_model(ess_model)\n\ness_fit1 <- ess_wflow |> \n add_recipe(ess_rec1) |> \n fit(ess_train)\ness_fit2 <- ess_wflow |> \n add_recipe(ess_rec2) |> \n fit(ess_train)\ness_fit3 <- ess_wflow |> \n add_recipe(ess_rec3) |> \n fit(ess_train)"
  },
  {
    "objectID": "W10.html#model-fits",
    "href": "W10.html#model-fits",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model fits",
    "text": "Model fits\n\n\n\ntidy(ess_fit1) |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 5 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   2.92  \n2 atchctr      -0.0952\n3 atcherp       0.285 \n4 imueclt       0.287 \n5 lrscale      -0.0401\n\n\n\n\ntidy(ess_fit2)  |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 33 × 2\n   term        estimate\n   <chr>          <dbl>\n 1 (Intercept)  2.10   \n 2 atchctr     -0.101  \n 3 atcherp      0.294  \n 4 imueclt      0.299  \n 5 lrscale     -0.0196 \n 6 cntry_BE     0.747  \n 7 cntry_BG     1.20   \n 8 cntry_CH    -0.0476 \n 9 cntry_CY     1.39   \n10 cntry_CZ     0.184  \n11 cntry_DE     1.32   \n12 cntry_DK     1.01   \n13 cntry_EE     0.532  \n14 cntry_ES     1.14   \n15 cntry_FI     0.00996\n16 cntry_FR     0.743  \n17 cntry_GB    -0.0705 \n18 cntry_HR     0.729  \n19 cntry_HU     0.216  \n20 cntry_IE     0.294  \n21 cntry_IS     0.0371 \n22 cntry_IT     0.593  \n23 cntry_LT     2.15   \n24 cntry_LV     0.426  \n25 cntry_ME     2.41   \n26 cntry_NL     0.600  \n27 cntry_NO    -0.170  \n28 cntry_PL     1.19   \n29 cntry_PT     1.42   \n30 cntry_RS     1.21   \n31 cntry_SE    -0.0761 \n32 cntry_SI     1.53   \n33 cntry_SK     0.178  \n\n\n\n\ntidy(ess_fit3)  |> select(term, estimate) |> print(n = 145)\n\n# A tibble: 145 × 2\n    term               estimate\n    <chr>                 <dbl>\n  1 (Intercept)         2.65   \n  2 atchctr            -0.135  \n  3 atcherp             0.259  \n  4 imueclt             0.412  \n  5 lrscale            -0.141  \n  6 cntry_BE            0.0202 \n  7 cntry_BG           -0.264  \n  8 cntry_CH            1.70   \n  9 cntry_CY            0.242  \n 10 cntry_CZ           -0.914  \n 11 cntry_DE            0.248  \n 12 cntry_DK            0.637  \n 13 cntry_EE           -0.709  \n 14 cntry_ES            1.15   \n 15 cntry_FI            0.143  \n 16 cntry_FR            0.136  \n 17 cntry_GB            0.486  \n 18 cntry_HR            0.177  \n 19 cntry_HU           -0.190  \n 20 cntry_IE            0.186  \n 21 cntry_IS            0.881  \n 22 cntry_IT           -0.582  \n 23 cntry_LT            0.925  \n 24 cntry_LV           -1.60   \n 25 cntry_ME            0.542  \n 26 cntry_NL           -0.804  \n 27 cntry_NO            0.763  \n 28 cntry_PL            2.24   \n 29 cntry_PT            2.18   \n 30 cntry_RS            0.220  \n 31 cntry_SE           -0.507  \n 32 cntry_SI            1.19   \n 33 cntry_SK           -0.604  \n 34 cntry_BE_x_atchctr -0.0249 \n 35 cntry_BE_x_atcherp  0.128  \n 36 cntry_BE_x_imueclt -0.130  \n 37 cntry_BE_x_lrscale  0.148  \n 38 cntry_BG_x_atchctr  0.220  \n 39 cntry_BG_x_atcherp -0.0614 \n 40 cntry_BG_x_imueclt -0.159  \n 41 cntry_BG_x_lrscale  0.151  \n 42 cntry_CH_x_atchctr -0.0286 \n 43 cntry_CH_x_atcherp  0.0194 \n 44 cntry_CH_x_imueclt -0.197  \n 45 cntry_CH_x_lrscale -0.106  \n 46 cntry_CY_x_atchctr  0.0956 \n 47 cntry_CY_x_atcherp  0.0496 \n 48 cntry_CY_x_imueclt -0.336  \n 49 cntry_CY_x_lrscale  0.300  \n 50 cntry_CZ_x_atchctr -0.0375 \n 51 cntry_CZ_x_atcherp -0.0267 \n 52 cntry_CZ_x_imueclt  0.0735 \n 53 cntry_CZ_x_lrscale  0.273  \n 54 cntry_DE_x_atchctr  0.0299 \n 55 cntry_DE_x_atcherp  0.159  \n 56 cntry_DE_x_imueclt -0.127  \n 57 cntry_DE_x_lrscale  0.0836 \n 58 cntry_DK_x_atchctr -0.0884 \n 59 cntry_DK_x_atcherp  0.0752 \n 60 cntry_DK_x_imueclt  0.0109 \n 61 cntry_DK_x_lrscale  0.0970 \n 62 cntry_EE_x_atchctr  0.0895 \n 63 cntry_EE_x_atcherp  0.0464 \n 64 cntry_EE_x_imueclt -0.137  \n 65 cntry_EE_x_lrscale  0.180  \n 66 cntry_ES_x_atchctr  0.0735 \n 67 cntry_ES_x_atcherp -0.0706 \n 68 cntry_ES_x_imueclt -0.0704 \n 69 cntry_ES_x_lrscale  0.0210 \n 70 cntry_FI_x_atchctr -0.106  \n 71 cntry_FI_x_atcherp  0.0602 \n 72 cntry_FI_x_imueclt -0.104  \n 73 cntry_FI_x_lrscale  0.176  \n 74 cntry_FR_x_atchctr -0.0254 \n 75 cntry_FR_x_atcherp  0.185  \n 76 cntry_FR_x_imueclt -0.195  \n 77 cntry_FR_x_lrscale  0.139  \n 78 cntry_GB_x_atchctr -0.0968 \n 79 cntry_GB_x_atcherp  0.161  \n 80 cntry_GB_x_imueclt -0.196  \n 81 cntry_GB_x_lrscale  0.0630 \n 82 cntry_HR_x_atchctr  0.145  \n 83 cntry_HR_x_atcherp -0.0138 \n 84 cntry_HR_x_imueclt -0.213  \n 85 cntry_HR_x_lrscale  0.120  \n 86 cntry_HU_x_atchctr  0.0531 \n 87 cntry_HU_x_atcherp -0.0656 \n 88 cntry_HU_x_imueclt -0.0588 \n 89 cntry_HU_x_lrscale  0.160  \n 90 cntry_IE_x_atchctr  0.0630 \n 91 cntry_IE_x_atcherp -0.0230 \n 92 cntry_IE_x_imueclt -0.138  \n 93 cntry_IE_x_lrscale  0.0874 \n 94 cntry_IS_x_atchctr -0.186  \n 95 cntry_IS_x_atcherp  0.111  \n 96 cntry_IS_x_imueclt -0.0756 \n 97 cntry_IS_x_lrscale  0.0596 \n 98 cntry_IT_x_atchctr  0.0381 \n 99 cntry_IT_x_atcherp -0.0405 \n100 cntry_IT_x_imueclt  0.114  \n101 cntry_IT_x_lrscale  0.111  \n102 cntry_LT_x_atchctr  0.145  \n103 cntry_LT_x_atcherp -0.113  \n104 cntry_LT_x_imueclt -0.0917 \n105 cntry_LT_x_lrscale  0.219  \n106 cntry_LV_x_atchctr  0.130  \n107 cntry_LV_x_atcherp -0.0431 \n108 cntry_LV_x_imueclt -0.161  \n109 cntry_LV_x_lrscale  0.365  \n110 cntry_ME_x_atchctr  0.198  \n111 cntry_ME_x_atcherp  0.136  \n112 cntry_ME_x_imueclt -0.179  \n113 cntry_ME_x_lrscale  0.0701 \n114 cntry_NL_x_atchctr  0.0416 \n115 cntry_NL_x_atcherp  0.138  \n116 cntry_NL_x_imueclt -0.130  \n117 cntry_NL_x_lrscale  0.182  \n118 cntry_NO_x_atchctr -0.0643 \n119 cntry_NO_x_atcherp -0.0283 \n120 cntry_NO_x_imueclt -0.202  \n121 cntry_NO_x_lrscale  0.187  \n122 cntry_PL_x_atchctr -0.00472\n123 cntry_PL_x_atcherp  0.0737 \n124 cntry_PL_x_imueclt -0.231  \n125 cntry_PL_x_lrscale -0.0290 \n126 cntry_PT_x_atchctr  0.0377 \n127 cntry_PT_x_atcherp -0.114  \n128 cntry_PT_x_imueclt -0.178  \n129 cntry_PT_x_lrscale  0.126  \n130 cntry_RS_x_atchctr  0.139  \n131 cntry_RS_x_atcherp  0.0996 \n132 cntry_RS_x_imueclt -0.285  \n133 cntry_RS_x_lrscale  0.167  \n134 cntry_SE_x_atchctr  0.0493 \n135 cntry_SE_x_atcherp -0.00505\n136 cntry_SE_x_imueclt -0.138  \n137 cntry_SE_x_lrscale  0.160  \n138 cntry_SI_x_atchctr  0.199  \n139 cntry_SI_x_atcherp -0.165  \n140 cntry_SI_x_imueclt -0.256  \n141 cntry_SI_x_lrscale  0.204  \n142 cntry_SK_x_atchctr  0.0955 \n143 cntry_SK_x_atcherp -0.0549 \n144 cntry_SK_x_imueclt -0.0424 \n145 cntry_SK_x_lrscale  0.137  \n\n\n\n\nNote: We omit std.error, p-values and so on in the display here because they are usually small in this large dataset, and we will not look at them now."
  },
  {
    "objectID": "W10.html#some-probability-topics-for-data-science",
    "href": "W10.html#some-probability-topics-for-data-science",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Some Probability Topics for Data Science",
    "text": "Some Probability Topics for Data Science\n\nThe concept of probability form the mathematical perspective.\nWhat is the difference between probability theory and statistics?\nMake precise what are probabilistic events, a probability functions and random variables.\nHow do random variables relate to data?\nProbabilistic simulations. For example bootstrapping.\nConditional probabilities and their relation to the confusion matrix.\nContinuous random variables and some theoretical distributions to link the theory with the practical treatment.\nThe central limit theorem.\nWhat is the difference between probability theory and statistics?\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#probability-topics-for-data-science",
    "href": "W10.html#probability-topics-for-data-science",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\nSome concepts and topics\n\n\nThe concept of probability form the mathematical perspective.\nWhat is the difference between probability theory and statistics?\nWhat are probabilistic events, probability functions and random variables.\nHow do random variables relate to data?\nProbabilistic simulations. For example bootstrapping.\nConditional probabilities and their relation to the confusion matrix.\nContinuous random variables and some theoretical distributions.\nThe central limit theorem.\nWhat is the difference between probability theory and statistics?"
  },
  {
    "objectID": "W10.html#what-is-probability",
    "href": "W10.html#what-is-probability",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "What is probability",
    "text": "What is probability\n\n\nThe systematic and rigorous treatment of uncertainty.\nWe have a certain intuition of probability visible in sentences like:\n\n“That’s not very probable.”\n“That is likely.”\n“I don’t have a prior for that.”\n\nWe can call it a model for uncertainty: A simplified but formalized way to think about uncertain events.\nThe model of probability is one of the most successful mathematical models. It is used in many domains."
  },
  {
    "objectID": "W10.html#two-different-flavors",
    "href": "W10.html#two-different-flavors",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Two different flavors",
    "text": "Two different flavors\n\nModel for uncertainty as subjective or objective probability of an uncertain event.\nThey are also called Bayesian vs. Frequentist interpretation of probability. * They differ in the way of reasoning, the interpretation what is random, and in terminology."
  },
  {
    "objectID": "W10.html#objective-interpretation-of-probability",
    "href": "W10.html#objective-interpretation-of-probability",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Objective interpretation of probability",
    "text": "Objective interpretation of probability\nThe objective interpretation of probability is relative frequency in the limit of indefinite sampling. It is the long run behavior of non-deterministic outcomes.\n\nIf we flip the same coin several times the relative frequency of the number of HEADS converges to the probability of HEADS.\nIf we draw people at random the relative frequency of people with heights between 1.70m and 1.75m converges to the probability that a person is in this range.\nIn this frequentist philosophy the parameters of the population we sample from is fixed and the data is a random selection."
  },
  {
    "objectID": "W10.html#subjective-interpretation-of-probability",
    "href": "W10.html#subjective-interpretation-of-probability",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Subjective interpretation of probability",
    "text": "Subjective interpretation of probability\nThe subjective interpretation of probability is the belief a person has about the likelihood that an event occurs. This can be formalized by the condition under which a person would make a bet.\n\nIf say we flip a coin, we can offer a person a bet for HEADS, e.g. you gain 1€ when the outcome is heads and you lose 2€ when it is TAILS. The person would be indifferent between accepting and rejecting the bet if their subjective belief is that HEADS will come two times more likely than TAILS (odds 2:1 or probability 2/3). The subjective probability is the probability where the person is indifferent.\nWe can make up similar bets for the heights of randomly drawn drawn people.\nIn Bayesian philosophy the data we know is fixed but the parameters of the population are random and associated with probabilities.\n\nThe objective and subjective views are not mutually exclusive and it is not important to take a side.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#objective-interpretation",
    "href": "W10.html#objective-interpretation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Objective interpretation",
    "text": "Objective interpretation\nProbability is relative frequency in the limit of indefinite sampling. It is the long run behavior of non-deterministic outcomes.\n\nIf we flip the same coin several times the relative frequency of the number of HEADS converges to the probability of HEADS.\nIf we draw people at random the relative frequency of people with heights between 1.70m and 1.75m converges to the probability that a person is in this range.\nIn this frequentist philosophy the parameters of the population we sample from is fixed and the data is a random selection."
  },
  {
    "objectID": "W10.html#subjective-interpretation",
    "href": "W10.html#subjective-interpretation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Subjective interpretation",
    "text": "Subjective interpretation\nProbability is a belief a person has about the likelihood that an event occurs. This can be formalized by the condition under which a person would make a bet.\n\nIf say we flip a coin, we can offer a person a bet for HEADS, e.g. you gain 1€ when the outcome is heads and you lose 2€ when it is TAILS. The person would be indifferent between accepting and rejecting the bet if their subjective belief is that HEADS will come two times more likely than TAILS (odds 2:1 or probability 2/3). The subjective probability is the probability where the person is indifferent.\nWe can make up similar bets for the heights of randomly drawn people.\nIn Bayesian philosophy the data we know is fixed but the parameters of the population are random and associated with probabilities.\n\nThe objective and subjective views are not mutually exclusive and it is not important to take a side.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "index.html#core-resources",
    "href": "index.html#core-resources",
    "title": "Data Science Concepts / Tools",
    "section": "2.1 Core resources",
    "text": "2.1 Core resources\nA large part of the course will build on or be inspired by material in Data Science in the Box https://datasciencebox.org/ by Mine Çetinkaya-Rundel and the data science education community around. You can also use the website for accompanying self-study on selected topics.\nA core resource for self-learning data science with R is R for Data Science https://r4ds.had.co.nz/ by Hadley Wickham and the R data science community around. This is a valuable resource for self-studying particular topics using tidyverse tools. Just out: Tidy Modeling with R https://www.tmwr.org/ by Max Kuhn and Julia Silge on using tidymodels.\nA core resource for data science basics with python is the Python Data Science Handbook https://jakevdp.github.io/PythonDataScienceHandbook/ by Jake VanderPlas. Also there is a good video playlist https://www.youtube.com/playlist?list=PLWKjhJtqVAbkmRvnFmOd4KhDdlK1oIq23 and a 4-hour “full-course” video https://youtu.be/rfscVS0vtbw."
  },
  {
    "objectID": "index.html#mathematics",
    "href": "index.html#mathematics",
    "title": "Data Science Concepts / Tools",
    "section": "2.2 Mathematics",
    "text": "2.2 Mathematics\nSome helpful YouTube videos for those recaping maths:\nHow to read math https://www.youtube.com/watch?v=Kp2bYWRQylk\nData Visualization\nggplot2: Elegant Graphics for Data Analysis https://ggplot2-book.org is a resource on understanding the logic of ggplot better.\nWebsites on how to decide for what visualization to choose:\nhttps://www.data-to-viz.com/\nhttps://datavizcatalogue.com/search.html\nThe number of freely available learning resources is increasing rapidly.\nEveryone has a different learning style and a different background. Thus, different material may be most helpful.\nThis list shall be extended. Good self-study resources for data science with python, mathematics, statistics, data visualization, etc. shall be added here. Contact me if you find something useful to share. Ideally, with a short description when it may be most helpful."
  },
  {
    "objectID": "index.html#week-8-oct-20-homework-topics-typical-data-issues-more-linear-models-and-interpretation",
    "href": "index.html#week-8-oct-20-homework-topics-typical-data-issues-more-linear-models-and-interpretation",
    "title": "Data Science Concepts / Tools",
    "section": "Week 8, Oct 20: Homework topics: Typical Data Issues, More Linear Models and Interpretation",
    "text": "Week 8, Oct 20: Homework topics: Typical Data Issues, More Linear Models and Interpretation\nTopics:\n\nErrors, Differences, Missings in Data\nMore Linear Models\n\nMore predictors\nMain effects and interaction effects\n\nTo be finished\n\nSlides Week 8\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 should come. Due Nov 6."
  },
  {
    "objectID": "index.html#week-10-nov-3-performance-metrics-cross-validation-hypothesis-testing-math-probability",
    "href": "index.html#week-10-nov-3-performance-metrics-cross-validation-hypothesis-testing-math-probability",
    "title": "Data Science Concepts / Tools",
    "section": "Week 10, Nov 3: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "text": "Week 10, Nov 3: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability\nSlides Week 10\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 due in 3 days\nHomework 05 should come. Due Nov 20."
  },
  {
    "objectID": "W10.html#comparison-of-metrics-1",
    "href": "W10.html#comparison-of-metrics-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\n\n\nFlipper length, body mass\n\npeng_test_pred1 |> \n accuracy(truth = sex, estimate = .pred_class_0.5)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.774\n\npeng_test_pred1 |> \n accuracy(truth = sex, estimate = .pred_class_0.55)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.726\n\npeng_test_pred1 |> \n accuracy(truth = sex, estimate = .pred_class_0.45)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.821\n\n\n\nBill length and depth\n\npeng_test_pred2 |> \n accuracy(truth = sex, estimate = .pred_class_0.5)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.738\n\npeng_test_pred2 |> \n accuracy(truth = sex, estimate = .pred_class_0.55)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.774\n\npeng_test_pred2 |> \n accuracy(truth = sex, estimate = .pred_class_0.45)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary          0.75"
  },
  {
    "objectID": "W10.html#comparison-of-metrics-2",
    "href": "W10.html#comparison-of-metrics-2",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\nFlipper length, body mass\n\npeng_test_pred1 |> select(1:6)\n\n# A tibble: 84 × 6\n   .pred_female .pred_male .pred_class_0.5 .pred_class_0.45 .pred_class_…¹ sex  \n          <dbl>      <dbl> <fct>           <fct>            <fct>          <fct>\n 1       0.756       0.244 female          female           female         fema…\n 2       0.373       0.627 male            male             male           fema…\n 3       0.286       0.714 male            male             male           male \n 4       0.352       0.648 male            male             male           fema…\n 5       0.365       0.635 male            male             male           male \n 6       0.451       0.549 male            female           male           fema…\n 7       0.0491      0.951 male            male             male           male \n 8       0.700       0.300 female          female           female         fema…\n 9       0.532       0.468 female          female           male           male \n10       0.824       0.176 female          female           female         fema…\n# … with 74 more rows, and abbreviated variable name ¹​.pred_class_0.55\n\n\nBill length and depth\n\npeng_test_pred2 |> select(1:6)\n\n# A tibble: 84 × 6\n   .pred_female .pred_male .pred_class_0.5 .pred_class_0.45 .pred_class_…¹ sex  \n          <dbl>      <dbl> <fct>           <fct>            <fct>          <fct>\n 1        0.487      0.513 male            female           male           fema…\n 2        0.642      0.358 female          female           female         fema…\n 3        0.278      0.722 male            male             male           male \n 4        0.614      0.386 female          female           female         fema…\n 5        0.543      0.457 female          female           male           male \n 6        0.556      0.444 female          female           female         fema…\n 7        0.339      0.661 male            male             male           male \n 8        0.736      0.264 female          female           female         fema…\n 9        0.410      0.590 male            male             male           male \n10        0.855      0.145 female          female           female         fema…\n# … with 74 more rows, and abbreviated variable name ¹​.pred_class_0.55"
  },
  {
    "objectID": "W10.html#overview-of-predictions",
    "href": "W10.html#overview-of-predictions",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Overview of predictions",
    "text": "Overview of predictions\nBill length and depth\n\npeng_test_pred2 |> select(1:6)\n\n# A tibble: 84 × 6\n   .pred_female .pred_male .pred_class_0.5 .pred_class_0.45 .pred_class_…¹ sex  \n          <dbl>      <dbl> <fct>           <fct>            <fct>          <fct>\n 1        0.487      0.513 male            female           male           fema…\n 2        0.642      0.358 female          female           female         fema…\n 3        0.278      0.722 male            male             male           male \n 4        0.614      0.386 female          female           female         fema…\n 5        0.543      0.457 female          female           male           male \n 6        0.556      0.444 female          female           female         fema…\n 7        0.339      0.661 male            male             male           male \n 8        0.736      0.264 female          female           female         fema…\n 9        0.410      0.590 male            male             male           male \n10        0.855      0.145 female          female           female         fema…\n# … with 74 more rows, and abbreviated variable name ¹​.pred_class_0.55"
  }
]