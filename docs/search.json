[
  {
    "objectID": "late.html",
    "href": "late.html",
    "title": "Info for late coming students",
    "section": "",
    "text": "We know there are reasons for coming late which may not lie in your hands. We try to help you to get into the course in this situation. The following checklist should guide you.\n\nCheck list for late coming students\n\nRead this Syllabus\nMost urgent\n\n\nLook at slides 30 to 32 of the Week 1: https://docs.google.com/presentation/d/1GyEClkRDo5aOKuHeOZxgvz2uqo_x5t-c1CQ-ksGXTq0/edit#slide=id.g14992662fb0_0_115 and start to build your data science toolkit on your computer.\nPart of it is that you should submit your GitHub-username in a Google Form. To speed up thing, also send your GitHub-username to Jan Lorenz via Teams!\n\n\nWork through the materials of the Sessions you have missed.\n\n\nYou find the materials in the Section “Course Material and Schedule” at the end of the syllabus.\nThere are links to the slides.\nOn MS Teams you find recordings in the Team F22_MDSSB-DSOC-02_Data Science Concepts in the General Channel -> Files -> Recordings\n\n\nDo the Homework you have missed even if you are beyond the deadline. You find the Homework instructions on this website. Homework 01 introduces you to the workflow needed for further Homework.\nVisit the next lectures in Data Science Concepts and the sessions in the two Data Science Tools course. When you cannot be there in person visit via Teams. Meetings will be started in the General Channel when the course starts.\nWhen you get stuck or lost, ask your fellow students or contact us!"
  },
  {
    "objectID": "W6.html#functions-mathematically",
    "href": "W6.html#functions-mathematically",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Functions mathematically",
    "text": "Functions mathematically\nConsider two sets: The domain \\(X\\) and the codomain \\(Y\\).\nA function \\(f\\) assigns each element of \\(X\\) to exactly one element of \\(Y\\).\n\n\nWe write \\(f : X \\to Y\\)\n“\\(f\\) maps from \\(X\\) to \\(Y\\)”\nand \\(x \\mapsto f(x)\\)\n“\\(x\\) maps to \\(f(x)\\)”\nThe yellow set is called the image of \\(f\\).\n\n\n\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W6.html#conventions-in-mathematical-text",
    "href": "W6.html#conventions-in-mathematical-text",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Conventions in mathematical text",
    "text": "Conventions in mathematical text\n\nSets are denoted with capital letters.\nTheir elements with (corresponding) small letters.\nFunctions are often called \\(f\\), \\(g\\), or \\(h\\).\nOther terminology can be used!\n\nResponsibility of the mathematical writer: Define objects.\nResponsibility of the mathematical reader: Keep track of what objects are."
  },
  {
    "objectID": "W6.html#is-this-a-function",
    "href": "W6.html#is-this-a-function",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Is this a function?",
    "text": "Is this a function?\nInput from \\(X = \\{\\text{A picture where a face can be recognized}\\}\\).\nFunction: Upload input at https://funny.pho.to/lion/ and download output.\n \\(\\ \\mapsto\\ \\) \nOutput from \\(Y = \\{\\text{Set of pictures with a specific format.}\\}\\)\n\nYes, it is a function. Important: Output is the same for the same input!"
  },
  {
    "objectID": "W6.html#is-this-a-function-1",
    "href": "W6.html#is-this-a-function-1",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Is this a function?",
    "text": "Is this a function?\nInput a text snippet. Function: Enter text at https://www.craiyon.com. Output a picture.\n\n\n\n\nOther examples:\n\n“Nuclear explosion broccoli”\n“The Eye of Sauron reading a newspaper”\n“The legendary attack of Hamster Godzilla wearing a tiny Sombrero”\n\n  \n\n\n\nNo, it is not a function. It has nine outcomes and these change when run again."
  },
  {
    "objectID": "W6.html#graphs-of-functions",
    "href": "W6.html#graphs-of-functions",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Graphs of functions",
    "text": "Graphs of functions\n\nA function is characterized by the set all possible pairs \\((x,f(x))\\).\nThis is called its graph. Note, this can be infinitely many.\nWhen domain and codomain are real numbers then the graph can be shown in a Cartesian coordinate system. Example \\(f(x) = x^3 - x^2\\)"
  },
  {
    "objectID": "W6.html#some-functions-f-mathbbr-to-mathbbr",
    "href": "W6.html#some-functions-f-mathbbr-to-mathbbr",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)",
    "text": "Some functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)\n\n\n\\(f(x) = x\\) identity function\n\\(f(x) = x^2\\) square function\n\\(f(x) = \\sqrt{x}\\) square root function\n\\(f(x) = e^x\\) exponential function\n\\(f(x) = \\log(x)\\) natural logarithm\n\nSquare function and square root function are the inverse of each other. Exponential and natural logarithm, too.\n\n\\(\\sqrt[2]{x}^2 = \\sqrt[2]{x^2} = x\\), \\(\\log(e^x) = e^{\\log(x)} = x\\)\n\nTheir graphs reflect each other with identity function graph as mirror axis.\n\n\n\n\n\n\n\n\n\n\n\n\\(e\\) is Euler’s number \\(2.71828\\dots\\). The natural logarithm is also often called \\(\\ln\\). The square root function is \\(\\mathbb{R}_{\\geq 0} \\to \\mathbb{R}\\), the logarithm \\(\\mathbb{R}_{>0} \\to \\mathbb{R}\\)."
  },
  {
    "objectID": "W6.html#shifts-and-scales",
    "href": "W6.html#shifts-and-scales",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Shifts and scales",
    "text": "Shifts and scales\nHow can we shift, stretch, or shrink a graph vertically and horizontally?\n\n\n\\(y\\)-shift\\(x\\)-shift\\(y\\)-scale\\(x\\)-scale\n\n\n\n\nAdd a constant to the function.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = x^3 - x^2 + a\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\n\n\n\n\n\n\n\n\n\n\n\n\nSubtract a constant from all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x - a)^3 - (x - a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nAttention:\nShifting as you think needs subtracting \\(a\\)!\nYou can think of the coordinate system being shifted in direction \\(a\\) while the graph stays.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiply a constant to all \\(x\\) within the function definition.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = a(x^3 - x^2)\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(x\\)-axis.\n\n\n\n\n\n\n\n\n\n\n\n\nDivide all \\(x\\) within the function definition by a constant.\n\\(f(x) = x^3 - x^2 \\leadsto\\)\n\\(\\quad f(x) = (x/a)^3 - (x/a)^2\\)\nFor \\(a =\\) -2, -0.5, 0.5, 2\nNegative numbers flip the graph around the \\(y\\)-axis.\nAttention: Stretching needs a division by \\(a\\)!\nYou can think of the coordinate system being stretched multiplicatively by \\(a\\) while the graph stays."
  },
  {
    "objectID": "W6.html#polynomials-and-exponentials",
    "href": "W6.html#polynomials-and-exponentials",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Polynomials and exponentials",
    "text": "Polynomials and exponentials\nA polynomial is a function which is composed of (many) addends of the form \\(ax^n\\) for different values of \\(a\\) and \\(n\\).\nIn an exponential the \\(x\\) appears in the exponent.\n\\(f(x) = x^3\\) vs. \\(f(x) = e^x\\)\n\nFor \\(x\\to\\infty\\), any exponential will finally “overtake” any polynomial."
  },
  {
    "objectID": "W6.html#rules-for-exponentiation",
    "href": "W6.html#rules-for-exponentiation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Rules for exponentiation",
    "text": "Rules for exponentiation\n\n\n\\(x^0\\)\n\\(0^x\\)\n\\(0^0\\)\n\\((x\\cdot y)^a\\)\n\\(x^{-a}\\), \\(x^{-1}\\)\n\\(x^\\frac{a}{b}\\), \\(x^\\frac{1}{2}\\)\n\\((x^a)^b\\)\n\n\n\\(x^0 = 1\\)\n\n\n\\(0^x = 0\\) for \\(x\\neq 0\\)\n\n\n\\(0^0 = 1\\) (discontinuity in \\(0^x\\))\n\n\n\\((x\\cdot y)^a = x^a\\cdot x^b\\)\n\n\n\\(x^{-a} = \\frac{1}{x^a}\\), \\(x^{-1} = \\frac{1}{x}\\)\n\n\n\\(x^\\frac{a}{b} = \\sqrt[b]{x^a} = (\\sqrt[b]{x})^a,\\ x^\\frac{1}{2} = \\sqrt{x}\\)\n\n\n\\((x^a)^b = x^{a\\cdot b} = (x^b)^a \\neq x^{a^b} = x^{(a^b)}\\)\nExample: \\((4^3)^2 = 64^2 = 4096 \\qquad 4^{3^2} = 4^9 = 262144\\)"
  },
  {
    "objectID": "W6.html#more-rules-for-exponentiation",
    "href": "W6.html#more-rules-for-exponentiation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "More rules for exponentiation",
    "text": "More rules for exponentiation\n\n\n\\(x^a\\cdot x^b\\)\n\n\n\\(x^a\\cdot x^b = x^{a+b}\\) Multiplication of powers (with same base \\(x\\)) becomes addition of exponents.\n\n\n\n\n\n\n\\((x+y)^a\\)\n\n\nNo “simple” form! For \\(a\\) integer use binomial expansion. \\((x+y)^2 = x^2 + 2xy + y^2\\)\n\\((x+y)^3 = x^3 + 3x^2y + 3xy^2 + y^3\\)\n\\((x+y)^n = \\sum_{k=0}^n {n \\choose k} x^{n-k}y^k\\)\n\n\n\nPascal’s triangle\n\n\n\n\n\n\n\n\n\n\n\n\nFrom wikipedia\n\n\n\nWe meet it again in Probability. (Binomial distribution, Central Limit Theorem)"
  },
  {
    "objectID": "W6.html#logarithms",
    "href": "W6.html#logarithms",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Logarithms",
    "text": "Logarithms\nDefinition: A logarithm of \\(a\\) for some base \\(b\\) is the value of the exponent which brings \\(b\\) to \\(a\\): \\(\\log_b(a) = x\\) means that \\(b^x =a\\)\nMost common:\n\n\\(\\log_{10}\\) for logarithmic axes in plots\n\\(\\log_{e}\\) natural logarithm (also \\(\\log\\) or \\(\\ln\\))\n\n\n\n\n\\(\\log_{10}(100) =\\)\n\n\n\\(2\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(1) =\\)\n\n\n\\(0\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(6590) =\\)\n\n\n\\(3.818885\\)\n\n\n\n\n\n\n\n\\(\\log_{10}(0.02) =\\)\n\n\n\\(-1.69897\\)"
  },
  {
    "objectID": "W6.html#rules-for-logarithms",
    "href": "W6.html#rules-for-logarithms",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Rules for logarithms",
    "text": "Rules for logarithms\nUsually only one base is used in the same context, because changing base is easy:\n\\(\\log_c(x) = \\frac{log_b(x)}{\\log_b(c)} = \\frac{\\log(x)}{\\log(c)}\\)\n\n\n\\(\\log(x\\cdot y)\\)\n\n\n\\(= \\log(x) + \\log(y)\\) Multiplication \\(\\to\\) addition.\n\n\n\n\n\n\n\\(\\log(x^y)\\)\n\n\n\\(= y\\cdot\\log(x)\\)\n\n\n\n\n\n\\(\\log(x+y)\\)\n\n\ncomplicated!\n\n\n\n\n\nAlso changing bases for powers is easy: \\(x^y = (e^{\\log(x)})^y = e^{y\\cdot\\log(x)}\\)"
  },
  {
    "objectID": "W6.html#input-to-output",
    "href": "W6.html#input-to-output",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Input \\(\\to\\) output",
    "text": "Input \\(\\to\\) output\n\n\nMetaphorically, a function is a machine or a blackbox that for each input yields and output.\nThe inputs of a function are also called arguments.\n\n\n\nPicture from wikipedia."
  },
  {
    "objectID": "W6.html#function-as-objects-in-r",
    "href": "W6.html#function-as-objects-in-r",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Function as objects in R",
    "text": "Function as objects in R\nfunction is a class of an object in R\n\nclass(c)\n\n[1] \"function\"\n\nclass(ggplot2::ggplot)\n\n[1] \"function\"\n\n\nCalling the function without brackets writes its code or some information.\n\nsd # This function is written in R\n\nfunction (x, na.rm = FALSE) \nsqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), \n    na.rm = na.rm))\n<bytecode: 0x55bedaea8280>\n<environment: namespace:stats>\n\nc  \n\nfunction (...)  .Primitive(\"c\")\n\nggplot2::ggplot \n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \n{\n    UseMethod(\"ggplot\")\n}\n<bytecode: 0x55bedae9e280>\n<environment: namespace:ggplot2>"
  },
  {
    "objectID": "W6.html#functions-in-r",
    "href": "W6.html#functions-in-r",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Functions in R",
    "text": "Functions in R\nDefine your own functions like this\n\nadd_one <- function(x) {\n  x + 1 \n}\n# Test it\nadd_one(10)\n\n[1] 11\n\n\nThe skeleton for a function definition is\nfunction_name <- function(input){\n  # do something with the input(s)\n  # return something as output\n}\n\nfunction_name should be a short but evocative verb.\nThe input can be empty or one or more name or name=expression terms as arguments.\nThe last evaluated expression is returned as output.\nWhen the body or the function is only one line {} can be omitted. For example\nadd_one <- function(x) x + 1"
  },
  {
    "objectID": "W6.html#flexibility-of-inputs-and-outputs",
    "href": "W6.html#flexibility-of-inputs-and-outputs",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Flexibility of inputs and outputs",
    "text": "Flexibility of inputs and outputs\n\nArguments can be specified by name=expression or just expression (then they are taken as the next argument)\nDefault values for arguments can be provided. Useful when an argument is a parameter.\n\n\n\nmymult <- function(x = 2, y = 3) x * (y - 1)\nmymult(3,4)\n\n\n[1] 9\n\n\n\n\n\nmymult()\n\n\n[1] 4\n\n\n\n\n\nmymult(y = 3,4)\n\n\n[1] 8\n\n\n\n\n\nmymult(5)\n\n\n[1] 10\n\n\n\n\n\nmymult(y = 2)\n\n\n[1] 2\n\n\n\n\nFor complex output use a list\n\n\nmymult <- function(x = 2, y = 3) \n  list(out1 = x * (y - 1), out2 = x * (y - 2))\nmymult()\n\n\n$out1\n[1] 4\n\n$out2\n[1] 2"
  },
  {
    "objectID": "W6.html#vectorized-functions",
    "href": "W6.html#vectorized-functions",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Vectorized functions",
    "text": "Vectorized functions\nMathematical functions in programming are often “vectorized”:\n\nOperations on a single value are applied to each component of the vector.\nOperations on two values are applied “component-wise” (for vectors of the same length)\n\n\nlog10(c(1,10,100,1000,10000))\n\n[1] 0 1 2 3 4\n\nc(1,1,2) + c(3,1,0)\n\n[1] 4 2 2\n\n(0:5)^2\n\n[1]  0  1  4  9 16 25"
  },
  {
    "objectID": "W6.html#vector-creation-functions",
    "href": "W6.html#vector-creation-functions",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Vector creation functions",
    "text": "Vector creation functions\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(from=-0.5, to=1.5, by=0.1)\n\n [1] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9\n[16]  1.0  1.1  1.2  1.3  1.4  1.5\n\nseq(from=0, to=1, length.out=10)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000\n\nrep(1:3, times=3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\nrep(1:3, each=3)\n\n[1] 1 1 1 2 2 2 3 3 3"
  },
  {
    "objectID": "W6.html#plotting-and-transformation",
    "href": "W6.html#plotting-and-transformation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Plotting and transformation",
    "text": "Plotting and transformation\nVector creation and vectorized functions are key for plotting and transformation.\n\nfunc <- function(x) x^3 - x^2    # Create a vectorized function\ndata <- tibble(x = seq(-0.5,1.5,by =0.01)) |>    # Vector creation\n    mutate(y = func(x))        # Vectorized transformation using the function\ndata |> ggplot(aes(x,y)) + geom_line()"
  },
  {
    "objectID": "W6.html#convenient-function-ggploting",
    "href": "W6.html#convenient-function-ggploting",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Convenient function ggploting",
    "text": "Convenient function ggploting\n\nggplot() +\n    geom_function(fun = log) +\n    geom_function(fun = function(x) 3*x - 4, color = \"red\")"
  },
  {
    "objectID": "W6.html#purpose-of-modeling",
    "href": "W6.html#purpose-of-modeling",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Purpose of modeling",
    "text": "Purpose of modeling\nWe use models to\n\nexplain relations between variables\nmake predictions\n\nFirst, we focus on linear models."
  },
  {
    "objectID": "W6.html#palmer-penguins",
    "href": "W6.html#palmer-penguins",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Palmer Penguins",
    "text": "Palmer Penguins\nWe use the dataset Palmer Penguins\nChinstrap, Gentoo, and Adélie Penguins\n  \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g"
  },
  {
    "objectID": "W6.html#body-mass-in-grams",
    "href": "W6.html#body-mass-in-grams",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Body mass in grams",
    "text": "Body mass in grams\n\npenguins |>\n  ggplot(aes(body_mass_g)) +\n  geom_histogram()"
  },
  {
    "objectID": "W6.html#flipper-length-in-millimeters",
    "href": "W6.html#flipper-length-in-millimeters",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Flipper length in millimeters",
    "text": "Flipper length in millimeters\n\npenguins |>\n  ggplot(aes(flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "W6.html#relate-variables-as-a-line",
    "href": "W6.html#relate-variables-as-a-line",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Relate variables as a line",
    "text": "Relate variables as a line\nA line is a shift-scale transformation of the identity function usually written in the form\n\\[f(x) = a\\cdot x + b\\]\nwhere \\(a\\) is the slope, \\(b\\) is the intercept.1\n\n\na <- 0.5\nb <- 1\nfunc <- function(x) a*x + b\nggplot() + geom_function(fun = func, size = 2) + \n    xlim(c(0,2)) + ylim(c(0,2)) + coord_fixed() + # Set axis limits and make axis equal\n    # intercept line:\n    geom_line(data=tibble(x=c(0,0),y=c(0,1)), mapping = aes(x,y), color = \"blue\") +\n    # slope:\n    geom_line(data=tibble(x=c(1.5,1.5),y=c(1.25,1.75)), mapping = aes(x,y), color = \"red\") +\n    # x-interval of length one:\n    geom_line(data=tibble(x=c(0.5,1.5),y=c(1.25,1.25)), mapping = aes(x,y), color = \"gray\") +\n    theme_classic(base_size = 24)\n\n\n\n\n\n\nThis a scale and a shift in the \\(y\\) direction. Note: For lines there are always an analog transformations on the \\(x\\) direction."
  },
  {
    "objectID": "W6.html#penguins-linear-model",
    "href": "W6.html#penguins-linear-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Penguins: Linear model",
    "text": "Penguins: Linear model\nFlipper length as a function of body mass.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W6.html#penguins-other-smoothing-method",
    "href": "W6.html#penguins-other-smoothing-method",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Penguins: Other smoothing method",
    "text": "Penguins: Other smoothing method\nFlipper length as a function of body mass with loess1 smoothing.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"loess\") + \n theme_classic(base_size = 24)\n\n\n\n\n\n\nThis is a less theory-driven and more data-driven model. Why? We don’t have a simple mathematical form of the function.\nloess = locally estimated scatterplot smoothing"
  },
  {
    "objectID": "W6.html#terminology",
    "href": "W6.html#terminology",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Terminology",
    "text": "Terminology\n\nResponse variable:1 Variable whose behavior or variation you are trying to understand, on the y-axis\nExplanatory variable(s):2 Other variable(s) that you want to use to explain the variation in the response, on the x-axis\nPredicted value: Output of the model function.\n\nThe model function gives the typical (expected) value of the response variable conditioning on the explanatory variables\nResidual(s): A measure of how far away a case is from its predicted value (based on a particular model)\nResidual = Observed value - Predicted value\nThe residual tells how far above/below the expected value each case is\n\n\nAlso dependent variable in statistics or empirical social sciences.Also independent variable(s) in statistics or empirical social sciences."
  },
  {
    "objectID": "W6.html#more-explanatory-variables",
    "href": "W6.html#more-explanatory-variables",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "More explanatory variables",
    "text": "More explanatory variables\nHow does the relation between flipper length and body mass change with different species?\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm, \n            color = species)) +\n geom_point() +\n geom_smooth(method = \"lm\",\n             se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W6.html#technical-how-to-color-penguins-but-keep-one-model",
    "href": "W6.html#technical-how-to-color-penguins-but-keep-one-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Technical: How to color penguins but keep one model?",
    "text": "Technical: How to color penguins but keep one model?\nPut the mapping of the color aesthetic into the geom_point command.\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n                                                y = flipper_length_mm)) +\n geom_point(aes(color = species)) +\n geom_smooth(method = \"lm\",\n                                                    se = FALSE) + \n theme_classic(base_size = 24)"
  },
  {
    "objectID": "W6.html#models---upsides-and-downsides",
    "href": "W6.html#models---upsides-and-downsides",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can reveal patterns that are not evident in a graph of the data. This is an advantage of modeling over simple visual inspection of data.\nThe risk is that a model is imposing structure that is not really there in the real world data.\n\nPeople imagined animal shapes in the stars. This is maybe a good model to detect and memorize shapes, but it has nothing to do with these animals.\nEvery model is a simplification of the real world, but there are good and bad models (for particular purposes).\nA skeptical (but constructive) approach to a model is always advisable."
  },
  {
    "objectID": "W6.html#variation-around-a-model",
    "href": "W6.html#variation-around-a-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Variation around a model",
    "text": "Variation around a model\nis as interesting and important as the model!\nStatistics is the explanation of uncertainty of variation in the context of what remains unexplained.\n\nThe scattered data of flipper length and body mass suggests that there maybe other factors that account for some parts of the variability.\nOr is it randomness?\nAdding more explanatory variables can help (but need not)"
  },
  {
    "objectID": "W6.html#all-models-are-wrong",
    "href": "W6.html#all-models-are-wrong",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "All models are wrong …",
    "text": "All models are wrong …\n… but some are useful. (George Box)\nExtending the range of the model:\n\n\npenguins |>\n ggplot(aes(x = body_mass_g, \n            y = flipper_length_mm)) +\n geom_point() +\n geom_smooth(method = \"lm\", \n             se = FALSE, \n                                                fullrange = TRUE) +\n    xlim(c(0,7000)) + ylim(c(0,230)) +\n theme_classic(base_size = 24)\n\n\n\n\n\n\n\nThe model predicts that penguins with zero weight still have flippers of about 140 mm on average.\nIs the model useless?"
  },
  {
    "objectID": "W6.html#two-model-purposes",
    "href": "W6.html#two-model-purposes",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Two model purposes",
    "text": "Two model purposes\nModels can be used for:\n\nExplanation: Understand the relations hip of variables in a quantitative way.\nFor the linear model, interpret slope and intercept.\nPrediction: Plug in new values for the explanatory variable(s) and receive the expected response value.\nFor the linear model, predict the flipper length of new penguins by their body mass."
  },
  {
    "objectID": "W6.html#in-r-tidymodels",
    "href": "W6.html#in-r-tidymodels",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "In R: tidymodels",
    "text": "In R: tidymodels\n\n\n\nFrom https://datasciencebox.org"
  },
  {
    "objectID": "W6.html#our-goal",
    "href": "W6.html#our-goal",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Our goal",
    "text": "Our goal\nPredict flipper length from body mass\naverage flipper_length_mm \\(= \\beta_0 + \\beta_1\\cdot\\) body_mass_g"
  },
  {
    "objectID": "W6.html#step-1-specify-model",
    "href": "W6.html#step-1-specify-model",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlibrary(tidymodels)\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W6.html#step-2-set-the-model-fitting-engine",
    "href": "W6.html#step-2-set-the-model-fitting-engine",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Step 2: Set the model fitting engine",
    "text": "Step 2: Set the model fitting engine\n\nlinear_reg() |> \n    set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "W6.html#step-3-fit-model-and-estimate-parameters",
    "href": "W6.html#step-3-fit-model-and-estimate-parameters",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Step 3: Fit model and estimate parameters",
    "text": "Step 3: Fit model and estimate parameters\nOnly now, the data and the variable selection comes in.\nUse of formula syntax\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\n\nNote: The fit command does not follow the tidyverse principle the the data comes first. Instead, the formula comes first. This is to relate to existing traditions of a much older established way of modeling in R."
  },
  {
    "objectID": "W6.html#what-does-the-output-say",
    "href": "W6.html#what-does-the-output-say",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "What does the output say?",
    "text": "What does the output say?\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = flipper_length_mm ~ body_mass_g, data = data)\n\nCoefficients:\n(Intercept)  body_mass_g  \n  136.72956      0.01528  \n\n\n\naverage flipper_length_mm \\(= 136.72956 + 0.01528\\cdot\\) body_mass_g\n\n\nInterpretation:\nThe penguins have a flipper length of 138 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg). Penguins with zero mass have a flipper length of 138 mm. However, this is not in the range where the model was fitted …"
  },
  {
    "objectID": "W6.html#show-output-in-tidy-form",
    "href": "W6.html#show-output-in-tidy-form",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Show output in tidy form",
    "text": "Show output in tidy form\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> \n    tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107"
  },
  {
    "objectID": "W6.html#parameter-estimation",
    "href": "W6.html#parameter-estimation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Parameter estimation",
    "text": "Parameter estimation\nNotation from statistics: \\(\\beta\\)’s for the population parameters and \\(b\\)’s for the parameters estimated from the sample statistics.\n\\[\\hat y = \\beta_0 + \\beta_1 x\\]\nIs what we cannot have. (\\(\\hat y\\) stands for predicted value of \\(y\\). )\n\nWe estimate \\(b_0\\) and \\(b_1\\) in\n\\[\\hat y = b_0 + b_1 x\\]\n\nA typical follow-up data analysis question is what the fitted values \\(b_0\\) and \\(b_1\\) tell us about the population-wide values \\(\\beta_0\\) and \\(\\beta_1\\)?\nWhat type of question is it?\nDescriptive, Exploratory, Inferential, Predictive, Causal, Mechanistic\n\n\n\n\nA typical inferential question."
  },
  {
    "objectID": "W6.html#fitting-method-least-squares-regression",
    "href": "W6.html#fitting-method-least-squares-regression",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Fitting method: Least squares regression",
    "text": "Fitting method: Least squares regression\n\nThe regression line shall minimize the sum of the squared residuals (or, identically, their mean).\nMathematically: The residual for case \\(i\\) is \\(e_i = \\hat y_i - y_i\\).\nNow we want to minimize \\(\\sum_{i=1}^n e_i^2\\)\n(or equivalently \\(\\frac{1}{n}\\sum_{i=1}^n e_i^2\\) the the mean of squared errors, which we will look at later)."
  },
  {
    "objectID": "W6.html#visualization-of-residuals",
    "href": "W6.html#visualization-of-residuals",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Visualization of residuals",
    "text": "Visualization of residuals\nThe residuals are the gray lines between predictid values on the regression line and the actual values."
  },
  {
    "objectID": "W6.html#proporties-of-least-squares-regression",
    "href": "W6.html#proporties-of-least-squares-regression",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nThe regression lines goes through the point (mean(x), mean(y)).\n\nmean(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4201.754\n\nmean(penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 200.9152"
  },
  {
    "objectID": "W6.html#proporties-of-least-squares-regression-1",
    "href": "W6.html#proporties-of-least-squares-regression-1",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Proporties of least squares regression",
    "text": "Proporties of least squares regression\nResiduals sum up to zero\n\npengmod <- linear_reg() |>  set_engine(\"lm\") |> fit(flipper_length_mm ~ body_mass_g, data = penguins)\npengmod$fit$residuals |> sum()\n\n[1] -3.765044e-14\n\n\nThere is no correlation between residuals and the explanatory variable\n\ncor(pengmod$fit$residuals, na.omit(penguins$body_mass_g))\n\n[1] -1.353445e-16\n\n\nThe correlation of \\(x\\) and \\(y\\) is the slope \\(b_1\\) corrected by their standard deviations.\n\ncorrelation <- cor(penguins$flipper_length_mm, penguins$body_mass_g, use = \"pairwise.complete.obs\")\nsd_flipper <- sd(penguins$flipper_length_mm, na.rm = T)\nsd_mass <- sd(penguins$body_mass_g, na.rm = T)\nc(correlation, sd_flipper, sd_mass)\n\n[1]   0.8712018  14.0617137 801.9545357\n\ncorrelation * sd_flipper / sd_mass\n\n[1] 0.01527592\n\npengmod$fit$coefficients\n\n (Intercept)  body_mass_g \n136.72955927   0.01527592"
  },
  {
    "objectID": "W6.html#linear-model-when-explanatory-variables-are-categorical",
    "href": "W6.html#linear-model-when-explanatory-variables-are-categorical",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Linear model when explanatory variables are categorical",
    "text": "Linear model when explanatory variables are categorical\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\nWhat happened? Two of the three species categories appear as variables now.\n\nCategorical variables are automatically encoded to dummy variables\nEach coefficient describes the expected difference between flipper length of that particular species compared to the baseline level\nWhat is the baseline level?\n\n\n\nThe first category! (Here alphabetically \"Adelie\")"
  },
  {
    "objectID": "W6.html#how-do-dummy-variables-look",
    "href": "W6.html#how-do-dummy-variables-look",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "How do dummy variables look",
    "text": "How do dummy variables look\n\n\n\nspecies\nspeciesChinstrap\nspeciesGentoo\n\n\n\n\nAdelie\n0\n0\n\n\nChinstrap\n1\n0\n\n\nGentoo\n0\n1\n\n\n\nThen the computation is as usual with the zero-one variables."
  },
  {
    "objectID": "W6.html#interpretation",
    "href": "W6.html#interpretation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Interpretation",
    "text": "Interpretation\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ species, data = penguins) |> \n    tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nFlipper length of the baseline species is the intercept.\nFlipper length of the two other species add their coefficient"
  },
  {
    "objectID": "W6.html#compare-to-a-visualization",
    "href": "W6.html#compare-to-a-visualization",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Compare to a visualization",
    "text": "Compare to a visualization\n\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)        190.       0.540    351.   0        \n2 speciesChinstrap     5.87     0.970      6.05 3.79e-  9\n3 speciesGentoo       27.2      0.807     33.8  1.84e-110\n\n\n\nThe red dots are the average values for species.\nThe rest is a boxplot. More on these later."
  },
  {
    "objectID": "W6.html#where-a-linear-model-is-bad",
    "href": "W6.html#where-a-linear-model-is-bad",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "Where a linear model is bad",
    "text": "Where a linear model is bad\nTotal corona cases in Germany in the first wave 2020."
  },
  {
    "objectID": "W6.html#log-transformation",
    "href": "W6.html#log-transformation",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\nInstead of Cumulative_cases we look at \\(\\log(\\)Cumulative_cases\\()\\)\n\nAlmost perfect fit of a linear model.\nThe model is \\(y=\\beta_0 e^{\\beta_1\\cdot x}\\) (\\(y=\\) Cumulative cases, \\(x=\\) Days).\n\\(\\log(y)=\\log(\\beta_0) + \\beta_1\\cdot x\\) (A linear model!)"
  },
  {
    "objectID": "W6.html#log-transformation-1",
    "href": "W6.html#log-transformation-1",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "\\(\\log\\) transformation",
    "text": "\\(\\log\\) transformation\n\nWhat is the difference to the penguin model?\n\n\n\\(x\\) has an ordered structure and no duplicates\n\nThe fit looks so good. Why?\n\n\nThis shows exponential growth (more later).\nMaybe we can go after a mechanistic explanation here."
  },
  {
    "objectID": "W6.html#however-it-works-only-in-a-certain-range",
    "href": "W6.html#however-it-works-only-in-a-certain-range",
    "title": "W#6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "section": "However, it works only in a certain range …",
    "text": "However, it works only in a certain range …"
  },
  {
    "objectID": "W7.html#descriptive-vs.-inferential-statistics",
    "href": "W7.html#descriptive-vs.-inferential-statistics",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Descriptive vs. Inferential Statistics",
    "text": "Descriptive vs. Inferential Statistics\n\nThe process of using and analyzing summary statistics\n\nSolely concerned with properties of the observed data.\n\nDistinct from inferential statistics:\n\nInference of properties of an underlying distribution given sampled observations from a larger population.\n\n\nSummary Statistics are used to summarize a set of observations to communicate the largest amount of information as simple as possible."
  },
  {
    "objectID": "W7.html#summary-statistics",
    "href": "W7.html#summary-statistics",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Summary statistics",
    "text": "Summary statistics\nUnivariate (for one variable)\n\nMeasures of location, or central tendency\nMeasures of statistical dispersion\nMeasure of the shape of the distribution like skewness or kurtosis\n\nBivariate (for two variables)\n\nMeasures of statistical dependence or correlation"
  },
  {
    "objectID": "W7.html#measures-of-central-tendency-1",
    "href": "W7.html#measures-of-central-tendency-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nGoal: For a sequence of numerical observations \\(x_1,\\dots,x_n\\) we want to measure\n\nthe “typical” value.\na value summarizing the location of values on the numerical axis.\n\nThree different ways:\n\nArithmetic mean (also mean, average): Sum of the all observations divided by the number of observations \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\)\nMedian: Assume \\(x_1 \\leq x_2 \\leq\\dots\\leq x_n\\). Then the median is middlemost values in the sequence \\(x_\\frac{n+1}{2}\\) when \\(n\\) odd. For \\(n\\) even there are two middlemost values and the median is \\(\\frac{x_\\frac{n}{2} + x_\\frac{n+1}{2}}{2}\\)\nMode: The value that appears most often in the sequence."
  },
  {
    "objectID": "W7.html#philosophy-of-aggregation",
    "href": "W7.html#philosophy-of-aggregation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Philosophy of aggregation",
    "text": "Philosophy of aggregation\n\nThe mean represents total value per value.\nExample: per capita income in a town is the total income per individual\nThe median represents the value such that half of the values are lower and higher.\nIn a democracy where each value is represented by one voter preferring it, the median is the value which is unbeatable by an absolute majority. Half of the people prefer higher the other half lower values. (Median voter model)\nThe mode represents the most common value.\nIn a democracy, the mode represents the winner of a plurality vote where each value runs as a candidate and the winner is the one with the most votes."
  },
  {
    "objectID": "W7.html#mean-median-mode-properties",
    "href": "W7.html#mean-median-mode-properties",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Mean, Median, Mode properties",
    "text": "Mean, Median, Mode properties\nAre all of these well defined? (That means, they deliver one unambiguous answer for any sequence.)\n\nMean and median, yes. The mode has no rules for a tie.\n\n\nCan they by generalized to variables with ordered or even unordered categories?\n\n\nMean: No. Median: For ordered categories. Mode: For any categorical variable.\n\n\nIs measure always also in the data sequence?\n\n\nMean: No. Median: Yes, for sequences of odd length. Mode: Yes."
  },
  {
    "objectID": "W7.html#generalized-means",
    "href": "W7.html#generalized-means",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Generalized means1",
    "text": "Generalized means1\nFor \\(x_1, \\dots, x_n > 0\\) and \\(p\\in \\mathbb{R}_{\\neq 0}\\) the generalized mean is\n\\[M_p(x_1, \\dots, x_n) = (\\frac{1}{n}\\sum_{i=1}^n x_i^p)^\\frac{1}{p}\\]\nFor \\(p = 0\\) it is \\(M_0(x_1, \\dots, x_n) = (\\prod_{i=1}^n x_i)^\\frac{1}{n}\\).\n\\(M_1\\) is the arithmetic mean. \\(M_0\\) is called the geometric mean. \\(M_{-1}\\) the harmonic mean.\nNote: Generalized means are often only reasonable when all values are positive \\(x_i > 0\\).\n\n\n\\(M_0\\) can also be expressed as the exponential (\\(\\exp(x) = e^x\\)) of the mean of the the \\(\\log\\)’s of the \\(x_i\\)’s: \\(\\exp(\\log((\\prod_{i=1}^n x_i)^\\frac{1}{n})) = \\exp(\\frac{1}{n}\\sum_{i=1}^n\\log(x_i))\\).\nAlso called power mean or \\(p\\)-mean."
  },
  {
    "objectID": "W7.html#box-cox-transformation-function",
    "href": "W7.html#box-cox-transformation-function",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Box-Cox transformation function",
    "text": "Box-Cox transformation function\nFor \\(p \\in \\mathbb{R}\\): \\(f(x) = \\begin{cases}\\frac{x^p - 1}{p} & \\text{for $p\\neq 0$} \\\\ \\log(x) & \\text{for $p= 0$}\\end{cases}\\)\n\n\nThe \\(p\\)-mean is\n\\[M_p(x) = f^{-1}(\\frac{1}{n}\\sum_{i=1}^n f(x_i))\\]\nwith \\(x = [x_1, \\dots, x_n]\\). \\(f^{-1}\\) is the inverse1 of \\(f\\).\n\n\n\n\n\n\n\n\nThat means \\(f^-1(f(x)) = x =f(f^-1(x))\\)."
  },
  {
    "objectID": "W7.html#application-the-wisdom-of-the-crowd",
    "href": "W7.html#application-the-wisdom-of-the-crowd",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Application: The Wisdom of the Crowd",
    "text": "Application: The Wisdom of the Crowd\n\n\n\nThe collective opinion of a diverse group of independent individuals rather than that of a single expert.\nThe classical wisdom-of-the-crowds finding is about point estimation of a continuous quantity.\nPopularized by James Surowiecki (2004).\nThe opening anecdote is about Francis Galton’s1 surprise in 1907 that the crowd at a county fair accurately guessed the weight of an ox’s meat when their individual guesses were averaged.\n\n\n\n\n\n\nGalton (1822-1911) was a half-cousin to Charles Darwin and one of the founding fathers of statistics. He also was a scientific racist, see https://twitter.com/kareem_carr/status/1575506343401775104?s=20&t=8T5TzrayAWNShmOSzJgCJQ.."
  },
  {
    "objectID": "W7.html#galtons-data",
    "href": "W7.html#galtons-data",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Galton’s data1",
    "text": "Galton’s data1\nWhat is the weight of the meat of this ox?\n\n\n\n\nlibrary(readxl)\ngalton <- read_excel(\"data/galton_data.xlsx\")\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\") + geom_vline(xintercept = median(galton$Estimate), color = \"blue\") + geom_vline(xintercept = Mode(galton$Estimate), color = \"purple\")\n\n\n\n\n787 estimates, true value 1198, mean 1196.7, median 1208, mode 1218\nKenneth Wallis dug out the data from Galton’s notebook and put it here https://warwick.ac.uk/fac/soc/economics/staff/kfwallis/publications/galton_data.xlsx"
  },
  {
    "objectID": "W7.html#viertelfest-bremen-2008",
    "href": "W7.html#viertelfest-bremen-2008",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Viertelfest Bremen 20081",
    "text": "Viertelfest Bremen 20081\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> ggplot(aes(`Schätzung`)) + geom_histogram() + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\")\n\n\n\n\n1226 estimates, the maximal value is 29530000!\nWe should filter out the highest values for the histogram…\nData collected as additional guessing game at the Lottery “Haste mal ’nen Euro?”, data provided by Jan Lorenz https://docs.google.com/spreadsheets/d/1HiYhUrYrsbeybJ10mwsae_hQCawZlUQFOOZzcugXzgA/edit#gid=0"
  },
  {
    "objectID": "W7.html#viertelfest-bremen-2008-1",
    "href": "W7.html#viertelfest-bremen-2008-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Viertelfest Bremen 2008",
    "text": "Viertelfest Bremen 2008\nHow many lots will be sold by the end of the festival?\n\nviertel <- read_csv(\"data/Viertelfest.csv\")\nviertel |> filter(Schätzung<100000) |> ggplot(aes(`Schätzung`)) + geom_histogram(binwidth = 500) + geom_vline(xintercept = 10788, color = \"green\") + \n geom_vline(xintercept = mean(viertel$Schätzung), color = \"red\") + geom_vline(xintercept = median(viertel$Schätzung), color = \"blue\") + geom_vline(xintercept = Mode(viertel$Schätzung), color = \"purple\") + geom_vline(xintercept = exp(mean(log(viertel$Schätzung))), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W7.html#log_10-transformation-viertelfest",
    "href": "W7.html#log_10-transformation-viertelfest",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "\\(\\log_{10}\\) transformation Viertelfest",
    "text": "\\(\\log_{10}\\) transformation Viertelfest\n\nviertel |> mutate(log10Est = log10(Schätzung)) |> ggplot(aes(log10Est)) + geom_histogram(binwidth = 0.05) + geom_vline(xintercept = log10(10788), color = \"green\") + \n geom_vline(xintercept = log10(mean(viertel$Schätzung)), color = \"red\") + geom_vline(xintercept = log10(median(viertel$Schätzung)), color = \"blue\") + geom_vline(xintercept = log10(Mode(viertel$Schätzung)), color = \"purple\") + geom_vline(xintercept = mean(log10(viertel$Schätzung)), color = \"orange\")\n\n\n1226 estimates, true value 10788, mean 53163.9, median 9843, mode 10000,\ngeometric mean 10510.1"
  },
  {
    "objectID": "W7.html#wisdom-of-the-crowd-insights",
    "href": "W7.html#wisdom-of-the-crowd-insights",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Wisdom of the crowd insights",
    "text": "Wisdom of the crowd insights\n\n\nIn Galton’s sample the different measures do not make a big difference\nIn the Viertelfest data the arithmetic mean performs very bad!\nThe mean is vulnerable to extreme values. Galton on the mean as a democratic aggregation function: “The mean gives voting power to the cranks in proportion to their crankiness.”\nThe mode tends to be on focal values as round numbers (10,000). In Galton’s data this is not so pronounced beause estimators used several units which Galton had to convert.\nHow to choose a measure to aggreagte the wisdom?\n\nBy the nature of the estimate problem? Is the scale mostly clear? (Are we in the hundreds, thousands, ten thousands, …)\nBy the nature of the distribution?\nThere is no real insurance against a systematic bias in the population."
  },
  {
    "objectID": "W7.html#measures-of-dispersion-1",
    "href": "W7.html#measures-of-dispersion-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Measures of dispersion1",
    "text": "Measures of dispersion1\nGoal: We want to measure\n\nhow spread out values are around the central tendency.\nHow stretched or squeezed is the distribution?\n\nVariance is the mean of the squared deviation from the mean: \\(\\text{Var}(x) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2\\) where \\(\\mu\\) (mu) is the mean.\nStandard deviation is the square root of the variance \\(\\text{SD}(x) = \\sqrt{\\text{Var}(x)}\\).\nThe standard deviation is often denoted \\(\\sigma\\) (sigma) and the variance \\(\\sigma^2\\).\nMean absolute deviation (MAD) is the mean of the absolute deviation from the mean: \\(\\text{MAD}(x) = \\frac{1}{n}\\sum_{i=1}^n|x_i - \\mu|\\).\nRange is the difference of the maximal and the minimal value \\(\\max(x) - \\min(x)\\).\nAlso called variability, scatter, or spread."
  },
  {
    "objectID": "W7.html#examples-of-measures-of-dispersion",
    "href": "W7.html#examples-of-measures-of-dispersion",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Examples of measures of dispersion",
    "text": "Examples of measures of dispersion\n\n\n\nvar(galton$Estimate)\n\n[1] 5415.013\n\nsd(galton$Estimate)\n\n[1] 73.58677\n\nmad(galton$Estimate)\n\n[1] 51.891\n\nrange(galton$Estimate)\n\n[1]  896 1516\n\ndiff(range(galton$Estimate))\n\n[1] 620\n\n\n\n\nvar(viertel$Schätzung)\n\n[1] 719774887849\n\nsd(viertel$Schätzung)\n\n[1] 848395.5\n\nmad(viertel$Schätzung)\n\n[1] 8771.803\n\nrange(viertel$Schätzung)\n\n[1]      120 29530000\n\ndiff(range(viertel$Schätzung))\n\n[1] 29529880\n\n\n\n\n\n\nVariance (and standard deviation) in statistics is usually computed with \\(\\frac{1}{n-1}\\) instead of \\(\\frac{1}{n}\\) to provide an unbiased estimator of the potentially underlying population variance. We omit more detail here."
  },
  {
    "objectID": "W7.html#standardization",
    "href": "W7.html#standardization",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Standardization",
    "text": "Standardization\nVariables are standardized by subtracting their mean and then dividing by their standard deviations.\nA value from a standardized variable is called a standard score or z-score.\n\\(z_i = \\frac{x_i - \\mu}{\\sigma}\\)\nwhere \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation of the vector \\(x\\).\n\nThis is a shift-scale transformation. We shift by the mean and scale by the standard deviation.\nA standard score \\(z_i\\) shows how mean standard deviations \\(x_i\\) is away from the mean of \\(x\\)."
  },
  {
    "objectID": "W7.html#quantiles",
    "href": "W7.html#quantiles",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Quantiles",
    "text": "Quantiles\nCut points specifying intervals which contain equal amounts of values of the distribution.\n\\(q\\)-quantiles divide into \\(q\\) intervals covering all values.\nThe quantiles are the cut points: For \\(q\\) intervals there are \\(q-1\\) cut points of interest.\n\nThe one 2-quantile is the median.\nThe three 4-quantiles are called quartiles. The second quartile is the median.\n100-quantiles are called percentiles\n\n\n\nWe omit problems of estimating quantiles from a sample where the number of estimates does not fit to a desired partion of equal size here."
  },
  {
    "objectID": "W7.html#examples-of-quantiles",
    "href": "W7.html#examples-of-quantiles",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Examples of quantiles",
    "text": "Examples of quantiles\n\n# Min, 3 Quartiles, Max\nquantile(galton$Estimate, prob = seq(0, 1, by = 0.25))\n\n    0%    25%    50%    75%   100% \n 896.0 1162.5 1208.0 1236.0 1516.0 \n\nquantile(viertel$Schätzung, prob = seq(0, 1, by = 0.25))\n\n      0%      25%      50%      75%     100% \n     120     5000     9843    20000 29530000 \n\n\nInterpretation: What does the value at 25% mean?\n\nThe 25% of all values are lower than the value. 75% are larger."
  },
  {
    "objectID": "W7.html#interquartile-range",
    "href": "W7.html#interquartile-range",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Interquartile range",
    "text": "Interquartile range\nThe difference between the 1st and the 3rd quartile.\n\nA very common measure of dispersion.\n\nExamples:\n\n# Min, 3 Quartiles, Max\nIQR(galton$Estimate)\n\n[1] 73.5\n\nsd(galton$Estimate) # for comparison\n\n[1] 73.58677\n\nIQR(viertel$Schätzung)\n\n[1] 15000\n\nsd(viertel$Schätzung)\n\n[1] 848395.5"
  },
  {
    "objectID": "W7.html#summary-of-numerical-vectors-in-r",
    "href": "W7.html#summary-of-numerical-vectors-in-r",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Summary of numerical vectors in R",
    "text": "Summary of numerical vectors in R\n\n\n\nsummary(galton$Estimate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    896    1162    1208    1197    1236    1516 \n\n\n\n\nsummary(viertel$Schätzung)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n     120     5000     9843    53164    20000 29530000 \n\n\n\n\nIt also works for data frames.\n\nlibrary(palmerpenguins)\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2"
  },
  {
    "objectID": "W7.html#boxplots",
    "href": "W7.html#boxplots",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Boxplots",
    "text": "Boxplots\nA condensed visualization of a distribution showing location, spread, skewness and outliers.\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\nThe box shows the median in the middle and the other two quartiles as their borders.\nWhiskers: From above the upper quartile, a distance of 1.5 times the IQR is measured out and a whisker is drawn up to the largest observed data point from the dataset that falls within this distance. Similarly, for the lower quartile.\nWhiskers must end at an observed data point! (So lengths can differ.)\nAll other values outside of box and whiskers are shown as points and often called outliers. (There may be none.)"
  },
  {
    "objectID": "W7.html#boxplots-vs.-histograms",
    "href": "W7.html#boxplots-vs.-histograms",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nHistograms can show the shape of the distribution well, but not the summary statistics like the median.\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_boxplot()\n\n\n\n\n\ngalton |> ggplot(aes(x = Estimate)) + geom_histogram(binwidth = 5)"
  },
  {
    "objectID": "W7.html#boxplots-vs.-histograms-1",
    "href": "W7.html#boxplots-vs.-histograms-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Boxplots vs. histograms",
    "text": "Boxplots vs. histograms\n\nBoxplots can not show the patterns of bimodal or multimodal distributions.\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_boxplot()\n\n\n\n\n\npalmerpenguins::penguins |> ggplot(aes(x = flipper_length_mm)) + geom_histogram(binwidth = 1)"
  },
  {
    "objectID": "W7.html#minimizing-proporties-of-mean-and-median",
    "href": "W7.html#minimizing-proporties-of-mean-and-median",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Minimizing proporties of Mean and Median",
    "text": "Minimizing proporties of Mean and Median\nMean minimizes the mean of squared deviations from it. No other value \\(a\\) has a lower mean of square distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n(x_i - a)^2\\).\nMedian minimizes the sum of the absolute deviation. No other value \\(a\\) has a lower mean of absolute distances from the data points. \\(\\frac{1}{n}\\sum_{i=1}^n|x_i - a|\\)."
  },
  {
    "objectID": "W7.html#two-families-of-summary-statistics",
    "href": "W7.html#two-families-of-summary-statistics",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Two families of summary statistics",
    "text": "Two families of summary statistics\n\nMeasures based on sums (related to mathematical moments)\n\nMean\nStandard deviation\n\nMeasures based on the ordered sequence of these observations (order statistics)\n\nMedian (and all quantiles)\nInterquartile range"
  },
  {
    "objectID": "W7.html#covariance",
    "href": "W7.html#covariance",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Covariance",
    "text": "Covariance\nGoal: We want to measure the joint variation in a sequences of numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nCovariance \\(\\text{cov}(x,y) = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)\\)\nwhere \\(\\mu_x\\) and \\(\\mu_y\\) are the arithmetic means of \\(x\\) and \\(y\\).\nNote: \\(\\text{cov}(x,x) = \\text{Var}(x)\\)"
  },
  {
    "objectID": "W7.html#correlation",
    "href": "W7.html#correlation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation",
    "text": "Correlation\nGoal: We want to measure the linear correlation in a sequences of numerical observations of two variables \\(x_1,\\dots,x_n\\) and \\(y_1, \\dots, y_n\\).\nPearson correlation coefficient \\(r_{xy} = \\frac{\\sum_{i=1}^n(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^n(x_i - \\mu_x)^2}\\sqrt{\\sum_{i=1}^n(y_i - \\mu_y)^2}}\\)\nNote, \\(r_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}\\)\nwhere \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(x\\) and \\(y\\).\nNote, when \\(x\\) and \\(y\\) are standard scores (each with mean zero and standard deviation one), then \\(\\text{cov}(x,y) = r_{xy}\\).\n\n\nThere are other correlation coefficients which we omit hear."
  },
  {
    "objectID": "W7.html#interpretation-of-correlation",
    "href": "W7.html#interpretation-of-correlation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Interpretation of correlation",
    "text": "Interpretation of correlation\nCorrelation between two vectors \\(x\\) and \\(y\\) is “normalized”.\n\nThe maximal possible values is \\(r_{xy} = 1\\)\n\n\\(x\\) and \\(y\\) are fully correlated\n\nThe minimal values is \\(r_{xy} = -1\\)\n\n\\(x\\) and \\(y\\) are anticorrelated\n\nWhen \\(r_{xy} \\approx 0\\) the variables are uncorrelated\n\\(r_{xy} = r_{yx}\\)"
  },
  {
    "objectID": "W7.html#correlation-and-linear-regression",
    "href": "W7.html#correlation-and-linear-regression",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation and linear regression",
    "text": "Correlation and linear regression\n\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(flipper_length_mm ~ body_mass_g, data = penguins) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) 137.      2.00          68.5 5.71e-201\n2 body_mass_g   0.0153  0.000467      32.7 4.37e-107\n\n\n\npenguins_standardized <- penguins |> \n mutate(flipper_length_mm_s = scale(flipper_length_mm)[,1],\n        body_mass_g_s = scale(body_mass_g)[,1])\nlinear_reg() |> \n set_engine(\"lm\") |> \n fit(flipper_length_mm_s ~ body_mass_g_s, data = penguins_standardized) |> tidy()\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   -9.33e-16    0.0266 -3.51e-14 1.00e+  0\n2 body_mass_g_s  8.71e- 1    0.0266  3.27e+ 1 4.37e-107\n\n\n\ncor(penguins_standardized$body_mass_g_s,\n    penguins_standardized$flipper_length_mm_s, \n    use = \"pairwise.complete.obs\")\n\n[1] 0.8712018"
  },
  {
    "objectID": "W7.html#correlation-and-linear-regression-1",
    "href": "W7.html#correlation-and-linear-regression-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation and linear regression",
    "text": "Correlation and linear regression\nWhen the two variables in the linear regression are standardized (standard scores)\n\nthe intercept is zero\nthe coefficient coincides with the correlation\n\n\n\n\nThe function scale is a function for standardization from base R which delivers a matrix which is unconventional for tidyverse.\nThe function cor is also from base R. It outputs NA whenever one value in one variable is NA. Therefore a methods to use has to be specified. use = \"pairwise.complete.obs\" keeps all values where both variables are not NA."
  },
  {
    "objectID": "W7.html#correlation-in-exploratory-data-analysis",
    "href": "W7.html#correlation-in-exploratory-data-analysis",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation in exploratory data analysis",
    "text": "Correlation in exploratory data analysis\nUsing corrr from tidymodels\n\nlibrary(corrr)\ness <- read_csv(\"data/ESS-Data-Wizard-subset-2022-09-17.csv\") \ness_na <- function(x) ifelse(x > 10, NA, x)\ness_sel <- ess |> select(cntry, essround, euftf:stflife) |> \n mutate(across(euftf:stflife, ess_na))\ness_sel |> select(euftf:stflife) |> \n correlate()\n\n# A tibble: 5 × 6\n  term       euftf  gincdif  lrscale  polintr stflife\n  <chr>      <dbl>    <dbl>    <dbl>    <dbl>   <dbl>\n1 euftf   NA        0.00580 -0.0247  -0.0658   0.0788\n2 gincdif  0.00580 NA        0.153   -0.00153  0.132 \n3 lrscale -0.0247   0.153   NA        0.00466  0.113 \n4 polintr -0.0658  -0.00153  0.00466 NA       -0.120 \n5 stflife  0.0788   0.132    0.113   -0.120   NA"
  },
  {
    "objectID": "W7.html#correlation-in-eda",
    "href": "W7.html#correlation-in-eda",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation in EDA",
    "text": "Correlation in EDA\n\ness_sel |> filter(cntry == \"DE\", essround == 9) |> \n select(euftf:stflife) |> \n correlate()\n\n# A tibble: 5 × 6\n  term      euftf gincdif lrscale polintr stflife\n  <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 euftf   NA      -0.0565 -0.147  -0.136   0.171 \n2 gincdif -0.0565 NA       0.161   0.0671  0.0727\n3 lrscale -0.147   0.161  NA       0.0222  0.0682\n4 polintr -0.136   0.0671  0.0222 NA      -0.104 \n5 stflife  0.171   0.0727  0.0682 -0.104  NA"
  },
  {
    "objectID": "W7.html#correlation-in-eda-1",
    "href": "W7.html#correlation-in-eda-1",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Correlation in EDA",
    "text": "Correlation in EDA\nUsing correlate and see from easystats\n\nlibrary(correlation)\nlibrary(see)\nresults <- ess_sel |> filter(cntry == \"DE\", essround == 9) |> \n select(euftf:stflife) |> \n correlation()\nresults %>%\n  summary(redundant = TRUE) %>%\n  plot()"
  },
  {
    "objectID": "W7.html#epidemic-modeling",
    "href": "W7.html#epidemic-modeling",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Epidemic Modeling",
    "text": "Epidemic Modeling\n\nAssume a population of N individuals.\n\nIndividuals can have different states, e.g.: Susceptible, Infectious, Recovered, …\nThe population divides into compartments of these states which change over time, e.g.: \\(S(t), I(t), R(t)\\) number of susceptible, infectious, recovered individuals\n\nNow we define dynamics like\n\nwhere the numbers on the arrows represent transition probabilities."
  },
  {
    "objectID": "W7.html#si-model",
    "href": "W7.html#si-model",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI model",
    "text": "SI model\nToday we only treat the SI part of the model.\n\nPeople who are susceptible can become infected through contact with infectious.\nPeople who are infectious stay infectious\n\nThe parameter \\(\\beta\\) is the average number of contacts per unit time multiplied with the probability that an infection happens during such a contact.\nTwo compartments:\n\\(S(t)\\) is the number of susceptible people at time \\(t\\).\n\\(I(t)\\) is the number of infected people at time \\(t\\).\nIt always holds \\(S(t) + I(t) = N\\). (The total population is constant.)"
  },
  {
    "objectID": "W7.html#how-many-infections-per-time",
    "href": "W7.html#how-many-infections-per-time",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "How many infections per time?",
    "text": "How many infections per time?\nThe change of the number of infectious\n\\[\\frac{dI}{dt} = \\underbrace{\\beta}_\\text{infection prob.} \\cdot \\underbrace{\\frac{S}{N}}_\\text{frac. of $S$ still there} \\cdot \\underbrace{\\frac{I}{N}}_\\text{frac. $I$ to meet} \\cdot N = \\frac{\\beta\\cdot S\\cdot I}{N}\\]\nwhere \\(dI\\) is the change of \\(I\\) (the newly infected here) and \\(dt\\) the time interval.\nInterpretation: The newly infected are from the fraction of susceptible times the probability that they meet an infected times the infection probability times the total number of individuals.\nUsing \\(S = N - I\\) we rewrite\n\\[\\frac{dI}{dt} = \\frac{\\beta (N-I)I}{N}\\]"
  },
  {
    "objectID": "W7.html#ordinary-differential-equation",
    "href": "W7.html#ordinary-differential-equation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Ordinary differential equation",
    "text": "Ordinary differential equation\nWe interpret \\(I(t)\\) as a function of time which gives us the number of infectious at each point in time. The change function is now\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nand \\(\\frac{dI(t)}{dt}\\) is also called the derivative of \\(I(t)\\)."
  },
  {
    "objectID": "W7.html#derivatives",
    "href": "W7.html#derivatives",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Derivatives",
    "text": "Derivatives\n\n\n\nThe derivative of a function is also a function with the same domain.\nMeasures the sensitivity to change of the function output when the input changes (a bit)\nExample from physics: The derivative of the position of a moving object is its speed. The derivative of its speed is its acceleration.\nGraphically: The derivative is the slope of a tangent line of the graph of a function."
  },
  {
    "objectID": "W7.html#differentiation",
    "href": "W7.html#differentiation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Differentiation",
    "text": "Differentiation\nis the process to compute the derivative. For parameters \\(a\\) and \\(b\\) and other functions \\(g\\) and \\(h\\), rules of differentiation are\n\n\nFunction \\(f(x)\\)\n\\(a\\cdot x\\)\n\\(b\\)\n\\(x^2,\\ x^{-1} = \\frac{1}{x},\\ x^k\\)\n\\(g(x) + h(x)\\)\n\\(g(x)\\cdot h(x)\\)\n\\(g(h(x))\\)\n\\(e^x,\\ 10^x = e^{\\log(10)x}\\)\n\\(\\log(x)\\)\n\nIts derivative \\(\\frac{df(x)}{dx}\\) or \\(\\frac{d}{dx}f(x)\\) or \\(f'(x)\\)\n\n\\(a\\)\n\n\n\\(0\\)\n\n\n\\(2\\cdot x,\\ -x^{-2} = -\\frac{1}{x^2},\\ k\\cdot x^{k-1}\\)\n\n\n\\(g'(x) + h'(x)\\)\n\n\n\\(g'(x)\\cdot h(x) + g(x)\\cdot h'(x)\\) (product rule)\n\n\n\\(g'(h(x))\\cdot h'(x)\\) (chain rule)\n\n\n\\(e^x,\\ 10^x = \\log(10)\\cdot10^x\\)\n\n\n\\(\\frac{1}{x}\\) (This is a “surprising” relation …)"
  },
  {
    "objectID": "W7.html#differential-equation",
    "href": "W7.html#differential-equation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Differential equation",
    "text": "Differential equation\nIn a differential equation the unknown is a function!\nWe are looking for a function which derivative is a function of the function itself.\nExample: SI-model\n\\[\\frac{dI(t)}{dt} = \\frac{\\beta (N-I(t))I(t)}{N}\\]\nWhich function \\(I(t)\\) fulfills this equation?\nThe analytical solution1 is\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nWhich is called the logistic equation.\nNote, we need to specify the initial number of infectious individuals \\(I(0)\\).\nCan you check that this is correct? If you want but can’t, don’t hesitate to ask later."
  },
  {
    "objectID": "W7.html#si-model-logistic-equation",
    "href": "W7.html#si-model-logistic-equation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI-model: Logistic Equation",
    "text": "SI-model: Logistic Equation\n\\(I(t) = \\frac{N}{1 + (\\frac{N}{I(0)} - 1)e^{-\\beta t}}\\)\nPlot the equation for \\(N = 10000\\), \\(I_0 = 1\\), and \\(\\beta = 0.3\\)\n\nN <- 10000\nI0 <- 1\nbeta <- 0.3\nggplot() + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ) + \n xlim(c(0,75))"
  },
  {
    "objectID": "W7.html#si-model-numerical-integration",
    "href": "W7.html#si-model-numerical-integration",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI-model: Numerical integration",
    "text": "SI-model: Numerical integration\nAnother way of solution using, e.g., using Euler’s method.\nWe compute the solution step-by-step using small increments of, e.g. \\(dt = 0.5\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 0.5 # time increment, supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) # this is the vector of timesteps\nIt <- I0 # this will become the vector of the number infected I(t) over time\nfor (i in 2:length(t)) { # We iterate over the vector of time steps and incrementally compute It\n  It[i] = It[i-1] + dt * dI(It[i-1], N, beta) # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ,\n                color = \"red\") + # Analytical solution for comparison\n geom_line() # The numerical solution in black"
  },
  {
    "objectID": "W7.html#numerical-integration-more-precise-with-small-dt",
    "href": "W7.html#numerical-integration-more-precise-with-small-dt",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Numerical integration more precise with small \\(dt\\)",
    "text": "Numerical integration more precise with small \\(dt\\)\nWe compute the solution step-by-step using small increments of, e.g. \\(dt = 0.01\\).\n\n\nN <- 10000\nI0 <- 1\ndI <- function(I,N,b) b*I*(N - I)/N\nbeta <- 0.3\ndt <- 0.01 # time increment, supposed to be infinitesimally small\ntmax <- 75\nt <- seq(0,tmax,dt) # this is the vector of timesteps\nIt <- I0 # this will become the vector of the number infected I(t) over time\nfor (i in 2:length(t)) { # We iterate over the vector of time steps and incrementally compute It\n  It[i] = It[i-1] + dt * dI(It[i-1], N, beta) # This is called Euler's method\n}\ntibble(t, It) |> ggplot(aes(t,It)) + \n geom_function( fun = function(t) N / (1 + (N/I0 - 1)*exp(-beta*t)) ,\n                color = \"red\") + # Analytical solution for comparison\n geom_line() # The numerical solution in black"
  },
  {
    "objectID": "W7.html#si-model-simulation",
    "href": "W7.html#si-model-simulation",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "SI-model: Simulation",
    "text": "SI-model: Simulation\nAnother more basic solution is direct individual-based simulation.\n\nWe produce a vector of length \\(N\\) with entries representing the state of each individual as \"S\" or \"I\".\nWe model the random infection process in each step of unit time\n\n\n\nN <- 10000\nbeta <- 0.3\nrandomly_infect <- function(N, prob) { runif(N) < prob }\n# Gives a logical vector of length N where TRUE appears with probability 0.3\ninit <- rep(\"S\",N) # All susceptible\ninit[sample.int(N, size=1)] <- \"I\" # Infect one individual\ntmax <- 75\nsim_run <- list(init)\nfor (i in 2:tmax) {\n contacts <- sample(sim_run[[i-1]], size = N)\n sim_run[[i]] <- if_else(contacts == \"I\" & randomly_infect(N, beta), \n                         true = \"I\", \n                         false = sim_run[[i-1]])\n}\nsim_output <- tibble(t = 0:(tmax-1), \n       # Compute a vector with length tmax with the count of \"I\" in sim_run list\n       infected = map_dbl(sim_run, function(x) sum(x == \"I\"))) \nsim_output |> ggplot(aes(t,infected)) + geom_line()"
  },
  {
    "objectID": "W7.html#questions-on-programming-concepts",
    "href": "W7.html#questions-on-programming-concepts",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Questions on programming concepts?",
    "text": "Questions on programming concepts?\nFrom base R:\nrunif random numbers\nsample.int and sample\nfor loops\nif and else ifelse\nFrom purrr:\nmap apply function to lists and collect output"
  },
  {
    "objectID": "W7.html#three-ways-to-explore-mechanistic-models",
    "href": "W7.html#three-ways-to-explore-mechanistic-models",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Three ways to explore mechanistic models",
    "text": "Three ways to explore mechanistic models\n\nIndividual-based simulation\n\nWe model every individual explicitly\nSimulation involve random numbers! So simulation runs can be different!\n\nNumerical integration of differential equation\n\nNeeds a more abstract concept of compartments\n\nAnalytical solutions of differential equation\n\noften not possible or not in nice form\n\n\nSuch mechanistic models are typical for natural sciences, but they also make sense for many processes in societies and ecomonies."
  },
  {
    "objectID": "W7.html#differentiation-with-data",
    "href": "W7.html#differentiation-with-data",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Differentiation with data",
    "text": "Differentiation with data\nIn empirical data we can compute the increase in a vector with the function diff:\n\nx <- c(1,2,4,5,5,3,0)\ndiff(x)\n\n[1]  1  2  1  0 -2 -3\n\n\nMore convenient for in a data frame is to use x - lag(x) because the vector has the same length.\n\nx - lag(x)\n\n[1] NA  1  2  1  0 -2 -3"
  },
  {
    "objectID": "W7.html#the-diff-of-our-simulation-output",
    "href": "W7.html#the-diff-of-our-simulation-output",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "The diff of our simulation output",
    "text": "The diff of our simulation output\n\ng2 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative_infected))\ng1 <- sim_output |> ggplot(aes(x = t)) + geom_line(aes(y = infected))\ng2\n\n\n\ng1"
  },
  {
    "objectID": "W7.html#nd-derivative-change-of-change",
    "href": "W7.html#nd-derivative-change-of-change",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "2nd derivative: Change of change",
    "text": "2nd derivative: Change of change\n\ng3 <- sim_output |> \n mutate(derivative_infected = infected - lag(infected),\n        derivative2_infected = derivative_infected - lag(derivative_infected)) |> \n ggplot(aes(x = t)) + geom_line(aes(y = derivative2_infected))\ng3\n\n\nIn empirical data: Derivatives of higher order tend to show fluctuation"
  },
  {
    "objectID": "W7.html#interpretation-in-si-model",
    "href": "W7.html#interpretation-in-si-model",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Interpretation in SI-model",
    "text": "Interpretation in SI-model\n\n\n\\(I(t)\\) total number of infected\n\\(I'(t)\\) number of new cases per day (time step)\n\\(I''(t)\\) how the number of new cases has changes compared to yesterday\n\nCan be a good early indicator for the end of a wave."
  },
  {
    "objectID": "W7.html#integration",
    "href": "W7.html#integration",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Integration",
    "text": "Integration\nThe integral of the daily new cases from the beginning to day \\(s\\) is \\(\\int_{-\\infty}^s f(t)dt\\) and represents the total cases at day \\(s\\).\n\nThe integral of a function \\(f\\) up to time \\(s\\) is also called the anti-derivative \\(F(s) = \\int_{-\\infty}^s f(t)dt\\).\nCompute the anti-derivative of data vector with cumsum.\n\n\nx <- c(1,2,4,5,5,3,0)\ncumsum(x)\n\n[1]  1  3  7 12 17 20 20\n\n\n\nEmpirically derivatives tend to become noisy, while integrals tend to become smooth."
  },
  {
    "objectID": "W7.html#fundamental-theorem-of-calculus",
    "href": "W7.html#fundamental-theorem-of-calculus",
    "title": "W#7 Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "section": "Fundamental theorem of calculus",
    "text": "Fundamental theorem of calculus\nThe integral of the derivative is the function itself.\nThis is not a proof but shows the idea:\n\nf <- c(1,2,4,5,5,3,0)\nantiderivative <- cumsum(f)\ndiff(c(0, antiderivative)) # We have to put 0 before to regain the full vector\n\n[1] 1 2 4 5 5 3 0\n\nderivative <- diff(f)\ncumsum(c(1,derivative)) # We have to put in the first value (here 1) manually because it was lost during the diff\n\n[1] 1 2 4 5 5 3 0"
  },
  {
    "objectID": "W8.html#strange-airports-homework-02",
    "href": "W8.html#strange-airports-homework-02",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Strange Airports (Homework 02)",
    "text": "Strange Airports (Homework 02)\n\nlibrary(nycflights13)\nggplot(data = airports, mapping = aes(x = lon, y = lat)) + geom_point(aes(color = tzone)) \n\n\n\nairports %>% filter(lon >= 0) \n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…"
  },
  {
    "objectID": "W8.html#airport-errors",
    "href": "W8.html#airport-errors",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Airport errors",
    "text": "Airport errors\n\n\n# A tibble: 4 × 8\n  faa   name                            lat   lon   alt    tz dst   tzone       \n  <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr> <chr>       \n1 DVT   Deer Valley Municipal Airport  33.4 112.   1478     8 A     Asia/Chongq…\n2 EEN   Dillant Hopkins Airport        72.3  42.9   149    -5 A     <NA>        \n3 MYF   Montgomery Field               32.5 118.     17     8 A     Asia/Chongq…\n4 SYA   Eareckson As                   52.7 174.     98    -9 A     America/Anc…\n\n\nCorrect locations (internet research and location of maps):\n\n\n\nDeer Valley Municipal Airport: Phoenix\n33°41′N 112°05′W Missing minus for lon (W)\nDillant Hopkins Airport: New Hampshire\n42°54′N 72°16′W lon-lat switched, minus (W)\nMontgomery Field: San Diego\n32°44′N 117°11″W Missing minus for lon (W)\nEareckson As: Alaska\n52°42′N 174°06′E No error: Too west,it’s east!"
  },
  {
    "objectID": "W8.html#conclusions-on-data-errors",
    "href": "W8.html#conclusions-on-data-errors",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Conclusions on data errors",
    "text": "Conclusions on data errors\n\nIn real-world datasets errors like the 3 airport are quite common.\nErrors of this type are often hard to detect and remain unnoticed.\n\nThis can (but need not) change results drastically!\n\n\n\nConclusions\n\nAlways remain alert for inconsistencies and be ready to check the plausibility of results.\nSkills in exploratory data analysis (EDA) are essential to find errors and explore their nature and implication\nErrors are unpredictable, of diverse types, and often deeply related to the reality the data presents.\n\nOne reason why EDA can not be a fully formalized and automatized process."
  },
  {
    "objectID": "W8.html#same-phenomenon-different-data",
    "href": "W8.html#same-phenomenon-different-data",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Same phenomenon different data?",
    "text": "Same phenomenon different data?\nQuestion: Is the data of OWiD and WHO the same?\n\n\n\nNo.\nWhy?\nWhat are the data sources of WHO and OWiD?\n\n\n\n\n\n\n\n\n\nOWiD documentation refers to have data from CSSE at Johns Hopkins University which document various data sources (e.g., a newspaper from Germany)\nWHO documentation says: “WHO collected the numbers of confirmed COVID-19 cases and deaths through official communications under the International Health Regulations (IHR, 2005), complemented by monitoring the official ministries of health websites and social media accounts.” For Germany this is the Robert-Koch-Institut RKI."
  },
  {
    "objectID": "W8.html#good-reasons-for-different-data",
    "href": "W8.html#good-reasons-for-different-data",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Good reasons for different data?",
    "text": "Good reasons for different data?\n\nDuring the pandemic daily new case numbers were relevant for decisions about safety measures.\nIn reality, data comes with delays.\n\nExample: Recent new cases in Germany (RKI). Notice many new cases several days ago."
  },
  {
    "objectID": "W8.html#conflict-day-to-day-consistency-and-correctness",
    "href": "W8.html#conflict-day-to-day-consistency-and-correctness",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Conflict day-to-day consistency and correctness",
    "text": "Conflict day-to-day consistency and correctness\n\nFixing daily cases is useful to record the numbers on which daily safety decisions are based.\nCorrected cases (which also change data from the past) are better for analysis in retrospect. It reflects the actual pandemic better."
  },
  {
    "objectID": "W8.html#reported-cases-and-real-cases",
    "href": "W8.html#reported-cases-and-real-cases",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Reported cases and real cases?",
    "text": "Reported cases and real cases?\nCase numbers are to inform us about real cases. What type of data analysis question is this? (Descriptive, Exploratory, Inferential, Predictive, Causal, Mechanistic)\n\n\nInferential: “Quantify whether the discovery is likely to hold in a new sample.”\n\nHere: What do reported cases tell us about cases in the whole population?\n\nLimitations\n\nWe cannot test all\nTests are not on a random sample\nMild/asymptomatic cases remain unnoticed even to individuals\n…\n\n\nThe unknown: What is the dark figure?"
  },
  {
    "objectID": "W8.html#excercise-for-german-new-case-counts",
    "href": "W8.html#excercise-for-german-new-case-counts",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Excercise for German new case counts",
    "text": "Excercise for German new case counts\n\n\nCan we infer the real incidence (= new cases per 100,000)?\nWhat can we infer the trend of the real incidence?\n\n\nIncidence: Not really, we would need a either a random sample (then we can infer the fraction of infected), or an idea how to estimate the dark figure.\nTrend: Yes! Under the assumptions that reported cases do reflect a relevant part of the pandemic and the limitation remain mostly constant during the observed trend."
  },
  {
    "objectID": "W8.html#smoothing-time-series",
    "href": "W8.html#smoothing-time-series",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Smoothing time series",
    "text": "Smoothing time series\n\n\n\nx <- c(1, 2, 5, 3, 0)\nx\n\n\n[1] 1 2 5 3 0\n\n\n\n\n\nzoo::rollmean(x, k = 3, na.pad = TRUE) # for centered window\n\n\n[1]       NA 2.666667 3.333333 2.666667       NA\n\n\n\n\n\n(x + lag(x, n = 1) + lag(x, n = 2))/3 # for lagged window\n\n\n[1]       NA       NA 2.666667 3.333333 2.666667\n\n\n\n\nCentered: Leaves smoothed data close to real data.\nLagged: Lags the smoothed data, but can be consistently computed for the newest day\nRemember: Data with weekly seasonality is best smoothed with a weekly window!"
  },
  {
    "objectID": "W8.html#total-death-per-million-and-human-development",
    "href": "W8.html#total-death-per-million-and-human-development",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Total death per million and human development",
    "text": "Total death per million and human development\n\nWhat findings? Explanations?"
  },
  {
    "objectID": "W8.html#level-of-measurement",
    "href": "W8.html#level-of-measurement",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Level of measurement",
    "text": "Level of measurement\n\nNominal: the data can only be categorized\nOrdinal: the data can be categorized and ranked\nInterval: the data can be categorized, ranked, and evenly spaced\nRatio: the data can be categorized, ranked, evenly spaced, and has a natural zero.\n\nWhat is the difference of level of measurement and data type?\n\nMainly perspective:\n\nLevel of measurement is about the variable/the thing which is measured.\nData type is more about the technical way to store data."
  },
  {
    "objectID": "W8.html#scales-in-surveysquestionaires",
    "href": "W8.html#scales-in-surveysquestionaires",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Scales in surveys/questionaires",
    "text": "Scales in surveys/questionaires\nLikert scale: Strongly disagree … [scale steps] … Strongly agree\nRating scales: Extreme statement … [scale steps] … Opposite statement\nWhat level of measurement do these questions have?\n \n\nOrdinal clearly, interval assuming scale steps are equal, ratio assuming 5 as natural zero"
  },
  {
    "objectID": "W8.html#dealing-with-missing-values",
    "href": "W8.html#dealing-with-missing-values",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\nCoding in the ESS data:\n\nInterval data coded numerically 0, 1, …, 10\nMissing values with numerical codes 77, 88, 99\n\nWhat is the reason for missing data? This can be important for inferential questions!\n\n\n\n\nFor numerical computations these must be filtered out or coded as NA!\n\ness |> select(euftf) |> \n mutate(euftf_na = euftf  |> na_if(77) |> na_if(88) |> na_if(99)) |> \n summarize(across(.fns = function(x) mean(x, na.rm = TRUE)))\n\n# A tibble: 1 × 2\n  euftf euftf_na\n  <dbl>    <dbl>\n1  13.7     5.20"
  },
  {
    "objectID": "W8.html#emotional-attachment",
    "href": "W8.html#emotional-attachment",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Emotional attachment",
    "text": "Emotional attachment\nQuestion: What is the relation of the emotional attachment of Europeans to their own country and to Europe?\n\n\ness |> \n filter(essround == 9) |> \n count(atchctr, atcherp) |> \n na.omit() |> \n ggplot(aes(atchctr, atcherp, \n            size = n, color = n)) + \n geom_point() +\n geom_smooth(aes(weight = n), \n             method = 'loess', \n             formula = 'y ~ x') +\n scale_color_continuous(type = \"viridis\") +\n scale_size_area(max_size = 10) + ylim(c(0,10)) +\n coord_fixed() + \n guides(size = \"none\") +\n theme_classic(base_size = 24)\n\n\n\n\n\n\n\n\nDifferences between lowess used in seaborn.regplot and loess, the default in ggplot::geom_smooth: https://stats.stackexchange.com/questions/161069/difference-between-loess-and-lowess"
  },
  {
    "objectID": "W8.html#emotional-attachment-eu-integration",
    "href": "W8.html#emotional-attachment-eu-integration",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Emotional attachment EU integration",
    "text": "Emotional attachment EU integration\nQuestion: What is the relation of the emotional attachment to the own country to attachment to Europe compared to the attitude about European integration?\n\n\nEmotional attachment to the own country and Europe is positively related. (“Positive” here means the sign of the correlation. It does not mean “good”!)\nNo compensation like “Emotion must be split between both.”\nRelation of country attachment to EU integration is weak but non-linear."
  },
  {
    "objectID": "W8.html#weighting-after-count",
    "href": "W8.html#weighting-after-count",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Weighting after count",
    "text": "Weighting after count\nHow many rows does the the data frame ess |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() have?\n\n11 times 11 = 121 (when each combination has a non-zero number of cases)\nHow many observations (not NA) do we have in the data set?\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> summarize(n = sum(n))\n\n# A tibble: 1 × 1\n      n\n  <int>\n1 45552\n\n\nWeighting points correctly is important!"
  },
  {
    "objectID": "W8.html#three-different-smooth-plots",
    "href": "W8.html#three-different-smooth-plots",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Three different smooth plots",
    "text": "Three different smooth plots\n\nA: Counts unweightedB: Counts weightedC: Individual cases\n\n\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> \n ggplot(aes(atchctr, euftf)) + geom_smooth() \n\n\n\n\nMakes no sense because, just 121 points in a square.\n\n\n\ness |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> \n ggplot(aes(atchctr, euftf, weight = n)) + geom_smooth() \n\n\n\n\nMakes sense weighted by the counts.\n\n\n\ness |> filter(essround == 9) |> \n ggplot(aes(atchctr, euftf)) + geom_smooth()\n\n\n\n\nSimilar to B. Lower uncertainty! Different y-axis limits!\nNote, geom_smooth uses stats::loess for less than 1,000 cases (as before), otherwise (as here) mgcv::gam() because it is more efficient computationally. We omit details here."
  },
  {
    "objectID": "W8.html#significance-of-nonlinear-relation",
    "href": "W8.html#significance-of-nonlinear-relation",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Significance of nonlinear relation?",
    "text": "Significance of nonlinear relation?\n\n\nTaking into account the real number of cases the uncertainty range indicates that the non-linear relationship is fairly certain, although small in magnitude.\nNote, we have not looked at uncertainty measures in detail yet.\nMain message here: Low uncertainty and large effect size is not the same!\n\nIn statistics the first is called significant.\nIn common language the second is often called significant.\n\nThis dual use of significant is a source of confusion in science communication!"
  },
  {
    "objectID": "W8.html#linear-models",
    "href": "W8.html#linear-models",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear models",
    "text": "Linear models\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n fit(atcherp ~ atchctr, data = ess) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    2.62    0.0411       63.8       0\n2 atchctr        0.414   0.00504      82.1       0\n\n\nInterpretation?\n\nWhen atchctr = 0 the average atcherp is 2.62. For an increase of country attachment by one there is an average increase of 0.414 in European attachment.\n\n\nFor EU integration attitude.\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(euftf ~ atchctr, data = ess) |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)   5.00     0.0481     104.   0           \n2 atchctr       0.0333   0.00589      5.65 0.0000000157"
  },
  {
    "objectID": "W8.html#r-squared-of-a-fitted-model",
    "href": "W8.html#r-squared-of-a-fitted-model",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "R-squared of a fitted model",
    "text": "R-squared of a fitted model\n\\(R^2\\) is the percentage of variability in the response explained by the regression model.\nR-squared is also called coefficient of determination.\nDefinition:\n\\(R^2 = 1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}\\)\nwhere \\(SS_\\text{res} = \\sum_i(y_i - f_i)^2 = \\sum_i e_i^2\\) is the sum of the squared residuals, and\n\\(SS_\\text{tot} = \\sum_i(y_i - \\bar y)^2\\) the total sum of squares which is proportional to the variance of \\(y\\). (\\(\\bar y\\) is the mean of \\(y\\).)\n\\(R^2\\) is the square of the correlation coefficient, hence the name. (No math on this today.)"
  },
  {
    "objectID": "W8.html#linear-models-r-squared",
    "href": "W8.html#linear-models-r-squared",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear models R-squared",
    "text": "Linear models R-squared\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n fit(atcherp ~ atchctr, data = ess) |>\n glance()  # glance shows summary statistics of model fit\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³\n      <dbl>      <dbl> <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n1     0.122      0.122  2.45   6747.       0     1 -1.12e5 2.25e5 2.25e5 291696.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nInterpretation R-square?\n\n12.2% of the variance of European emotional attachment can be explained by a linear relation with country emotional attachment.\n\n\nFor EU integration attitude.\n\nlinear_reg() |> set_engine(\"lm\") |> \n fit(euftf ~ atchctr, data = ess) |> \n glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³\n      <dbl>      <dbl> <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n1  0.000702   0.000680  2.75    32.0 1.57e-8     1 -1.11e5 2.21e5 2.21e5 343473.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance"
  },
  {
    "objectID": "W8.html#linear-model-with-more-predictors",
    "href": "W8.html#linear-model-with-more-predictors",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear model with more predictors",
    "text": "Linear model with more predictors\n\nlibrary(tidymodels)\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(euftf ~ atchctr + atcherp, data = ess) |> tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    4.05    0.0478       84.8 0       \n2 atchctr       -0.114   0.00599     -19.1 1.12e-80\n3 atcherp        0.354   0.00509      69.7 0       \n\n\n\nNote, that atchctr now has a negative coefficient!\nThe tiny bit of positive relation explained by atchctr in a one predictor model can better be explained by atcherp (which we know is correlated with atchctr)."
  },
  {
    "objectID": "W8.html#corona-deaths-vs.-human-development",
    "href": "W8.html#corona-deaths-vs.-human-development",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Corona deaths vs. Human development",
    "text": "Corona deaths vs. Human development\n\nNote: Not weighted by population!"
  },
  {
    "objectID": "W8.html#linear-model-total-deaths-vs.-hdi",
    "href": "W8.html#linear-model-total-deaths-vs.-hdi",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Linear model: Total deaths vs. HDI",
    "text": "Linear model: Total deaths vs. HDI\n\nowid_aug22 <- owid |> \n filter(date == \"2022-08-31\", !is.na(human_development_index), \n        !is.na(total_deaths_per_million), !is.na(continent))\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index, data = owid_aug22) |> tidy()\n\n# A tibble: 2 × 5\n  term                    estimate std.error statistic  p.value\n  <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)               -2350.      385.     -6.11 5.84e- 9\n2 human_development_index    4917.      522.      9.43 1.71e-17"
  },
  {
    "objectID": "W8.html#adding-a-main-effect-of-continents",
    "href": "W8.html#adding-a-main-effect-of-continents",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Adding a main effect of continents",
    "text": "Adding a main effect of continents\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index + continent, \n        data = owid_aug22) |> tidy()\n\n# A tibble: 7 × 5\n  term                    estimate std.error statistic       p.value\n  <chr>                      <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)               -754.       392.    -1.92  0.0560       \n2 human_development_index   1901.       666.     2.86  0.00480      \n3 continentAsia               58.5      213.     0.274 0.784        \n4 continentEurope           1685.       279.     6.04  0.00000000867\n5 continentNorth America     742.       254.     2.92  0.00397      \n6 continentOceania          -312.       298.    -1.05  0.297        \n7 continentSouth America    1910.       311.     6.15  0.00000000489\n\n\nA main effect by categorical dummy variables allows for different intercepts per continent."
  },
  {
    "objectID": "W8.html#adding-as-interaction",
    "href": "W8.html#adding-as-interaction",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Adding as interaction",
    "text": "Adding as interaction\n\nlinear_reg() |> set_engine(\"lm\") |> \n    fit(total_deaths_per_million ~ human_development_index * continent, \n        data = owid_aug22) |> tidy()\n\n# A tibble: 12 × 5\n   term                                         estim…¹ std.e…² stati…³  p.value\n   <chr>                                          <dbl>   <dbl>   <dbl>    <dbl>\n 1 (Intercept)                                   -1555.    581.  -2.67  8.19e- 3\n 2 human_development_index                        3329.   1019.   3.27  1.31e- 3\n 3 continentAsia                                   563.    940.   0.598 5.50e- 1\n 4 continentEurope                               14759.   1956.   7.55  2.32e-12\n 5 continentNorth America                        -1346.   1512.  -0.890 3.75e- 1\n 6 continentOceania                               1125.   1424.   0.790 4.31e- 1\n 7 continentSouth America                        -4243.   3436.  -1.23  2.19e- 1\n 8 human_development_index:continentAsia         -1027.   1419.  -0.724 4.70e- 1\n 9 human_development_index:continentEurope      -15377.   2351.  -6.54  6.40e-10\n10 human_development_index:continentNorth Amer…   2394.   2099.   1.14  2.56e- 1\n11 human_development_index:continentOceania      -2318.   2063.  -1.12  2.63e- 1\n12 human_development_index:continentSouth Amer…   7684.   4543.   1.69  9.25e- 2\n# … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic\n\n\n\nNote the * for interaction effect!\nAlso main effects for both variables are in as coefficients.\nAfrica has been chosen as reference category (because it is first in the alphabet).\nAn interaction effect allows for different slopes for each continent!"
  },
  {
    "objectID": "W8.html#regression-lines-by-continent",
    "href": "W8.html#regression-lines-by-continent",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Regression lines by continent",
    "text": "Regression lines by continent\n\nThe relation between deaths and human development is reverse in Europe."
  },
  {
    "objectID": "W8.html#simpsons-paradox",
    "href": "W8.html#simpsons-paradox",
    "title": "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nSlopes for all groups can be in the opposite direction of the main effect’s slope!\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W5.html#named-vectors",
    "href": "W5.html#named-vectors",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Named vectors",
    "text": "Named vectors\nAll types of vectors can be named upon creation\n\nc(Num1 = 4, Second = 7, Last = 8)\n\n\n\n  Num1 Second   Last \n     4      7      8 \n\n\n\nor names can be set afterward.\n\nx <- 1:4\ny <- set_names(x, c(\"a\",\"b\",\"c\",\"d\"))\ny\n\n\n\na b c d \n1 2 3 4 \n\n\n\n\nNamed vectors can be used for subsetting.\n\ny[c(\"b\",\"d\")]\n\n\n\nb d \n2 4"
  },
  {
    "objectID": "W5.html#reminder-indexing-and-vectorized-thinking",
    "href": "W5.html#reminder-indexing-and-vectorized-thinking",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Reminder: Indexing and vectorized thinking",
    "text": "Reminder: Indexing and vectorized thinking\n\nx <- set_names(1:10,LETTERS[1:10])\nx\n\n\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\n\n\n\nx[c(4,2,1,1,1,1,4,1,5)]\n\n\n\nD B A A A A D A E \n4 2 1 1 1 1 4 1 5 \n\n\n\n\nRemoving with negative index numbers.\n\nx[c(-3,-5,-2)]\n\n\n\n A  D  F  G  H  I  J \n 1  4  6  7  8  9 10 \n\n\n\n\nMixing does not work.\nx[c(-3,1)]  # Will throw an error"
  },
  {
    "objectID": "W5.html#r-objects-can-have-attributes",
    "href": "W5.html#r-objects-can-have-attributes",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "R objects can have attributes",
    "text": "R objects can have attributes\nIn a named vector, the names are an attribute.\n\nx\n\n A  B  C  D  E  F  G  H  I  J \n 1  2  3  4  5  6  7  8  9 10 \n\nattributes(x)\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n\n\nAttributes can be assigned freely.\n\nattr(x, \"SayHi\") <- \"Hi\"\nattr(x, \"SayBye\") <- \"Bye\"\nattributes(x)\n\n\n\n$names\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\"\n\n$SayHi\n[1] \"Hi\"\n\n$SayBye\n[1] \"Bye\""
  },
  {
    "objectID": "W5.html#attributes-in-data-structures",
    "href": "W5.html#attributes-in-data-structures",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Attributes in data structures",
    "text": "Attributes in data structures\n\nlibrary(nycflights13)\nattributes(airports)\n\n\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n   [1]    1    2    3    4    5    6    7    8    9   10   11   12   13   14\n  [15]   15   16   17   18   19   20   21   22   23   24   25   26   27   28\n  [29]   29   30   31   32   33   34   35   36   37   38   39   40   41   42\n  [43]   43   44   45   46   47   48   49   50   51   52   53   54   55   56\n  [57]   57   58   59   60   61   62   63   64   65   66   67   68   69   70\n  [71]   71   72   73   74   75   76   77   78   79   80   81   82   83   84\n  [85]   85   86   87   88   89   90   91   92   93   94   95   96   97   98\n  [99]   99  100  101  102  103  104  105  106  107  108  109  110  111  112\n [113]  113  114  115  116  117  118  119  120  121  122  123  124  125  126\n [127]  127  128  129  130  131  132  133  134  135  136  137  138  139  140\n [141]  141  142  143  144  145  146  147  148  149  150  151  152  153  154\n [155]  155  156  157  158  159  160  161  162  163  164  165  166  167  168\n [169]  169  170  171  172  173  174  175  176  177  178  179  180  181  182\n [183]  183  184  185  186  187  188  189  190  191  192  193  194  195  196\n [197]  197  198  199  200  201  202  203  204  205  206  207  208  209  210\n [211]  211  212  213  214  215  216  217  218  219  220  221  222  223  224\n [225]  225  226  227  228  229  230  231  232  233  234  235  236  237  238\n [239]  239  240  241  242  243  244  245  246  247  248  249  250  251  252\n [253]  253  254  255  256  257  258  259  260  261  262  263  264  265  266\n [267]  267  268  269  270  271  272  273  274  275  276  277  278  279  280\n [281]  281  282  283  284  285  286  287  288  289  290  291  292  293  294\n [295]  295  296  297  298  299  300  301  302  303  304  305  306  307  308\n [309]  309  310  311  312  313  314  315  316  317  318  319  320  321  322\n [323]  323  324  325  326  327  328  329  330  331  332  333  334  335  336\n [337]  337  338  339  340  341  342  343  344  345  346  347  348  349  350\n [351]  351  352  353  354  355  356  357  358  359  360  361  362  363  364\n [365]  365  366  367  368  369  370  371  372  373  374  375  376  377  378\n [379]  379  380  381  382  383  384  385  386  387  388  389  390  391  392\n [393]  393  394  395  396  397  398  399  400  401  402  403  404  405  406\n [407]  407  408  409  410  411  412  413  414  415  416  417  418  419  420\n [421]  421  422  423  424  425  426  427  428  429  430  431  432  433  434\n [435]  435  436  437  438  439  440  441  442  443  444  445  446  447  448\n [449]  449  450  451  452  453  454  455  456  457  458  459  460  461  462\n [463]  463  464  465  466  467  468  469  470  471  472  473  474  475  476\n [477]  477  478  479  480  481  482  483  484  485  486  487  488  489  490\n [491]  491  492  493  494  495  496  497  498  499  500  501  502  503  504\n [505]  505  506  507  508  509  510  511  512  513  514  515  516  517  518\n [519]  519  520  521  522  523  524  525  526  527  528  529  530  531  532\n [533]  533  534  535  536  537  538  539  540  541  542  543  544  545  546\n [547]  547  548  549  550  551  552  553  554  555  556  557  558  559  560\n [561]  561  562  563  564  565  566  567  568  569  570  571  572  573  574\n [575]  575  576  577  578  579  580  581  582  583  584  585  586  587  588\n [589]  589  590  591  592  593  594  595  596  597  598  599  600  601  602\n [603]  603  604  605  606  607  608  609  610  611  612  613  614  615  616\n [617]  617  618  619  620  621  622  623  624  625  626  627  628  629  630\n [631]  631  632  633  634  635  636  637  638  639  640  641  642  643  644\n [645]  645  646  647  648  649  650  651  652  653  654  655  656  657  658\n [659]  659  660  661  662  663  664  665  666  667  668  669  670  671  672\n [673]  673  674  675  676  677  678  679  680  681  682  683  684  685  686\n [687]  687  688  689  690  691  692  693  694  695  696  697  698  699  700\n [701]  701  702  703  704  705  706  707  708  709  710  711  712  713  714\n [715]  715  716  717  718  719  720  721  722  723  724  725  726  727  728\n [729]  729  730  731  732  733  734  735  736  737  738  739  740  741  742\n [743]  743  744  745  746  747  748  749  750  751  752  753  754  755  756\n [757]  757  758  759  760  761  762  763  764  765  766  767  768  769  770\n [771]  771  772  773  774  775  776  777  778  779  780  781  782  783  784\n [785]  785  786  787  788  789  790  791  792  793  794  795  796  797  798\n [799]  799  800  801  802  803  804  805  806  807  808  809  810  811  812\n [813]  813  814  815  816  817  818  819  820  821  822  823  824  825  826\n [827]  827  828  829  830  831  832  833  834  835  836  837  838  839  840\n [841]  841  842  843  844  845  846  847  848  849  850  851  852  853  854\n [855]  855  856  857  858  859  860  861  862  863  864  865  866  867  868\n [869]  869  870  871  872  873  874  875  876  877  878  879  880  881  882\n [883]  883  884  885  886  887  888  889  890  891  892  893  894  895  896\n [897]  897  898  899  900  901  902  903  904  905  906  907  908  909  910\n [911]  911  912  913  914  915  916  917  918  919  920  921  922  923  924\n [925]  925  926  927  928  929  930  931  932  933  934  935  936  937  938\n [939]  939  940  941  942  943  944  945  946  947  948  949  950  951  952\n [953]  953  954  955  956  957  958  959  960  961  962  963  964  965  966\n [967]  967  968  969  970  971  972  973  974  975  976  977  978  979  980\n [981]  981  982  983  984  985  986  987  988  989  990  991  992  993  994\n [995]  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008\n[1009] 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022\n[1023] 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036\n[1037] 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050\n[1051] 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064\n[1065] 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078\n[1079] 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092\n[1093] 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106\n[1107] 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120\n[1121] 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134\n[1135] 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148\n[1149] 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162\n[1163] 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176\n[1177] 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190\n[1191] 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204\n[1205] 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218\n[1219] 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232\n[1233] 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246\n[1247] 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260\n[1261] 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274\n[1275] 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288\n[1289] 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302\n[1303] 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316\n[1317] 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330\n[1331] 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344\n[1345] 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358\n[1359] 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372\n[1373] 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386\n[1387] 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400\n[1401] 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414\n[1415] 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428\n[1429] 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442\n[1443] 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456\n[1457] 1457 1458\n\n$spec\ncols(\n  id = col_double(),\n  name = col_character(),\n  city = col_character(),\n  country = col_character(),\n  faa = col_character(),\n  icao = col_character(),\n  lat = col_double(),\n  lon = col_double(),\n  alt = col_double(),\n  tz = col_double(),\n  dst = col_character(),\n  tzone = col_character()\n)\n\n$names\n[1] \"faa\"   \"name\"  \"lat\"   \"lon\"   \"alt\"   \"tz\"    \"dst\"   \"tzone\""
  },
  {
    "objectID": "W5.html#three-important-attributes",
    "href": "W5.html#three-important-attributes",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Three important attributes",
    "text": "Three important attributes\n\nNames are used to name element of a vector (counting also lists as vectors and therefore also data frames as lists of atomic vectors of the same length)\nDimensions (dim()) is a short numeric vector making a vector behave as a matrix or a higher dimensional array. A vector 1:6 together with dim being c(2,3) is a matrix with 2 rows and 3 columns\n\\(\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}\\)\nClass is used to implement the S3 object oriented system. We don’t need to know the details here. The class system makes it for example possible that the same function, e.g. print() behaves differently for objects of different a different class.\n\nClass plays a role in specifying augmented vectors like factors, dates, date-times, or tibbles."
  },
  {
    "objectID": "W5.html#factors",
    "href": "W5.html#factors",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Factors",
    "text": "Factors\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values\n\nx <- factor(c(\"BS\", \"MS\", \"PhD\", \"MS\", \"BS\", \"BS\"))\nx\n\n\n\n[1] BS  MS  PhD MS  BS  BS \nLevels: BS MS PhD\n\n\n\nTechnically, a factor is vector of integers with a levels attribute which specifies the categories for the integers.\n\ntypeof(x)\n\n[1] \"integer\"\n\nas.integer(x)\n\n[1] 1 2 3 2 1 1\n\nattributes(x)\n\n$levels\n[1] \"BS\"  \"MS\"  \"PhD\"\n\n$class\n[1] \"factor\"\n\n\n\n\nThe class factor makes R print the level of each element of the vector instead of the underlying integer."
  },
  {
    "objectID": "W5.html#factors-for-data-visualization",
    "href": "W5.html#factors-for-data-visualization",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Factors for data visualization",
    "text": "Factors for data visualization\nWe manipulate factors with functions from the forcats package of the tidyverse core.\n\nPlotReverseOrder by frequencyRegroup\n\n\n\nmpg |> ggplot(aes(y = manufacturer)) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(manufacturer))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_rev(fct_infreq(manufacturer)))) + geom_bar()\n\n\n\n\n\n\n\nmpg |> ggplot(aes(y = fct_other(manufacturer, keep = c(\"dodge\", \"toyota\", \"volkswagen\")))) + geom_bar()"
  },
  {
    "objectID": "W5.html#dates",
    "href": "W5.html#dates",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Dates",
    "text": "Dates\n\nISO 8601 standard for dates: YYYY-MM-DD. Today: 2022-10-25.\nDates in R are numeric vectors that represent the number of days since 1 January 1970.\n\n\ny <- as.Date(\"2020-01-01\"); y\n\n[1] \"2020-01-01\"\n\ntypeof(y)\n\n[1] \"double\"\n\nattributes(y)\n\n$class\n[1] \"Date\"\n\nas.double(y)\n\n[1] 18262\n\nas.double(as.Date(\"1970-01-01\"))\n\n[1] 0\n\nas.double(as.Date(\"1969-01-01\"))\n\n[1] -365"
  },
  {
    "objectID": "W5.html#how-many-days-are-you-old",
    "href": "W5.html#how-many-days-are-you-old",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "How many days are you old?",
    "text": "How many days are you old?\n\n\nSys.Date() - as.Date(\"1976-01-16\")  # Sys.Date() gives as the current day your computer is set to\n\nTime difference of 17084 days"
  },
  {
    "objectID": "W5.html#date-times",
    "href": "W5.html#date-times",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Date-times",
    "text": "Date-times\nFor date-time manipulation use lubridate form the tidyverse. Not in the core so it has to be loaded.1\n\nx <- lubridate::ymd_hm(\"1970-01-01 01:00\")\nx\n\n[1] \"1970-01-01 01:00:00 UTC\"\n\nattributes(x)\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"UTC\"\n\nas.double(x)\n\n[1] 3600\n\n\nUTC: Coordinated Universal Time. We are in the UTC+1 timezone.\nPOSIXct: Portable Operating System Interface, calendar time. Stores date and time in seconds with the number of seconds beginning at 1 January 1970.\nInstead of loading package pack to use its function func you can also write pack::func all the time. This works when the package is installed even when not loaded."
  },
  {
    "objectID": "W5.html#how-many-seconds-are-you-old",
    "href": "W5.html#how-many-seconds-are-you-old",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "How many seconds are you old?",
    "text": "How many seconds are you old?\n\nas.double(lubridate::now()) - as.double(lubridate::ymd_hm(\"1976-01-16_12:04\"))\n\n[1] 1476061826"
  },
  {
    "objectID": "W5.html#more-about",
    "href": "W5.html#more-about",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "More about …",
    "text": "More about …\n\nFactors: R for Data Science Chapter 15\nDates and times: R for Data Science Chapter 16"
  },
  {
    "objectID": "W5.html#string-modification",
    "href": "W5.html#string-modification",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "String modification",
    "text": "String modification\nWe modify strings with the stringr package from the tidyverse core.\nAll functions from stringr start with str_.\nVery few examples:\n\nc(\"x\",\"y\")\n\n[1] \"x\" \"y\"\n\nstr_c(\"x\",\"y\")\n\n[1] \"xy\"\n\nstr_c(\"x\",\"y\",\"z\", sep=\",\")\n\n[1] \"x,y,z\"\n\nlength(c(\"x\",\"y\",\"z\"))\n\n[1] 3\n\nstr_length(c(\"x\",\"y\",\"z\"))\n\n[1] 1 1 1\n\nstr_length(c(\"This is a string.\",\"z\"))\n\n[1] 17  1"
  },
  {
    "objectID": "W5.html#string-wrangling-with-variable-names",
    "href": "W5.html#string-wrangling-with-variable-names",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "String wrangling with variable names",
    "text": "String wrangling with variable names\n\ndata <- tibble(Name = c(\"A\",\"B\",\"C\"), Age_2020 = c(20,30,40), Age_2021 = c(21,31,41), Age_2022 = c(22,32,42))\ndata\n\n# A tibble: 3 × 4\n  Name  Age_2020 Age_2021 Age_2022\n  <chr>    <dbl>    <dbl>    <dbl>\n1 A           20       21       22\n2 B           30       31       32\n3 C           40       41       42\n\n\nWe tidy that data set by creating a year variable.\n\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\")\n\n\n\n# A tibble: 9 × 3\n  Name  Year       Age\n  <chr> <chr>    <dbl>\n1 A     Age_2020    20\n2 A     Age_2021    21\n3 A     Age_2022    22\n4 B     Age_2020    30\n5 B     Age_2021    31\n6 B     Age_2022    32\n7 C     Age_2020    40\n8 C     Age_2021    41\n9 C     Age_2022    42\n\n\n\n\nOK, but the year variable is a string but we want numbers."
  },
  {
    "objectID": "W5.html#use-word",
    "href": "W5.html#use-word",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Use word",
    "text": "Use word\nword extracts words from a sentence. However, the separator need not be \" \" but can be any character.\n\nword(\"This is a string.\", start=2, end=-2) \n\n[1] \"is a\"\n\n#Selects from the second to the second last word.\nword(\"Age_2022\", start=2, sep = \"_\")\n\n[1] \"2022\"\n\n\n\nIt also works vectorized.\n\ndata |> pivot_longer(c(\"Age_2020\", \"Age_2021\", \"Age_2022\"), names_to = \"Year\", values_to=\"Age\") |> \n  mutate(Year = word(Year, start = 2, sep = \"_\") |> as.numeric())\n\n\n\n# A tibble: 9 × 3\n  Name   Year   Age\n  <chr> <dbl> <dbl>\n1 A      2020    20\n2 A      2021    21\n3 A      2022    22\n4 B      2020    30\n5 B      2021    31\n6 B      2022    32\n7 C      2020    40\n8 C      2021    41\n9 C      2022    42\n\n\n… More on strings and regular expressions: R for Data Science Chapter 14"
  },
  {
    "objectID": "W5.html#working-with-more-data-frames",
    "href": "W5.html#working-with-more-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Working with more data frames",
    "text": "Working with more data frames\n\nData can be distributed in several data frames which have relations which each other.\nFor example, they share variables as the five data frames in nycflights13.\n\n\n\n\nOften variables in different data frame have the same name, but that need not be the case! See the variable faa in airports matches origin and dest in flights."
  },
  {
    "objectID": "W5.html#data-women-in-science",
    "href": "W5.html#data-women-in-science",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Data: Women in science",
    "text": "Data: Women in science\n10 women in science who changed the world: Ada Lovelace, Marie Curie, Janaki Ammal, Chien-Shiung Wu, Katherine Johnson, Rosalind Franklin, Vera Rubin, Gladys West, Flossie Wong-Staal, Jennifer Doudna\n\n\n\n\nProfessionsDatesWorks\n\n\n\nprofessions <- read_csv(\"data/scientists/professions.csv\")\nprofessions\n\n# A tibble: 10 × 2\n   name               profession                        \n   <chr>              <chr>                             \n 1 Ada Lovelace       Mathematician                     \n 2 Marie Curie        Physicist and Chemist             \n 3 Janaki Ammal       Botanist                          \n 4 Chien-Shiung Wu    Physicist                         \n 5 Katherine Johnson  Mathematician                     \n 6 Rosalind Franklin  Chemist                           \n 7 Vera Rubin         Astronomer                        \n 8 Gladys West        Mathematician                     \n 9 Flossie Wong-Staal Virologist and Molecular Biologist\n10 Jennifer Doudna    Biochemist                        \n\n\n\n\n\ndates <- read_csv(\"data/scientists/dates.csv\")\ndates\n\n# A tibble: 8 × 3\n  name               birth_year death_year\n  <chr>                   <dbl>      <dbl>\n1 Janaki Ammal             1897       1984\n2 Chien-Shiung Wu          1912       1997\n3 Katherine Johnson        1918       2020\n4 Rosalind Franklin        1920       1958\n5 Vera Rubin               1928       2016\n6 Gladys West              1930         NA\n7 Flossie Wong-Staal       1947         NA\n8 Jennifer Doudna          1964         NA\n\n\n\n\n\nworks <- read_csv(\"data/scientists/works.csv\")\nworks\n\n# A tibble: 9 × 2\n  name               known_for                                                  \n  <chr>              <chr>                                                      \n1 Ada Lovelace       first computer algorithm                                   \n2 Marie Curie        theory of radioactivity,  discovery of elements polonium a…\n3 Janaki Ammal       hybrid species, biodiversity protection                    \n4 Chien-Shiung Wu    confim and refine theory of radioactive beta decy, Wu expe…\n5 Katherine Johnson  calculations of orbital mechanics critical to sending the …\n6 Vera Rubin         existence of dark matter                                   \n7 Gladys West        mathematical modeling of the shape of the Earth which serv…\n8 Flossie Wong-Staal first scientist to clone HIV and create a map of its genes…\n9 Jennifer Doudna    one of the primary developers of CRISPR, a ground-breaking…\n\n\n\n\n\n\n\nSource: Discover Magazine\nThe data can be downloaded: professions.csv, dates.csv, works.csv"
  },
  {
    "objectID": "W5.html#we-want-this-data-frame",
    "href": "W5.html#we-want-this-data-frame",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "We want this data frame",
    "text": "We want this data frame\n\n\n# A tibble: 10 × 5\n   name               profession                         birth…¹ death…² known…³\n   <chr>              <chr>                                <dbl>   <dbl> <chr>  \n 1 Ada Lovelace       Mathematician                           NA      NA first …\n 2 Marie Curie        Physicist and Chemist                   NA      NA theory…\n 3 Janaki Ammal       Botanist                              1897    1984 hybrid…\n 4 Chien-Shiung Wu    Physicist                             1912    1997 confim…\n 5 Katherine Johnson  Mathematician                         1918    2020 calcul…\n 6 Rosalind Franklin  Chemist                               1920    1958 <NA>   \n 7 Vera Rubin         Astronomer                            1928    2016 existe…\n 8 Gladys West        Mathematician                         1930      NA mathem…\n 9 Flossie Wong-Staal Virologist and Molecular Biologist    1947      NA first …\n10 Jennifer Doudna    Biochemist                            1964      NA one of…\n# … with abbreviated variable names ¹​birth_year, ²​death_year, ³​known_for"
  },
  {
    "objectID": "W5.html#joining-data-frames",
    "href": "W5.html#joining-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Joining data frames",
    "text": "Joining data frames\nsomething_join(x, y)^{The notion join comes from SQL database. In other data manipulation frameworks joining is called merging.} for data frames x and y which have a relation\n\nleft_join(): all rows from x\nright_join(): all rows from y\nfull_join(): all rows from both x and y\ninner_join(): all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches\n…"
  },
  {
    "objectID": "W5.html#simple-setup-for-x-and-y",
    "href": "W5.html#simple-setup-for-x-and-y",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Simple setup for x and y",
    "text": "Simple setup for x and y\n\nx <- tibble(\n  id = c(1, 2, 3),\n  value_x = c(\"x1\", \"x2\", \"x3\")\n  )\ny <- tibble(\n  id = c(1, 2, 4),\n  value_y = c(\"y1\", \"y2\", \"y4\")\n  )\nx\n\n# A tibble: 3 × 2\n     id value_x\n  <dbl> <chr>  \n1     1 x1     \n2     2 x2     \n3     3 x3     \n\ny\n\n# A tibble: 3 × 2\n     id value_y\n  <dbl> <chr>  \n1     1 y1     \n2     2 y2     \n3     4 y4"
  },
  {
    "objectID": "W5.html#left_join",
    "href": "W5.html#left_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "left_join()",
    "text": "left_join()\n\n\n\n\n\nleft_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>"
  },
  {
    "objectID": "W5.html#right_join",
    "href": "W5.html#right_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "right_join()",
    "text": "right_join()\n\n\n\n\n\nright_join(x, y)\n\n# A tibble: 3 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     4 <NA>    y4"
  },
  {
    "objectID": "W5.html#full_join",
    "href": "W5.html#full_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "full_join()",
    "text": "full_join()\n\n\n\n\n\nfull_join(x, y)\n\n# A tibble: 4 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2     \n3     3 x3      <NA>   \n4     4 <NA>    y4"
  },
  {
    "objectID": "W5.html#inner_join",
    "href": "W5.html#inner_join",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "inner_join()",
    "text": "inner_join()\n\n\n\n\n\ninner_join(x, y)\n\n# A tibble: 2 × 3\n     id value_x value_y\n  <dbl> <chr>   <chr>  \n1     1 x1      y1     \n2     2 x2      y2"
  },
  {
    "objectID": "W5.html#women-in-science",
    "href": "W5.html#women-in-science",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Women in science",
    "text": "Women in science\n\nleft_joinright_joinfull_joininner_joinFinal\n\n\n\nprofessions |> left_join(works)\n\n# A tibble: 10 × 3\n   name               profession                         known_for              \n   <chr>              <chr>                              <chr>                  \n 1 Ada Lovelace       Mathematician                      first computer algorit…\n 2 Marie Curie        Physicist and Chemist              theory of radioactivit…\n 3 Janaki Ammal       Botanist                           hybrid species, biodiv…\n 4 Chien-Shiung Wu    Physicist                          confim and refine theo…\n 5 Katherine Johnson  Mathematician                      calculations of orbita…\n 6 Rosalind Franklin  Chemist                            <NA>                   \n 7 Vera Rubin         Astronomer                         existence of dark matt…\n 8 Gladys West        Mathematician                      mathematical modeling …\n 9 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clo…\n10 Jennifer Doudna    Biochemist                         one of the primary dev…\n\n\n\n\n\nprofessions |> right_join(works)\n\n# A tibble: 9 × 3\n  name               profession                         known_for               \n  <chr>              <chr>                              <chr>                   \n1 Ada Lovelace       Mathematician                      first computer algorithm\n2 Marie Curie        Physicist and Chemist              theory of radioactivity…\n3 Janaki Ammal       Botanist                           hybrid species, biodive…\n4 Chien-Shiung Wu    Physicist                          confim and refine theor…\n5 Katherine Johnson  Mathematician                      calculations of orbital…\n6 Vera Rubin         Astronomer                         existence of dark matter\n7 Gladys West        Mathematician                      mathematical modeling o…\n8 Flossie Wong-Staal Virologist and Molecular Biologist first scientist to clon…\n9 Jennifer Doudna    Biochemist                         one of the primary deve…\n\n\n\n\n\ndates |> full_join(works)\n\n# A tibble: 10 × 4\n   name               birth_year death_year known_for                           \n   <chr>                   <dbl>      <dbl> <chr>                               \n 1 Janaki Ammal             1897       1984 hybrid species, biodiversity protec…\n 2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioac…\n 3 Katherine Johnson        1918       2020 calculations of orbital mechanics c…\n 4 Rosalind Franklin        1920       1958 <NA>                                \n 5 Vera Rubin               1928       2016 existence of dark matter            \n 6 Gladys West              1930         NA mathematical modeling of the shape …\n 7 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cr…\n 8 Jennifer Doudna          1964         NA one of the primary developers of CR…\n 9 Ada Lovelace               NA         NA first computer algorithm            \n10 Marie Curie                NA         NA theory of radioactivity,  discovery…\n\n\n\n\n\ndates |> inner_join(works)\n\n# A tibble: 7 × 4\n  name               birth_year death_year known_for                            \n  <chr>                   <dbl>      <dbl> <chr>                                \n1 Janaki Ammal             1897       1984 hybrid species, biodiversity protect…\n2 Chien-Shiung Wu          1912       1997 confim and refine theory of radioact…\n3 Katherine Johnson        1918       2020 calculations of orbital mechanics cr…\n4 Vera Rubin               1928       2016 existence of dark matter             \n5 Gladys West              1930         NA mathematical modeling of the shape o…\n6 Flossie Wong-Staal       1947         NA first scientist to clone HIV and cre…\n7 Jennifer Doudna          1964         NA one of the primary developers of CRI…\n\n\n\n\n\nprofessions |> left_join(dates) |> left_join(works)\n\n# A tibble: 10 × 5\n   name               profession                         birth…¹ death…² known…³\n   <chr>              <chr>                                <dbl>   <dbl> <chr>  \n 1 Ada Lovelace       Mathematician                           NA      NA first …\n 2 Marie Curie        Physicist and Chemist                   NA      NA theory…\n 3 Janaki Ammal       Botanist                              1897    1984 hybrid…\n 4 Chien-Shiung Wu    Physicist                             1912    1997 confim…\n 5 Katherine Johnson  Mathematician                         1918    2020 calcul…\n 6 Rosalind Franklin  Chemist                               1920    1958 <NA>   \n 7 Vera Rubin         Astronomer                            1928    2016 existe…\n 8 Gladys West        Mathematician                         1930      NA mathem…\n 9 Flossie Wong-Staal Virologist and Molecular Biologist    1947      NA first …\n10 Jennifer Doudna    Biochemist                            1964      NA one of…\n# … with abbreviated variable names ¹​birth_year, ²​death_year, ³​known_for"
  },
  {
    "objectID": "W5.html#keys",
    "href": "W5.html#keys",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Keys",
    "text": "Keys\n\nA key is a variable or a set of variables which uniquely identifies observations\nWhat was the key in the data frame of women in science?\n\n\n\nSwitching back to nycflights13 as example\nIn simple cases, a single variable is sufficient to identify an observation, e.g. each plane in planes is identified by tailnum.\nSometimes, multiple variables are needed; e.g. to identify an observation in weather you need five variables: year, month, day, hour, and origin"
  },
  {
    "objectID": "W5.html#how-can-we-check",
    "href": "W5.html#how-can-we-check",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "How can we check?",
    "text": "How can we check?\nCounting observation and filter those more than one\n\nlibrary(nycflights13)\nplanes |> count(tailnum) |> filter(n > 1)\n\n# A tibble: 0 × 2\n# … with 2 variables: tailnum <chr>, n <int>\n\nweather |> count(year, month, day, hour, origin) |> filter(n > 1) \n\n# A tibble: 3 × 6\n   year month   day  hour origin     n\n  <int> <int> <int> <int> <chr>  <int>\n1  2013    11     3     1 EWR        2\n2  2013    11     3     1 JFK        2\n3  2013    11     3     1 LGA        2\n\n# OK, here 3 observations are twice. Probably a data error.\n# Example: Without hour it is not a key\nweather |> count(year, month, day, origin) |> filter(n > 1) \n\n# A tibble: 1,092 × 5\n    year month   day origin     n\n   <int> <int> <int> <chr>  <int>\n 1  2013     1     1 EWR       22\n 2  2013     1     1 JFK       22\n 3  2013     1     1 LGA       23\n 4  2013     1     2 EWR       24\n 5  2013     1     2 JFK       24\n 6  2013     1     2 LGA       24\n 7  2013     1     3 EWR       24\n 8  2013     1     3 JFK       24\n 9  2013     1     3 LGA       24\n10  2013     1     4 EWR       24\n# … with 1,082 more rows"
  },
  {
    "objectID": "W5.html#primary-and-foreign-keys",
    "href": "W5.html#primary-and-foreign-keys",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Primary and foreign keys",
    "text": "Primary and foreign keys\n\nA primary key uniquely identifies an observation in its own table. E.g, planes$tailnum in planes.\nA foreign key uniquely identifies an observation in another data frame E.g. flights$tailnum is a foreign key in flights because it matches each flight to a unique plane in planes.\nData frames need not have a key and the joins will still do their work.\nA primary key and a foreign key form a relation.\nRelations are typically 1-to-many. Each plane has many flights\nRelations can also be many-to-many. Airlines can fly to many airports; airport can host many airplanes."
  },
  {
    "objectID": "W5.html#joining-when-key-names-differ",
    "href": "W5.html#joining-when-key-names-differ",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Joining when key names differ?",
    "text": "Joining when key names differ?\nWe have to specify the key relation with a named vector in the by argument.\n\ndim(flights)\n\n[1] 336776     19\n\nflights |> left_join(airports, by = c(\"dest\" = \"faa\"))\n\n# A tibble: 336,776 × 26\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 16 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>, lon <dbl>,\n#   alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nWhy does the number of rows stays the same after joining?\n\nfaa is a primary key in airports."
  },
  {
    "objectID": "W5.html#left_join-essentially-right_join-with-switched-data-frames",
    "href": "W5.html#left_join-essentially-right_join-with-switched-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "left_join essentially right_join with switched data frames",
    "text": "left_join essentially right_join with switched data frames\n\nairports_right_flights <- airports |> right_join(flights, by = c(\"faa\" = \"dest\"))\nairports_right_flights \n\n# A tibble: 336,776 × 26\n   faa   name        lat   lon   alt    tz dst   tzone  year month   day dep_t…¹\n   <chr> <chr>     <dbl> <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>   <int>\n 1 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     1    1955\n 2 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     2    2010\n 3 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     3    1955\n 4 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     4    2017\n 5 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     5    1959\n 6 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     6    1959\n 7 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     7    2002\n 8 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     8    1957\n 9 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10     9    1957\n10 ABQ   Albuquer…  35.0 -107.  5355    -7 A     Amer…  2013    10    10    2011\n# … with 336,766 more rows, 14 more variables: sched_dep_time <int>,\n#   dep_delay <dbl>, arr_time <int>, sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, air_time <dbl>,\n#   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated\n#   variable name ¹​dep_time\n\n\nDifferences\n\nIn a join where keys have different column names the name of the first data frame survives (unless you use keep = TRUE). Here, faa instead of dest\nThe columns from the first data frame come first\nThe order of rows is taken from the first data frame, while duplication and dropping of variables is determined by the second data frame (because it is a right_join)\n\nUsing the fact that flights seem to be ordered by year, month, day, dep_time we can re-arrange:\n\nairports_right_flights |> \n  rename(dest = faa) |> \n  select(names(flights)) |> # Use order of flights\n  arrange(year, month, day, dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     924     917       7 UA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nNote of caution: A deeper analysis shows that the order is still not exactly the same."
  },
  {
    "objectID": "W5.html#left_join-with-reversed-data-frames",
    "href": "W5.html#left_join-with-reversed-data-frames",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "left_join with reversed data frames",
    "text": "left_join with reversed data frames\n\ndim(airports)\n\n[1] 1458    8\n\ndim(flights)\n\n[1] 336776     19\n\nairports |> \n  left_join(flights, by = c(\"faa\" = \"dest\"))\n\n# A tibble: 330,531 × 26\n   faa   name       lat    lon   alt    tz dst   tzone  year month   day dep_t…¹\n   <chr> <chr>    <dbl>  <dbl> <dbl> <dbl> <chr> <chr> <int> <int> <int>   <int>\n 1 04G   Lansdow…  41.1  -80.6  1044    -5 A     Amer…    NA    NA    NA      NA\n 2 06A   Moton F…  32.5  -85.7   264    -6 A     Amer…    NA    NA    NA      NA\n 3 06C   Schaumb…  42.0  -88.1   801    -6 A     Amer…    NA    NA    NA      NA\n 4 06N   Randall…  41.4  -74.4   523    -5 A     Amer…    NA    NA    NA      NA\n 5 09J   Jekyll …  31.1  -81.4    11    -5 A     Amer…    NA    NA    NA      NA\n 6 0A9   Elizabe…  36.4  -82.2  1593    -5 A     Amer…    NA    NA    NA      NA\n 7 0G6   William…  41.5  -84.5   730    -5 A     Amer…    NA    NA    NA      NA\n 8 0G7   Finger …  42.9  -76.8   492    -5 A     Amer…    NA    NA    NA      NA\n 9 0P2   Shoestr…  39.8  -76.6  1000    -5 U     Amer…    NA    NA    NA      NA\n10 0S9   Jeffers…  48.1 -123.    108    -8 A     Amer…    NA    NA    NA      NA\n# … with 330,521 more rows, 14 more variables: sched_dep_time <int>,\n#   dep_delay <dbl>, arr_time <int>, sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, air_time <dbl>,\n#   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated\n#   variable name ¹​dep_time\n\n\nWhy does the number of rows changes after joining?\ndest is not a primary key in flights. There are more flights with the same destination so rows of airports get duplicated.\nWhy is the number of rows then less than the number of rows in flights?\nLet us do some checks:\n\nlength(unique(airports$faa)) # Unique turns out to be redundant because faa is a primary key\n\n[1] 1458\n\nlength(unique(flights$dest))\n\n[1] 105\n\n# There are much more airports then destinations in flights!\n# ... but the rows of airports prevail when it is the first in a left_join.\n# So, the data frame should even increase because \n# we get several rows of airports without flights\n# Let us dig deeper.\n\nsetdiff( unique(airports$faa), unique(flights$dest)) |> length()\n\n[1] 1357\n\n# 1,357 airports have no flights. But also:\nsetdiff( unique(flights$dest), unique(airports$faa)) |> length()\n\n[1] 4\n\n# There are four destinations in flights, which are not in the airports list!\n\n# How many flights are to these?\nflights |> \n  filter(dest %in% setdiff( unique(flights$dest), unique(airports$faa))) |> \n  nrow()\n\n[1] 7602\n\n# 7,602 flights go to destinations not listed as airport\n\n# Check\nnrow(airports |> left_join(flights, by = c(\"faa\" = \"dest\"))) == nrow(flights) - 7602 + 1357\n\n[1] TRUE\n\n# OK, now we have a clear picture\n# airport with left_joined flights duplicates the rows an airports for each flight flying to it\n# So the total number of rows is the number of flights plus the number of airport which do not \n# appear as a destination minus the flights which go to destinations which are not listed in airports\n\nThe new number of observation after a join can be a complex combination of duplication and dropping."
  },
  {
    "objectID": "W5.html#definition-sets-and-vectors",
    "href": "W5.html#definition-sets-and-vectors",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Definition: Sets and vectors",
    "text": "Definition: Sets and vectors\nA set is mathematical model for the collection of different things.\nExamples:\n\n\\(\\{3, \\text{Hi}, 😀, 🖖 \\}\\)\n\\(\\{1,3,5\\}\\)\nThe natural numbers \\(\\mathbb{N} = \\{1, 2, 3, \\dots\\}\\) (infinite!)\n\\(\\{\\mathtt{\"EWR\"} \\mathtt{\"LGA\"} \\mathtt{\"JFK\"}\\}\\)\nthese are origin airports in flights"
  },
  {
    "objectID": "W5.html#math-sets-and-vectors-1",
    "href": "W5.html#math-sets-and-vectors-1",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Math: Sets and vectors",
    "text": "Math: Sets and vectors\nA vector is an ordered collection of things (elements) of the same type.\nIn a set each thing can only be once and the order does not matter!\n\\(\\{1,3,5\\} = \\{3,5,1\\} = \\{1,1,1,3,5,5\\}\\)\nFor vectors:\n\\([1\\ 3\\ 5] \\neq [3\\ 5\\ 1]\\) because we compare component-wise, so we cannot even compare with = \\([1\\ 1\\ 1\\ 3\\ 5\\ 5]\\)"
  },
  {
    "objectID": "W5.html#math-set-operations",
    "href": "W5.html#math-set-operations",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Math: Set operations",
    "text": "Math: Set operations\nFor sets \\(A = \\{🐺, 🦊, 🐶\\}\\) and \\(B = \\{🐶, 🐷, 🐹\\}\\):\n\nSet union \\(A \\cup B\\) = {🐺, 🦊, 🐶, 🐷, 🐹}\nSet intersection \\(A \\cap B\\) = {🐶}\nSet different \\(A \\setminus B\\) = {🐺, 🦊}$, \\(B \\setminus A\\) = {🐷, 🐹}"
  },
  {
    "objectID": "W5.html#set-operations-in-r",
    "href": "W5.html#set-operations-in-r",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Set operations in R",
    "text": "Set operations in R\nunique shows the set of elements in a vector\n\nunique(flights$origin)\n\n[1] \"EWR\" \"LGA\" \"JFK\"\n\n\nsetequal tests for set equality\n\nsetequal(c(\"EWR\",\"LGA\",\"JFK\"), c(\"EWR\",\"EWR\",\"LGA\",\"JFK\"))\n\n[1] TRUE\n\n\nunion, intersect, setdiff treat vectors as sets and operate as expected\n\nunion(1:5,3:7)\n\n[1] 1 2 3 4 5 6 7\n\nintersect(1:5,3:7)\n\n[1] 3 4 5\n\nsetdiff(1:5,3:7)\n\n[1] 1 2"
  },
  {
    "objectID": "W5.html#exploratory-data-analysis-1",
    "href": "W5.html#exploratory-data-analysis-1",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nEDA is the systematic exploration of data using\n\nvisualization\ntransformation\ncomputation of characteristic values\nmodeling\n\n\n\nWe haven’t talked much about the latter two, but will do soon.\nComputation of characteristic values: Functions like mean, median, mode, standard deviation, or interquartile range\nModeling: Operations like linear regression or dimensionality reduction"
  },
  {
    "objectID": "W5.html#systematic-but-no-standard-routine",
    "href": "W5.html#systematic-but-no-standard-routine",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\n“There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey"
  },
  {
    "objectID": "W5.html#systematic-but-no-standard-routine-1",
    "href": "W5.html#systematic-but-no-standard-routine-1",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Systematic but no standard routine",
    "text": "Systematic but no standard routine\n\nGoal of EDA: Develop understanding of your data.\nEDA’s iterative cycle\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modelling your data.\nUse what you learn to refine your questions and/or generate new questions.\n\nEDA is fundamentally a creative process."
  },
  {
    "objectID": "W5.html#questions",
    "href": "W5.html#questions",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Questions",
    "text": "Questions\n\nThe way to ask quality questions:\n\nGenerate many questions!\nYou cannot come up with most interesting questions when you start.\n\nThere is no rule which questions to ask. These are useful\n\nWhat type of variation occurs within my variables?\n(Barplots, Histograms,…)\nWhat type of covariation occurs between my variables?\n(Scatterplots, Timelines,…)"
  },
  {
    "objectID": "W5.html#eda-embedded-in-a-data-science-project",
    "href": "W5.html#eda-embedded-in-a-data-science-project",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "EDA embedded in a data science project",
    "text": "EDA embedded in a data science project\n\nStating and refining the question\nExploring the data\nBuilding formal statistical models\nInterpreting the results\nCommunicating the results\n\n\n\nRoger D. Peng and Elizabeth Matsui. “The Art of Data Science.” A Guide for Anyone Who Works with Data. Skybrude Consulting, LLC (2015)."
  },
  {
    "objectID": "W5.html#six-types-of-questions",
    "href": "W5.html#six-types-of-questions",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Six types of questions",
    "text": "Six types of questions\n\nDescriptive: summarize a characteristic of a set of data\nExploratory: analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\nInferential: analyze patterns, trends, or relationships in representative data from a population\nPredictive: make predictions for individuals or groups of individuals\nCausal: whether changing one factor will change another factor, on average, in a population\nMechanistic: explore “how” as opposed to whether\n\n\n\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146."
  },
  {
    "objectID": "W5.html#data-analysis-flowchart",
    "href": "W5.html#data-analysis-flowchart",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Data Analysis Flowchart",
    "text": "Data Analysis Flowchart"
  },
  {
    "objectID": "W5.html#example-covid-19-and-vitamin-d",
    "href": "W5.html#example-covid-19-and-vitamin-d",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Example: COVID-19 and Vitamin D",
    "text": "Example: COVID-19 and Vitamin D\n\nDescriptive: frequency of hospitalisations due to COVID-19 in a set of data collected from a group of individuals\nExploratory: examine relationships between a range of dietary factors and COVID-19 hospitalisations\nInferential: examine whether any relationship between taking Vitamin D supplements and COVID-19 hospitalisations found in the sample hold for the population at large\nPredictive: what types of people will take Vitamin D supplements during the next year\nCausal: whether people with COVID-19 who were randomly assigned to take Vitamin D supplements or those who were not are hospitalised\nMechanistic: how increased vitamin D intake leads to a reduction in the number of viral illnesses"
  },
  {
    "objectID": "W5.html#questions-to-data-science-problems",
    "href": "W5.html#questions-to-data-science-problems",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Questions to data science problems",
    "text": "Questions to data science problems\n\nDo you have appropriate data to answer your question?\nDo you have information on confounding variables?\nWas the data you’re working with collected in a way that introduces bias?\n\n\n\nExample\nI want to estimate the average number of children in households in Bremen. I conduct a survey at an elementary school and ask pupils how many children, including themselves, live in their house. Then, I take the average of the responses.\n\nIs this a biased or an unbiased estimate of the number of children in households in Bremen?\nIf biased, will the value be an overestimate or underestimate?"
  },
  {
    "objectID": "W5.html#context-information-and-codebooks",
    "href": "W5.html#context-information-and-codebooks",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Context Information and Codebooks",
    "text": "Context Information and Codebooks\n\nNot all information is in the data!\nPotential confounding variables you infer from general knowledge\nInformation about data collection you may receive from an accompanying report\nInformation about computed variables you may need to look up in accompanying documentation\nInformation about certain variables you may find in an accompanying codebook. For example the exact wording of questions in survey data."
  },
  {
    "objectID": "W5.html#next",
    "href": "W5.html#next",
    "title": "W#5 More under the hood, Relational Data, Exploratory Data Analysis",
    "section": "Next",
    "text": "Next\nNext Week\n\nSummarizing functions for data\nSome more math background (linked to programming)\n\nHomework 03\n\nshall come over the weekend, due in two week\nwill move towards\n\nexploratory data analysis\nanswering questions (You have some technical tools now at hand.)\nasking question\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts\n\n\nHint: Everyone has read access to the project repositories https://github.com/JU-F22-MDSSB-MET-01/ess-ind-janlorenz and https://github.com/JU-F22-MDSSB-MET-01/corona-ind-janlorenz as an example for data access. This should help to unify the data for work on Homework 03."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Concepts / Tools",
    "section": "",
    "text": "Information for late coming students\n\n\n\nWelcome! You are in the right place to get into the course. There is a lot of material. Here we provide a checklist for late coming students"
  },
  {
    "objectID": "index.html#modules-data-science-concepts-methods",
    "href": "index.html#modules-data-science-concepts-methods",
    "title": "Data Science Concepts / Tools",
    "section": "1.1 Modules Data Science Concepts / Methods",
    "text": "1.1 Modules Data Science Concepts / Methods\nThese two modules are orchestrated in close cooperation\n\nData Science Concepts (Core module: MSDSSB-DSOC-02)\nData Science Tools (Methods module: MDSSB-MET-01)\n\nYou should know how the courses are integrated in the overall Master program and the module descriptions from the DSSB Handbook."
  },
  {
    "objectID": "index.html#courses-and-instructors",
    "href": "index.html#courses-and-instructors",
    "title": "Data Science Concepts / Tools",
    "section": "1.2 Courses and Instructors",
    "text": "1.2 Courses and Instructors\nThursday is the concepts and tools day!\nData Science Tools in R: Thursday 9:45 - 11:00 Armin Müller\nData Science Tools in Python: Thursday 11:15 - 12:30 Martin Gestefeld\nData Science Concepts Lectures: Thursday 14:15 - 17:00 (2 Sessions) Jan Lorenz"
  },
  {
    "objectID": "index.html#goal",
    "href": "index.html#goal",
    "title": "Data Science Concepts / Tools",
    "section": "1.3 Goal",
    "text": "1.3 Goal\nOur goal in the two modules is to enable you to\n\ncreate and maintain a digital working environment on your computer to do data science\nlearn core concepts in data science, that means\n\nlearn concepts to explore data (visualize, import, wrangle)\nlearn or refresh some mathematics and statistics concepts through the data science lens\nlearn concepts to model and draw conclusions from data (model, infer, predict)\n\nlearn to program in the data science languages R and python, and become able to learn new skills in these independently\ndo a data science project of your own interest\n\nYou can build a good basis for your more and more independent work in the whole program."
  },
  {
    "objectID": "index.html#expectations-for-students",
    "href": "index.html#expectations-for-students",
    "title": "Data Science Concepts / Tools",
    "section": "1.4 Expectations for students",
    "text": "1.4 Expectations for students\nWe rely on your engagement. Taken holistically, engagement is many-faceted and includes\n\nPreparation (looking at readings and material before class, being informed about syllabus and course material)\nFocus (avoid distraction during in class and online activities)\nPresence (listening and responding during group activities)\nAsking questions (in class, out of class, online, offline)\nSpecificity (being specific when referring to ideas from readings and discussions)\nSynthesizing (making connections between concepts from reading and discussion)\n\n(Adapted from Twitter: Mark Sample)"
  },
  {
    "objectID": "index.html#online-infrastructure",
    "href": "index.html#online-infrastructure",
    "title": "Data Science Concepts / Tools",
    "section": "1.5 Online Infrastructure",
    "text": "1.5 Online Infrastructure\nCampusnet This is the official registration site. Besides the final grade input for the modules we will not use it in the courses, but you should be able to get back to this site from there once you are lost.\njMoodle An e-learning platform provided by Jacobs University. Log in with your campusnet credentials. Under the Data Science Concepts Course (identifier MDSSB-DSOC-02_f2022_1) we publish information (e.g. survey links) which shall be shared only among participants of the course. (After all this is a public website, although it is not intended to be announced publicly.)\n\n\n\n\n\n\nImportant\n\n\n\nTest to log in and visit the Data Science Concepts course on jMoodle. Be prepared to go there during class!\n\n\nMicrosoft Teams: In MS Teams there is a Team for the course. Teams will be used for hybrid online and in class teaching. It is expected that you come to class in presence if possible. Participation should only be online when abroad or CoViD-isolated at home. The “General” channel can be used for announcements. You can also post there. You can also post to me directly over Teams and hope for a quick answer.\nIf possible, recordings are made via Teams and saved in the Teams general channel under Files -> Recordings.\nGitHub: All delivery and submission of homework and the final exam is via individual repositories on Github in the GitHub organization https://github.com/JU-F22-MDSSB-MET-01. Details are in the lecture slides of week 1 and Homework 01 on this page."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Data Science Concepts / Tools",
    "section": "1.6 Software",
    "text": "1.6 Software\nWe use a mandatory set of software. Details are in the slides of week 1. Our task with highest priority is to realize that everyone has full installation and functionality on their local machines. All software is freely available."
  },
  {
    "objectID": "index.html#assessment-and-grading",
    "href": "index.html#assessment-and-grading",
    "title": "Data Science Concepts / Tools",
    "section": "1.7 Assessment and Grading",
    "text": "1.7 Assessment and Grading\n\n1.7.1 Data Science Concepts Module\nAssessment is based on an exam of 120 minutes. It is scheduled officially on campusnet by academic offices for Dec 15, 12:30-14:30.\nThe exam will be open-book and take-home exam. It will be delivered individually via GitHub through the organization https://github.com/JU-F22-MDSSB-MET-01 as the Homework.\n\n\n1.7.2 Data Science Tools Module\nAssessment is based on a team project of approximately 4000-5000 words. Which are delivered in personalized repositories in the GitHub organization https://github.com/JU-F22-MDSSB-MET-01.\nAdditionally, the handbook lists a requirement to pass the module and a bonus option to increase the grade by 0.33 (while the best grade is also possible with a great project report without the bonus).\n\nRequirement: Half of the homework assignment tasks need to be correctly solved. These do not determine the grade. We will check this after the deadline for the last homework passed at the end of classes.\nBonus: The individual repositories about Corona hw-ind-corona-USERNAME and the ESS hw-ind-ess-USERNAME shall be cleaned from not necessary files. All analysis the qmd-Document should be complete and the questions should be answered in a consistent way. The rendered html document should look well formatted and should be readable as a report independent from the instructions."
  },
  {
    "objectID": "index.html#week-1-sep-1-what-is-data-science-course-organization-toolkit",
    "href": "index.html#week-1-sep-1-what-is-data-science-course-organization-toolkit",
    "title": "Data Science Concepts / Tools",
    "section": "Week 1, Sep 1: What is Data Science? Course organization, toolkit",
    "text": "Week 1, Sep 1: What is Data Science? Course organization, toolkit\nSlides Week 1\nFind the homework assignment “Homework 01” on this website. Deadline: Sunday, Sep 18. The content will be discussed in the Tools course in Week 2 and 3"
  },
  {
    "objectID": "index.html#week-2-sep-8-no-lectures",
    "href": "index.html#week-2-sep-8-no-lectures",
    "title": "Data Science Concepts / Tools",
    "section": "Week 2, Sep 8: No lectures",
    "text": "Week 2, Sep 8: No lectures\nThere will be no lectures, because of a mandatory central events!\nThe first Data Science Tools courses in the morning will take place!\nReading instead of lecture: R for Data Science: Chapter 3 on Data Visualization"
  },
  {
    "objectID": "index.html#week-3-sep-15-data-visualization-data-formats",
    "href": "index.html#week-3-sep-15-data-visualization-data-formats",
    "title": "Data Science Concepts / Tools",
    "section": "Week 3, Sep 15: Data visualization, Data formats",
    "text": "Week 3, Sep 15: Data visualization, Data formats\nSlides Week 3\nLab work on code. Clone from: https://github.com/JU-F22-MDSSB-MET-01/codebase-janlorenz.git In RStudio, work through\n\n2022-09-15_ggplot_and_pipe.R\n2022-09-15_profiles.qmd"
  },
  {
    "objectID": "index.html#week-4-sep-22-data-import-data-wrangling",
    "href": "index.html#week-4-sep-22-data-import-data-wrangling",
    "title": "Data Science Concepts / Tools",
    "section": "Week 4, Sep 22: Data import, data Wrangling",
    "text": "Week 4, Sep 22: Data import, data Wrangling\nSlides Week 4\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nIncluding some recap of the toolkit.\nQuestions and advice on Homework 02 (mostly in Recording)."
  },
  {
    "objectID": "index.html#week-5-sep-29-more-under-the-hood-relational-data-exploratory-data-analysis",
    "href": "index.html#week-5-sep-29-more-under-the-hood-relational-data-exploratory-data-analysis",
    "title": "Data Science Concepts / Tools",
    "section": "Week 5, Sep 29: More under the hood, Relational Data, Exploratory Data Analysis",
    "text": "Week 5, Sep 29: More under the hood, Relational Data, Exploratory Data Analysis\nSlides Week 5\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nTopics:\n\nMore under the hood\nAugmented vectors: Factors and Dates\nStrings\nRelational Data: Joins, joins, joins …\nMath: Sets and vectors\nExploratory Data Analysis\nData Science Projects\n\nHomework 02 due next Sunday.\nHomework 03 comes over the weekend."
  },
  {
    "objectID": "index.html#week-6-oct-6-functions-logarithms-and-exponentials-modeling-fitting-a-linear-model",
    "href": "index.html#week-6-oct-6-functions-logarithms-and-exponentials-modeling-fitting-a-linear-model",
    "title": "Data Science Concepts / Tools",
    "section": "Week 6, Oct 6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model",
    "text": "Week 6, Oct 6: Functions, Logarithms and Exponentials, Modeling, Fitting a Linear Model\nSlides Week 6\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 03 is due Oct 16."
  },
  {
    "objectID": "index.html#week-7-oct-13-descriptive-statistics-wisdom-of-crowds-calculus-epidemic-models",
    "href": "index.html#week-7-oct-13-descriptive-statistics-wisdom-of-crowds-calculus-epidemic-models",
    "title": "Data Science Concepts / Tools",
    "section": "Week 7, Oct 13: Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models",
    "text": "Week 7, Oct 13: Descriptive statistics, Wisdom of Crowds, Calculus, Epidemic Models\nSlides Week 7\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 03 is due Oct 16."
  },
  {
    "objectID": "index.html#week-8-oct-20-homework-topics-typical-data-issual-more-linear-models-and-interpretation",
    "href": "index.html#week-8-oct-20-homework-topics-typical-data-issual-more-linear-models-and-interpretation",
    "title": "Data Science Concepts / Tools",
    "section": "Week 8, Oct 20: Homework topics: Typical Data Issual, More Linear Models and Interpretation",
    "text": "Week 8, Oct 20: Homework topics: Typical Data Issual, More Linear Models and Interpretation\nTopics:\n\nErrors, Differences, Missings in Data\nMore Linear Models\n\nMore predictors\nMain effects and interaction effects\n\nTo be finished\n\nSlides Week 8\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 should come. Due Nov 6."
  },
  {
    "objectID": "index.html#week-9-oct-27",
    "href": "index.html#week-9-oct-27",
    "title": "Data Science Concepts / Tools",
    "section": "Week 9, Oct 27:",
    "text": "Week 9, Oct 27:"
  },
  {
    "objectID": "index.html#week-10-nov-3",
    "href": "index.html#week-10-nov-3",
    "title": "Data Science Concepts / Tools",
    "section": "Week 10, Nov 3:",
    "text": "Week 10, Nov 3:\nHomework 04 due in 3 days\nHomework 05 should come. Due Nov 20."
  },
  {
    "objectID": "index.html#week-11-nov-10",
    "href": "index.html#week-11-nov-10",
    "title": "Data Science Concepts / Tools",
    "section": "Week 11, Nov 10:",
    "text": "Week 11, Nov 10:"
  },
  {
    "objectID": "index.html#week-12-nov-17",
    "href": "index.html#week-12-nov-17",
    "title": "Data Science Concepts / Tools",
    "section": "Week 12, Nov 17:",
    "text": "Week 12, Nov 17:\nSlides Week 12\nHomework 05 due in 3 days\nHomework 06 should come. Due Dec 4."
  },
  {
    "objectID": "index.html#week-13-nov-24",
    "href": "index.html#week-13-nov-24",
    "title": "Data Science Concepts / Tools",
    "section": "Week 13, Nov 24:",
    "text": "Week 13, Nov 24:"
  },
  {
    "objectID": "index.html#week-14-dec-1-course-review-exam-preparation",
    "href": "index.html#week-14-dec-1-course-review-exam-preparation",
    "title": "Data Science Concepts / Tools",
    "section": "Week 14, Dec 1: Course Review, Exam preparation",
    "text": "Week 14, Dec 1: Course Review, Exam preparation\nHomework 06 should come before Thursday. It includes questions similar to what will be asked in the exam of “Data Science Concepts”. We can discuss it in class.\nThe deadline of Homework 06 will be the end of the semester. However, it obviously makes sense to do it before the exam."
  },
  {
    "objectID": "index.html#exam-data-science-concepts-module",
    "href": "index.html#exam-data-science-concepts-module",
    "title": "Data Science Concepts / Tools",
    "section": "Exam Data Science Concepts module",
    "text": "Exam Data Science Concepts module\nIn December, will be scheduled officially on campusnet by academic offices."
  },
  {
    "objectID": "W9.html#how-many-observations-are-there-for-each-country-year-combination",
    "href": "W9.html#how-many-observations-are-there-for-each-country-year-combination",
    "title": "W#9 Logisitc Regression",
    "section": "How many observations are there for each country-year combination?",
    "text": "How many observations are there for each country-year combination?\nWeighting"
  },
  {
    "objectID": "W9.html#more-weights-from-the-survey",
    "href": "W9.html#more-weights-from-the-survey",
    "title": "W#9 Logisitc Regression",
    "section": "More weights from the survey",
    "text": "More weights from the survey"
  },
  {
    "objectID": "W9.html#mean-squared-error",
    "href": "W9.html#mean-squared-error",
    "title": "W#9 Logisitc Regression",
    "section": "Mean squared error",
    "text": "Mean squared error"
  },
  {
    "objectID": "W9.html#how-deaths-follow-cases",
    "href": "W9.html#how-deaths-follow-cases",
    "title": "W#9 Logisitc Regression",
    "section": "How Deaths Follow Cases",
    "text": "How Deaths Follow Cases"
  },
  {
    "objectID": "W9.html#next",
    "href": "W9.html#next",
    "title": "W#9 Logisitc Regression",
    "section": "Next",
    "text": "Next\nhttps://datasciencebox.org/course-materials/_slides/u4-d05-more-model-multiple-predictors/u4-d05-more-model-multiple-predictors.html#1"
  },
  {
    "objectID": "W9.html#test-und-training-set",
    "href": "W9.html#test-und-training-set",
    "title": "W#9 Logisitc Regression",
    "section": "Test und Training set",
    "text": "Test und Training set\nCross validation"
  },
  {
    "objectID": "W9.html#decision-trees",
    "href": "W9.html#decision-trees",
    "title": "W#9 Logisitc Regression",
    "section": "Decision Trees",
    "text": "Decision Trees"
  },
  {
    "objectID": "W4.html#programming-languages",
    "href": "W4.html#programming-languages",
    "title": "W#4 Data import, data wrangling",
    "section": "Programming languages",
    "text": "Programming languages\nSystems of rules which can process instructions to be executed by the computer.\nOur programming languages are:\n\n   \n\n\n\n\nIn R with function:\ndo_this(to_this)\ndo_that(to_this, with_those)\nto_this |> do_this() |> do_that(with_those) \nstore <- do_that(to_this)\n\nIn python:\nto_this.do_this()\nto_this.do_this(with_those)\nto_this.do_this().do_that(with_those)\nstore = do_that(to_this)\n\n\nWe can use R and python in a standard terminal (write R or python3) and write scripts with any editor (Wordpad)."
  },
  {
    "objectID": "W4.html#integrated-development-environment",
    "href": "W4.html#integrated-development-environment",
    "title": "W#4 Data import, data wrangling",
    "section": "Integrated development environment",
    "text": "Integrated development environment\nIDEs provide terminals, a source code editor, an object browser, output and help view, tools for rendering and version control, and more to help in the workflow. Our IDEs are:\n\n    VS Code\n\n\nEditors delight us with\n\nsyntax highlighting Then we see if code looks good\n\nc(1O, Text, true, 10,\"Text\",TRUE)\n\ncode completion Start writing, and press Tab to see options\nautomatic indentation, brace matching, keyboard shortcuts, …"
  },
  {
    "objectID": "W4.html#publishing-system",
    "href": "W4.html#publishing-system",
    "title": "W#4 Data import, data wrangling",
    "section": "Publishing system",
    "text": "Publishing system\nWeaves together text and code to produces good-looking formatted scientific or technical output.\n\n   \n\n\nA YAML header and Markdown text with code chunks is rendered to a document in several formats.\n notebook is a similar concept: text and executable code mixed together in a browser tab. Can be rendered by quarto. Popular in the python world."
  },
  {
    "objectID": "W4.html#publish-what",
    "href": "W4.html#publish-what",
    "title": "W#4 Data import, data wrangling",
    "section": "Publish what?",
    "text": "Publish what?\n\nyour project report as (html)\nmake a personal website (using GitHub pages)\nwrite your thesis (pdf)"
  },
  {
    "objectID": "W4.html#feedback-on-homework-01",
    "href": "W4.html#feedback-on-homework-01",
    "title": "W#4 Data import, data wrangling",
    "section": "Feedback on Homework 01",
    "text": "Feedback on Homework 01\nSome of you did not modify the line\n“The dimension with the most experience is … The dimension with the least experience is …”\nor did not replace the line\n“Remove this text and write you answer to Exercise 4.”\nwith your text.\nYou did the programming right, but forgot the textual part. Now, this doesn’t matter. However, the learning goal was not only programming, but also taking care that the rendered output communicates your work well."
  },
  {
    "objectID": "W4.html#version-control",
    "href": "W4.html#version-control",
    "title": "W#4 Data import, data wrangling",
    "section": "Version control",
    "text": "Version control\n   \ngit manages local versioning of files in a directory1 as repository2, and merging different versions of the repository.\nGitHub provides git server for repositories and collaborative tools.\nDirectory = FolderRepository = A directory including a subfolder .git which stores the history of as commits."
  },
  {
    "objectID": "W4.html#command-line-interfaces",
    "href": "W4.html#command-line-interfaces",
    "title": "W#4 Data import, data wrangling",
    "section": "Command line interfaces",
    "text": "Command line interfaces\nIn CLIs you communicate with your computer using the Read-Evaluate-Print-Loop (REPL). Terminal, Shell, Console all mostly synonym to CLI\n\nTerminal to access files and programs via commands in bash or zsh1. Also available in RStudio and VS Code.\nR console provided in RStudio2\npython3 console provided by VS Code3\n\nLanguages used in the Terminal with commands like cd=change directory, pwd=print working directory, or ls=list files.Can also be started in Terminal with RCan also be started in Terminal with python3"
  },
  {
    "objectID": "W4.html#feedback-on-version-control",
    "href": "W4.html#feedback-on-version-control",
    "title": "W#4 Data import, data wrangling",
    "section": "Feedback on version control",
    "text": "Feedback on version control\n\nUsing git and GitHub is one of the most common modes of collaboration involving coding\nIt is a learning goal to get used to it\nIt will probably not go away after this course\ngit problems are sometimes uncomfortable to solve and require concentration and grit (also for experienced people)"
  },
  {
    "objectID": "W4.html#quarto-and-git-only-use-the-cli",
    "href": "W4.html#quarto-and-git-only-use-the-cli",
    "title": "W#4 Data import, data wrangling",
    "section": "quarto and git only use the CLI",
    "text": "quarto and git only use the CLI\nFor example:\n\nquarto render MyFile.qmd --to docx renders the MyFile.qmd to a Word file  git add MyFile.qmd adds MyFile.qmd (or ots changes) to the staging area\ngit commit -m \"Update of code\" creates new commit with staged files\ngit push merge local commit into repository it was cloned from\n\n\nRStudio/VS Code provide buttons and shortcuts for the most common commands\nYou can also do it yourself.\nUse it to solve a problem in an “uncommon” situation, after research about the problem:\n\nread error message carefully (often they give a hint, but not always)\nsearching StackOverflow\nasking others\nfiling an issue in our General Discussion"
  },
  {
    "objectID": "W4.html#readr-and-readxl",
    "href": "W4.html#readr-and-readxl",
    "title": "W#4 Data import, data wrangling",
    "section": "readr and readxl",
    "text": "readr and readxl\n\n\n\n\nread_csv() - comma delimited files\nread_csv2() - semicolon delimited files (common where “,” is used as decimal place)\nread_tsv() - tab delimited files\nread_delim() - reads in files with any delimiter\n…\n\n\n\n\nread_excel() read xls or xlsx files from MS Excel\n…"
  },
  {
    "objectID": "W4.html#other-data-formats",
    "href": "W4.html#other-data-formats",
    "title": "W#4 Data import, data wrangling",
    "section": "Other data formats",
    "text": "Other data formats\nR packages, analog libraries will exist for python\n\ngooglesheets4: Google Sheets\nhaven: SPSS, Stata, and SAS files\nDBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc): allows you to run SQL queries against a database and return a data frame\njsonline: JSON\nxml2: xml\nrvest: web scraping\nhttr: web APIs\n…"
  },
  {
    "objectID": "W4.html#comma-separated-values-csv",
    "href": "W4.html#comma-separated-values-csv",
    "title": "W#4 Data import, data wrangling",
    "section": "Comma-separated values (CSV)",
    "text": "Comma-separated values (CSV)\nWe use this when there is no certain reason to do otherwise (it is not provided, or storage is an issue).\nCSV files are delimited text file\n\nCan be viewed with any text editor\nShow each row of the data frame in a line\nSeparates the content of columns by commas (or the delimiter character)\nEach cell could be surrounded by quotes (when long text with commas (!) is in cells)\nThe first line is interpreted as listing the variable names by default\n\nreadr tries to guess the data type of variables\nYou can also customize it yourself!"
  },
  {
    "objectID": "W4.html#data-import-workflow",
    "href": "W4.html#data-import-workflow",
    "title": "W#4 Data import, data wrangling",
    "section": "Data import workflow",
    "text": "Data import workflow\n\nYou download your CSV file to the data/ directory. You may use download.file() for this, but make sure you do not download large amounts of data each time you render your file! (Comment out # and use again only when needed.)\nRead the data with data <- read_csv(\"data/FILENAME.csv\") and read the report in the console.\nExplore if you are happy and iterate by customizing you data import line using specifications (see the function help) until the data is as you want it to be.\n\nUse this for Homework 02 for the ESS and corona projects.\nSelf-learning:  concepts similar for loading CSV in python."
  },
  {
    "objectID": "W4.html#columns-types",
    "href": "W4.html#columns-types",
    "title": "W#4 Data import, data wrangling",
    "section": "Columns types",
    "text": "Columns types\n\n\n\ntype function\ndata type\n\n\n\n\ncol_character()\ncharacter\n\n\ncol_date()\ndate\n\n\ncol_datetime()\nPOSIXct (date-time)\n\n\ncol_double()\ndouble (numeric)\n\n\ncol_factor()\nfactor\n\n\ncol_guess()\nlet readr guess (default)\n\n\ncol_integer()\ninteger\n\n\ncol_logical()\nlogical\n\n\ncol_number()\nnumbers mixed with non-number characters\n\n\ncol_numeric()\ndouble or integer\n\n\ncol_skip()\ndo not read\n\n\ncol_time()\ntime"
  },
  {
    "objectID": "W4.html#data-hotel-bookings",
    "href": "W4.html#data-hotel-bookings",
    "title": "W#4 Data import, data wrangling",
    "section": "Data: Hotel bookings",
    "text": "Data: Hotel bookings\n\nData from two hotels: one resort and one city hotel\nObservations: Each row represents a hotel booking\n\n\n\nhotels<- read_csv(\"data/hotels.csv\")\n\nRows: 119390 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): hotel, arrival_date_month, meal, country, market_segment, distrib...\ndbl  (18): is_canceled, lead_time, arrival_date_year, arrival_date_week_numb...\ndate  (1): reservation_status_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "W4.html#grammar-of-data-wrangling",
    "href": "W4.html#grammar-of-data-wrangling",
    "title": "W#4 Data import, data wrangling",
    "section": "Grammar of Data Wrangling",
    "text": "Grammar of Data Wrangling\n\n\n\n\n\nGrammar of data wrangling: Start with a dataset and pipe it through several manipulations with |>\nmpg |> \n  filter(cyl == 8) |> \n  select(manufacturer, hwy) |> \n  group_by(manufacturer) |> \n  summarize(mean_hwy = mean(hwy))\n\n\nIn python: Similar concept making a chain using . to apply pandas methods for data frames one after the other.\nCompare the grammar of graphics ggplot2: Start creating a ggplot object, specifying data, and mapping variables to aesthetics, add graphical layers (geom_ functions) with +\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = trans)) + \n  geom_point() + \n  geom_smooth()"
  },
  {
    "objectID": "W4.html#dplyr-uses-verbs-to-manipulate",
    "href": "W4.html#dplyr-uses-verbs-to-manipulate",
    "title": "W#4 Data import, data wrangling",
    "section": "dplyr uses verbs to manipulate",
    "text": "dplyr uses verbs to manipulate\n\nselect: pick columns by name\narrange: reorder rows\nslice: pick rows using index(es)\nfilter: pick rows matching criteria\ndistinct: filter for unique rows\nmutate: add new variables\nsummarise: reduce variables to values\ngroup_by: for grouped operations\n… (many more)\n\nWhy does piping with |> work?\n\nBecause every dplyr function takes a data frame as first argument and outputs a (manipulated) data frame."
  },
  {
    "objectID": "W4.html#back-to-hotel-data-first-look",
    "href": "W4.html#back-to-hotel-data-first-look",
    "title": "W#4 Data import, data wrangling",
    "section": "Back to hotel data: First look",
    "text": "Back to hotel data: First look\n\nhotels <- read_csv(\"data/hotels.csv\")\n\n\nFirst look on variables names\n\nnames(hotels)\n\n\n\n [1] \"hotel\"                          \"is_canceled\"                   \n [3] \"lead_time\"                      \"arrival_date_year\"             \n [5] \"arrival_date_month\"             \"arrival_date_week_number\"      \n [7] \"arrival_date_day_of_month\"      \"stays_in_weekend_nights\"       \n [9] \"stays_in_week_nights\"           \"adults\"                        \n[11] \"children\"                       \"babies\"                        \n[13] \"meal\"                           \"country\"                       \n[15] \"market_segment\"                 \"distribution_channel\"          \n[17] \"is_repeated_guest\"              \"previous_cancellations\"        \n[19] \"previous_bookings_not_canceled\" \"reserved_room_type\"            \n[21] \"assigned_room_type\"             \"booking_changes\"               \n[23] \"deposit_type\"                   \"agent\"                         \n[25] \"company\"                        \"days_in_waiting_list\"          \n[27] \"customer_type\"                  \"adr\"                           \n[29] \"required_car_parking_spaces\"    \"total_of_special_requests\"     \n[31] \"reservation_status\"             \"reservation_status_date\"       \n\n\n\nTo download the data you can use in R: download.file(\"https://raw.githubusercontent.com/rstudio-education/datascience-box/main/course-materials/_slides/u2-d06-grammar-wrangle/data/hotels.csv\", \"data/hotels.csv\")"
  },
  {
    "objectID": "W4.html#second-look-glimpse",
    "href": "W4.html#second-look-glimpse",
    "title": "W#4 Data import, data wrangling",
    "section": "Second look glimpse",
    "text": "Second look glimpse\n\nglimpse(hotels)\n\n\n\nRows: 119,390\nColumns: 32\n$ hotel                          <chr> \"Resort Hotel\", \"Resort Hotel\", \"Resort…\n$ is_canceled                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, …\n$ lead_time                      <dbl> 342, 737, 7, 13, 14, 14, 0, 9, 85, 75, …\n$ arrival_date_year              <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 201…\n$ arrival_date_month             <chr> \"July\", \"July\", \"July\", \"July\", \"July\",…\n$ arrival_date_week_number       <dbl> 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ arrival_date_day_of_month      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ stays_in_weekend_nights        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ stays_in_week_nights           <dbl> 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, …\n$ adults                         <dbl> 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ children                       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ babies                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ meal                           <chr> \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB\", \"BB…\n$ country                        <chr> \"PRT\", \"PRT\", \"GBR\", \"GBR\", \"GBR\", \"GBR…\n$ market_segment                 <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ distribution_channel           <chr> \"Direct\", \"Direct\", \"Direct\", \"Corporat…\n$ is_repeated_guest              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_cancellations         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_bookings_not_canceled <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ reserved_room_type             <chr> \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ assigned_room_type             <chr> \"C\", \"C\", \"C\", \"A\", \"A\", \"A\", \"C\", \"C\",…\n$ booking_changes                <dbl> 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ deposit_type                   <chr> \"No Deposit\", \"No Deposit\", \"No Deposit…\n$ agent                          <chr> \"NULL\", \"NULL\", \"NULL\", \"304\", \"240\", \"…\n$ company                        <chr> \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\",…\n$ days_in_waiting_list           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ customer_type                  <chr> \"Transient\", \"Transient\", \"Transient\", …\n$ adr                            <dbl> 0.00, 0.00, 75.00, 75.00, 98.00, 98.00,…\n$ required_car_parking_spaces    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_of_special_requests      <dbl> 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 3, …\n$ reservation_status             <chr> \"Check-Out\", \"Check-Out\", \"Check-Out\", …\n$ reservation_status_date        <date> 2015-07-01, 2015-07-01, 2015-07-02, 20…"
  },
  {
    "objectID": "W4.html#select-a-sinlge-colum",
    "href": "W4.html#select-a-sinlge-colum",
    "title": "W#4 Data import, data wrangling",
    "section": "Select a sinlge colum",
    "text": "Select a sinlge colum\n\nhotels |> select(lead_time)     \n\n\n\n# A tibble: 119,390 × 1\n   lead_time\n       <dbl>\n 1       342\n 2       737\n 3         7\n 4        13\n 5        14\n 6        14\n 7         0\n 8         9\n 9        85\n10        75\n# … with 119,380 more rows\n\n\nNote: select(hotels, lead_time) is identical.\n\n\nIn hotel business, lead time is the time betweeen booking and arrival."
  },
  {
    "objectID": "W4.html#select-more-columns",
    "href": "W4.html#select-more-columns",
    "title": "W#4 Data import, data wrangling",
    "section": "Select more columns",
    "text": "Select more columns\n\nhotels |> select(hotel, lead_time)     \n\n\n\n# A tibble: 119,390 × 2\n   hotel        lead_time\n   <chr>            <dbl>\n 1 Resort Hotel       342\n 2 Resort Hotel       737\n 3 Resort Hotel         7\n 4 Resort Hotel        13\n 5 Resort Hotel        14\n 6 Resort Hotel        14\n 7 Resort Hotel         0\n 8 Resort Hotel         9\n 9 Resort Hotel        85\n10 Resort Hotel        75\n# … with 119,380 more rows\n\n\nNote that hotel is a variable, but hotels the data frame object name"
  },
  {
    "objectID": "W4.html#select-helper-starts_with",
    "href": "W4.html#select-helper-starts_with",
    "title": "W#4 Data import, data wrangling",
    "section": "Select helper starts_with",
    "text": "Select helper starts_with\n\nhotels |> select(starts_with(\"arrival\"))\n\n\n\n# A tibble: 119,390 × 4\n   arrival_date_year arrival_date_month arrival_date_week_number arrival_date_…¹\n               <dbl> <chr>                                 <dbl>           <dbl>\n 1              2015 July                                     27               1\n 2              2015 July                                     27               1\n 3              2015 July                                     27               1\n 4              2015 July                                     27               1\n 5              2015 July                                     27               1\n 6              2015 July                                     27               1\n 7              2015 July                                     27               1\n 8              2015 July                                     27               1\n 9              2015 July                                     27               1\n10              2015 July                                     27               1\n# … with 119,380 more rows, and abbreviated variable name\n#   ¹​arrival_date_day_of_month"
  },
  {
    "objectID": "W4.html#bring-columns-to-the-front",
    "href": "W4.html#bring-columns-to-the-front",
    "title": "W#4 Data import, data wrangling",
    "section": "Bring columns to the front",
    "text": "Bring columns to the front\n\nhotels |> select(hotel, market_segment, children, everything())\n\n\n\n# A tibble: 119,390 × 32\n   hotel marke…¹ child…² is_ca…³ lead_…⁴ arriv…⁵ arriv…⁶ arriv…⁷ arriv…⁸ stays…⁹\n   <chr> <chr>     <dbl>   <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>\n 1 Reso… Direct        0       0     342    2015 July         27       1       0\n 2 Reso… Direct        0       0     737    2015 July         27       1       0\n 3 Reso… Direct        0       0       7    2015 July         27       1       0\n 4 Reso… Corpor…       0       0      13    2015 July         27       1       0\n 5 Reso… Online…       0       0      14    2015 July         27       1       0\n 6 Reso… Online…       0       0      14    2015 July         27       1       0\n 7 Reso… Direct        0       0       0    2015 July         27       1       0\n 8 Reso… Direct        0       0       9    2015 July         27       1       0\n 9 Reso… Online…       0       1      85    2015 July         27       1       0\n10 Reso… Offlin…       0       1      75    2015 July         27       1       0\n# … with 119,380 more rows, 22 more variables: stays_in_week_nights <dbl>,\n#   adults <dbl>, babies <dbl>, meal <chr>, country <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#more-select-helpers",
    "href": "W4.html#more-select-helpers",
    "title": "W#4 Data import, data wrangling",
    "section": "More select helpers",
    "text": "More select helpers\n\nstarts_with(): Starts with a prefix\nends_with(): Ends with a suffix\ncontains(): Contains a literal string\nnum_range(): Matches a numerical range like x01, x02, x03\none_of(): Matches variable names in a character vector\neverything(): Matches all variables\nlast_col(): Select last variable, possibly with an offset\nmatches(): Matches a regular expression (a sequence of symbols/characters expressing a string/pattern to be searched for within text)\n\n\n\nCheck details with ?one_of"
  },
  {
    "objectID": "W4.html#slice-for-certain-rows",
    "href": "W4.html#slice-for-certain-rows",
    "title": "W#4 Data import, data wrangling",
    "section": "slice for certain rows",
    "text": "slice for certain rows\n\nhotels |> slice(2:4)\n\n\n\n# A tibble: 3 × 32\n  hotel   is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n  <chr>     <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n1 Resort…       0     737    2015 July         27       1       0       0      2\n2 Resort…       0       7    2015 July         27       1       0       1      1\n3 Resort…       0      13    2015 July         27       1       0       1      1\n# … with 22 more variables: children <dbl>, babies <dbl>, meal <chr>,\n#   country <chr>, market_segment <chr>, distribution_channel <chr>,\n#   is_repeated_guest <dbl>, previous_cancellations <dbl>,\n#   previous_bookings_not_canceled <dbl>, reserved_room_type <chr>,\n#   assigned_room_type <chr>, booking_changes <dbl>, deposit_type <chr>,\n#   agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#filter-for-rows-with-certain-criteria",
    "href": "W4.html#filter-for-rows-with-certain-criteria",
    "title": "W#4 Data import, data wrangling",
    "section": "filter for rows with certain criteria",
    "text": "filter for rows with certain criteria\n\nhotels |> filter(hotel == \"City Hotel\")\n\n\n\n# A tibble: 79,330 × 32\n   hotel  is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n   <chr>    <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 City …       0       6    2015 July         27       1       0       2      1\n 2 City …       1      88    2015 July         27       1       0       4      2\n 3 City …       1      65    2015 July         27       1       0       4      1\n 4 City …       1      92    2015 July         27       1       2       4      2\n 5 City …       1     100    2015 July         27       2       0       2      2\n 6 City …       1      79    2015 July         27       2       0       3      2\n 7 City …       0       3    2015 July         27       2       0       3      1\n 8 City …       1      63    2015 July         27       2       1       3      1\n 9 City …       1      62    2015 July         27       2       2       3      2\n10 City …       1      62    2015 July         27       2       2       3      2\n# … with 79,320 more rows, 22 more variables: children <dbl>, babies <dbl>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#filter-for-multiple-criteria",
    "href": "W4.html#filter-for-multiple-criteria",
    "title": "W#4 Data import, data wrangling",
    "section": "filter for multiple criteria",
    "text": "filter for multiple criteria\n\nhotels |> filter(\n  babies >= 1,\n  children >= 1, \n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 175 × 4\n   hotel        adults babies children\n   <chr>         <dbl>  <dbl>    <dbl>\n 1 Resort Hotel      2      1        1\n 2 Resort Hotel      2      1        1\n 3 Resort Hotel      2      1        1\n 4 Resort Hotel      2      1        1\n 5 Resort Hotel      2      1        1\n 6 Resort Hotel      2      1        1\n 7 Resort Hotel      2      1        1\n 8 Resort Hotel      2      1        2\n 9 Resort Hotel      2      1        2\n10 Resort Hotel      1      1        2\n# … with 165 more rows\n\n\nComma-separated conditions are interpreted as all these should be fulfilled.\nThis is identical to the logical AND &.\nhotels |> filter(babies >= 1 & children >= 1)"
  },
  {
    "objectID": "W4.html#filter-for-complexer-criteria",
    "href": "W4.html#filter-for-complexer-criteria",
    "title": "W#4 Data import, data wrangling",
    "section": "filter for complexer criteria",
    "text": "filter for complexer criteria\n\nhotels |> filter(\n  babies >= 1 | children >= 1\n  ) |> \n  select(hotel, adults, babies, children)\n\n\n\n# A tibble: 9,332 × 4\n   hotel        adults babies children\n   <chr>         <dbl>  <dbl>    <dbl>\n 1 Resort Hotel      2      0        1\n 2 Resort Hotel      2      0        2\n 3 Resort Hotel      2      0        2\n 4 Resort Hotel      2      0        2\n 5 Resort Hotel      2      0        1\n 6 Resort Hotel      2      0        1\n 7 Resort Hotel      1      0        2\n 8 Resort Hotel      2      0        2\n 9 Resort Hotel      2      1        0\n10 Resort Hotel      2      1        0\n# … with 9,322 more rows\n\n\n| is the logical OR. Only one criterion needs to be fulfilled."
  },
  {
    "objectID": "W4.html#logical-operators",
    "href": "W4.html#logical-operators",
    "title": "W#4 Data import, data wrangling",
    "section": "Logical operators1",
    "text": "Logical operators1\n\n\n\noperator\ndefinition\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\nx & y\nx AND y\n\n\nx | y\nx OR y\n\n\nis.na(x)\ntest if x is NA (misssing data)\n\n\n!is.na(x)\ntest if x is not NA (not missing data)\n\n\nx %in% y\ntest if x is in y (often used for strings)\n\n\n!(x %in% y)\ntest if x is not in y\n\n\n!x\nnot x\n\n\n\nLogical is sometimes called Boolean"
  },
  {
    "objectID": "W4.html#indexing",
    "href": "W4.html#indexing",
    "title": "W#4 Data import, data wrangling",
    "section": "Indexing",
    "text": "Indexing\nSelect and filter can also be achieved by indexing.\nIn R as well as in python.\nSelect ranges of rows and columns\n\nhotels[1:3,5:7]\n\n\n\n# A tibble: 3 × 3\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month\n  <chr>                                 <dbl>                     <dbl>\n1 July                                     27                         1\n2 July                                     27                         1\n3 July                                     27                         1\n\n\nYou can use any vector (with non-overshooting indexes)\n\nhotels[c(1:3,100232),c(5:7,1)]\n\n\n\n# A tibble: 4 × 4\n  arrival_date_month arrival_date_week_number arrival_date_day_of_month hotel   \n  <chr>                                 <dbl>                     <dbl> <chr>   \n1 July                                     27                         1 Resort …\n2 July                                     27                         1 Resort …\n3 July                                     27                         1 Resort …\n4 October                                  44                        23 City Ho…"
  },
  {
    "objectID": "W4.html#python-is-0-indexed-r-is-1-indexed",
    "href": "W4.html#python-is-0-indexed-r-is-1-indexed",
    "title": "W#4 Data import, data wrangling",
    "section": "python is 0-indexed, R is 1-indexed!",
    "text": "python is 0-indexed, R is 1-indexed!\npython: indexes go from 0 to n-1\nR: indexes go from 1 to n\nBe aware!\nThere is no correct way. For some use cases one is more natural for others the other.\nAnalog: In mathematics there is an unsettled debate if 0 is the first natural number or 1"
  },
  {
    "objectID": "W4.html#logical-indexing-with-logical-vectors",
    "href": "W4.html#logical-indexing-with-logical-vectors",
    "title": "W#4 Data import, data wrangling",
    "section": "Logical indexing with logical vectors",
    "text": "Logical indexing with logical vectors\n\n\ndata <- tibble(x = LETTERS[1:5], y = letters[6:10])\ndata\n\n\n# A tibble: 5 × 2\n  x     y    \n  <chr> <chr>\n1 A     f    \n2 B     g    \n3 C     h    \n4 D     i    \n5 E     j    \n\n\n\n\n\n\ndata[c(TRUE,FALSE,TRUE,FALSE,TRUE),c(TRUE,FALSE)]\n\n\n# A tibble: 3 × 1\n  x    \n  <chr>\n1 A    \n2 C    \n3 E"
  },
  {
    "objectID": "W4.html#logical-vectors-from-conditional-statements",
    "href": "W4.html#logical-vectors-from-conditional-statements",
    "title": "W#4 Data import, data wrangling",
    "section": "Logical vectors from conditional statements",
    "text": "Logical vectors from conditional statements\n\n\ndata$x\n\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n\n\n\n\n\ndata$x %in% c(\"C\",\"E\")\n\n\n[1] FALSE FALSE  TRUE FALSE  TRUE\n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\"),]\n\n\n# A tibble: 2 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 E     j    \n\n\n\n\n\n\n\ndata[data$x %in% c(\"C\",\"E\") | \n       data$y %in% c(\"h\",\"i\"),]\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j    \n\n\n\n\n\n\n\ndata |> \n  filter(\n    x %in% c(\"C\",\"E\") | y %in% c(\"h\",\"i\")\n    )\n\n\n# A tibble: 3 × 2\n  x     y    \n  <chr> <chr>\n1 C     h    \n2 D     i    \n3 E     j"
  },
  {
    "objectID": "W4.html#unique-combinations-arranging",
    "href": "W4.html#unique-combinations-arranging",
    "title": "W#4 Data import, data wrangling",
    "section": "Unique combinations, arranging",
    "text": "Unique combinations, arranging\ndistinct and arrange\n\nhotels |> \n  distinct(hotel, market_segment) |> \n  arrange(hotel, market_segment)\n\n\n\n# A tibble: 14 × 2\n   hotel        market_segment\n   <chr>        <chr>         \n 1 City Hotel   Aviation      \n 2 City Hotel   Complementary \n 3 City Hotel   Corporate     \n 4 City Hotel   Direct        \n 5 City Hotel   Groups        \n 6 City Hotel   Offline TA/TO \n 7 City Hotel   Online TA     \n 8 City Hotel   Undefined     \n 9 Resort Hotel Complementary \n10 Resort Hotel Corporate     \n11 Resort Hotel Direct        \n12 Resort Hotel Groups        \n13 Resort Hotel Offline TA/TO \n14 Resort Hotel Online TA"
  },
  {
    "objectID": "W4.html#counting",
    "href": "W4.html#counting",
    "title": "W#4 Data import, data wrangling",
    "section": "Counting",
    "text": "Counting\ncount\n\nhotels |> \n  count(hotel, market_segment) |>      # This produces a new variable n\n  arrange(n)\n\n\n\n# A tibble: 14 × 3\n   hotel        market_segment     n\n   <chr>        <chr>          <int>\n 1 City Hotel   Undefined          2\n 2 Resort Hotel Complementary    201\n 3 City Hotel   Aviation         237\n 4 City Hotel   Complementary    542\n 5 Resort Hotel Corporate       2309\n 6 City Hotel   Corporate       2986\n 7 Resort Hotel Groups          5836\n 8 City Hotel   Direct          6093\n 9 Resort Hotel Direct          6513\n10 Resort Hotel Offline TA/TO   7472\n11 City Hotel   Groups         13975\n12 City Hotel   Offline TA/TO  16747\n13 Resort Hotel Online TA      17729\n14 City Hotel   Online TA      38748"
  },
  {
    "objectID": "W4.html#create-a-new-variable-with-mutate",
    "href": "W4.html#create-a-new-variable-with-mutate",
    "title": "W#4 Data import, data wrangling",
    "section": "Create a new variable with mutate",
    "text": "Create a new variable with mutate\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  select(children, babies, little_ones) |>\n  arrange(desc(little_ones)) # This sorts in descending order. See the big thing!\n\n\n\n# A tibble: 119,390 × 3\n   children babies little_ones\n      <dbl>  <dbl>       <dbl>\n 1       10      0          10\n 2        0     10          10\n 3        0      9           9\n 4        2      1           3\n 5        2      1           3\n 6        2      1           3\n 7        3      0           3\n 8        2      1           3\n 9        2      1           3\n10        3      0           3\n# … with 119,380 more rows"
  },
  {
    "objectID": "W4.html#more-mutating",
    "href": "W4.html#more-mutating",
    "title": "W#4 Data import, data wrangling",
    "section": "More mutating",
    "text": "More mutating\n\nhotels |>\n  mutate(little_ones = children + babies) |>\n  count(hotel, little_ones) |>\n  mutate(prop = n / sum(n))\n\n\n\n# A tibble: 12 × 4\n   hotel        little_ones     n       prop\n   <chr>              <dbl> <int>      <dbl>\n 1 City Hotel             0 73923 0.619     \n 2 City Hotel             1  3263 0.0273    \n 3 City Hotel             2  2056 0.0172    \n 4 City Hotel             3    82 0.000687  \n 5 City Hotel             9     1 0.00000838\n 6 City Hotel            10     1 0.00000838\n 7 City Hotel            NA     4 0.0000335 \n 8 Resort Hotel           0 36131 0.303     \n 9 Resort Hotel           1  2183 0.0183    \n10 Resort Hotel           2  1716 0.0144    \n11 Resort Hotel           3    29 0.000243  \n12 Resort Hotel          10     1 0.00000838"
  },
  {
    "objectID": "W4.html#summarizing",
    "href": "W4.html#summarizing",
    "title": "W#4 Data import, data wrangling",
    "section": "Summarizing",
    "text": "Summarizing\n\nhotels |>\n  summarize(mean_adr = mean(adr))\n\n\n\n# A tibble: 1 × 1\n  mean_adr\n     <dbl>\n1     102.\n\n\n\nThat shrinks the data frame to one row!\nDon’t forget to name the new variable (here mean_adr)\nYou can use any function you can apply to a vector!\n(Sometimes you may need to write your own one.)\n\n\n\nIn hoteling, ADR is the average daily rate, the average daily rental income per paid occupied room. A performce indicator."
  },
  {
    "objectID": "W4.html#grouped-operations",
    "href": "W4.html#grouped-operations",
    "title": "W#4 Data import, data wrangling",
    "section": "Grouped operations",
    "text": "Grouped operations\n\nhotels |>\n  group_by(hotel) |>\n  summarise(mean_adr = mean(adr))\n\n\n\n# A tibble: 2 × 2\n  hotel        mean_adr\n  <chr>           <dbl>\n1 City Hotel      105. \n2 Resort Hotel     95.0\n\n\nLook at the grouping attributes:\n\nhotels |>\n  group_by(hotel)\n\n\n\n# A tibble: 119,390 × 32\n# Groups:   hotel [2]\n   hotel  is_ca…¹ lead_…² arriv…³ arriv…⁴ arriv…⁵ arriv…⁶ stays…⁷ stays…⁸ adults\n   <chr>    <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Resor…       0     342    2015 July         27       1       0       0      2\n 2 Resor…       0     737    2015 July         27       1       0       0      2\n 3 Resor…       0       7    2015 July         27       1       0       1      1\n 4 Resor…       0      13    2015 July         27       1       0       1      1\n 5 Resor…       0      14    2015 July         27       1       0       2      2\n 6 Resor…       0      14    2015 July         27       1       0       2      2\n 7 Resor…       0       0    2015 July         27       1       0       2      2\n 8 Resor…       0       9    2015 July         27       1       0       2      2\n 9 Resor…       1      85    2015 July         27       1       0       3      2\n10 Resor…       1      75    2015 July         27       1       0       3      2\n# … with 119,380 more rows, 22 more variables: children <dbl>, babies <dbl>,\n#   meal <chr>, country <chr>, market_segment <chr>,\n#   distribution_channel <chr>, is_repeated_guest <dbl>,\n#   previous_cancellations <dbl>, previous_bookings_not_canceled <dbl>,\n#   reserved_room_type <chr>, assigned_room_type <chr>, booking_changes <dbl>,\n#   deposit_type <chr>, agent <chr>, company <chr>, days_in_waiting_list <dbl>,\n#   customer_type <chr>, adr <dbl>, required_car_parking_spaces <dbl>, …"
  },
  {
    "objectID": "W4.html#grouping-summarizing-visualizing",
    "href": "W4.html#grouping-summarizing-visualizing",
    "title": "W#4 Data import, data wrangling",
    "section": "Grouping, summarizing, visualizing",
    "text": "Grouping, summarizing, visualizing\n\nhotels |>\n  group_by(hotel, arrival_date_week_number) |>\n  summarise(mean_adr = mean(adr)) |> \n  ggplot(aes(x = arrival_date_week_number, y = mean_adr, color = hotel)) +\n  geom_line()"
  },
  {
    "objectID": "W4.html#resources",
    "href": "W4.html#resources",
    "title": "W#4 Data import, data wrangling",
    "section": "Resources",
    "text": "Resources\n\nFor systemic understanding: Learning resources linked in the syllabus\n\nR for Data Science\n\nChapters 3, 5, 9 (short), 10 (short), 11\n\nCorresponding chapters in Python Data Science Handbook\n\nFor quick overview to get inpiration\n\nCheatsheets (find some in RStudio -> Help, other by google)\n\nggplot2 Cheatsheet\ndplyr Cheatsheet\n\n\nFor detailed help with a function\n\nHelp file of the function ?FUNCTION-NAME, or search box in Help tab\nReference page on the package webpage"
  },
  {
    "objectID": "W4.html#questions-and-advice-for-the-homework-of-starting-projects",
    "href": "W4.html#questions-and-advice-for-the-homework-of-starting-projects",
    "title": "W#4 Data import, data wrangling",
    "section": "Questions and advice for the homework of starting projects",
    "text": "Questions and advice for the homework of starting projects\n\nData search\n\nAny insights about corona data?\n\nStarting a new quarto markdown document\n\nWhat to write into the YAML?\n\nData import\n\nExpect that some customization is needed!\n\nFirst graph\n\nYou are encouraged to play further!\nThese projects can be the seed for you Data Science Tools module project.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html",
    "href": "hw-instructions/hw-04-instr.html",
    "title": "Homework 04",
    "section": "",
    "text": "Important\n\n\n\nHomework 04 is due Sunday, Nov 6. When you get stuck, you are encouraged to push intermediate steps before you contact us.\nProject teams should be formed by Oct 30.\nIn this homework you are to continue to do data analysis of the data with the data about corona in R and the European Social Survey (ESS) in python."
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html#corona",
    "href": "hw-instructions/hw-04-instr.html#corona",
    "title": "Homework 04",
    "section": "1 Corona",
    "text": "1 Corona\nContinue to work on your repository corona-ind-USERNAME.\nContinue to work on the Quarto-document “Corona_Analysis.qmd” from Homework 03 and add new headlines and new code chunks for each new question.\n\n1.1 Question 5: Do the cumulative cases reported by the WHO for Germany, France, India and the country of your choice coincide with the cumulative sum of new cases?\n\nFilter the who dataset for the four countries. Group by these countries and compute a new variable called Total_cases as the cumulative sum (cumsum) of the new cases for each each country.\nNow, test if this variable coincides with the variable Cumulative_cases which is already present in the dataset. To that end, compute a new variable with the difference of the two time lines and check if the difference is zero in every time step. (You can use count on the the difference variable to count different values.)\nWrite down the answer to the question.\n\n\n\n1.2 Question 6: What can the visualization of the daily change of new cases in Germany tell us about the evolution of the pandemic?\n\n\n\n\n\n\nTip\n\n\n\nDownload the two datasets (who and owid) anew to get the most recent values.\n\n\n\nUse the dataset of the WHO, filter for Germany and all dates after August 15, 2022.\nMake a new variable New_cases_smoothed where you smooth New_cases with a 7-day lagged window.\nCreate another variable called Change which is the difference of in the smoothed new cases with the day before.\nDevelop two ggplots (see below), one for the smoothed new cases and one for the change. Once you have the code for each plot save each plot as a ggplot object (e.g., g1 and g2) and show them in your rendered document with new cases above the change. (Hint: Use the package patchwork and g1 / g2 to show the two plots.)\n\nUse a bar-chart for both plots. Use geom_col.\nFor the plot of the change make the bars for negative numbers filled with blue and the bars for positive numbers filled with red. The way to do it is to add two geom_cols to the same plot. Both have different y-aesthetics but both based on the Change variable. In one the negative values are set to zero in the other the positive values. For example you can create such a variable like ifelse(Change >= 0, Change, 0).\nLet us make these plots look nice. Use + labs() to create meaningful axis labels. Use scale_x_date to make the x-axis-ticks look nice. For example, use date_breaks = ... to specify the main labelled ticks, date_minor_breaks = ... for smaller unlabelled ticks, and use date_labels = ... to make the labels shorter by omitting the year in the dates. You have to look up the specification in the help of scale_x_date.\n\nDescribe what the blue and the red regions in the change plot tell us about the recent evolution of corona cases in Germany.\nFrom the total case, the new cases and the change of new cases. Which timeline is the derivative an the anti-derivative of which?\n\n\n\n1.3 Question 7: Which variable explains more variance of total deaths per million, the human development index (HDI) or the median age?\nUse the owid dataset and filter the rows with data Oct 15, 2022 only and with a valid continent (this should be 223 rows).\nEstimate two linear model with this dataset. Both have total_death_per_million as dependent variable.\n\nUse human_development_index as independent variable and save the model object as mod_hdi.\nUse median_age as independent variable and save the model object as mod_age.\n\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html. Answer the question.\n\n\n1.4 Question 8: How do the two models improve when the continent is added as an interaction effect?\nEstimate two more linear models, mod_hdicont and mod_agecont, by adding continent as an interaction effect to the two models from Question 8 (human_development_index * continent and median_age * continent).\n\nPrint the coefficients of mod_agecont in tidy form and interpret the coefficient of the intercept, the main effects, and the interaction effects.\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html.\n\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html. Answer the question. In which case is the improvement stronger? Why is the improvement different?\n\n\n1.5 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html#european-social-survey",
    "href": "hw-instructions/hw-04-instr.html#european-social-survey",
    "title": "Homework 04",
    "section": "2 European Social Survey",
    "text": "2 European Social Survey\nContinue to work on your repository ess-ind-USERNAME. You have to look ESS Data Documentation https://ess-search.nsd.no/.\nContinue to work on the Quarto-document “ESS-analysis.qmd” (alternatively “ESS-analysis.ipynb” if you can render html-files from it) from Homework 03 and add new headlines and new code chunks for each new question.\n\n2.1 Question 6: What are the correlations between life satisfaction, trust in the police, religiosity, emotional attachment to Europe, and social activity?\nFilter the ess dataset for essround 9 and select the variables stflife, trstplc, rlgdgr, atcherp, and sclact.\nCompute the correlation matrix and visualize correlation coefficients with color.\nFor calculating the correlation of a dataframe use .corr() which returns the correlation matrix. Visualizing a value with a certain color is called heatmap. Use your created correlation matrix in the seaborn function sns.heatmap(). Look up the function and vary the parameters for better visibility.\nExplain the two highest and two lowest correlations.\n\n\n2.2 Question 7: Which variables can explain the variance of life satisfaction in a linear model how well?\nWith the same dataset compute a linear model where life satisfaction is explained as a linear combination of all other four variables.\nPrint and interpret the coefficients. (Look up and report the variable descriptions and their scales at https://ess-search.nsd.no for this purpose!)\nThere are many python packages that include linear models. For similarity with R we use the (statsmodel package)[https://www.statsmodels.org/stable/index.html].\nimport statsmodel.api as sm\nThe default model within statsmodel does not have an intercept defined. Therefore, add a constant to the predictors with sm.add_constant(). Now apply the sm.OLS() to your data with the predictors and dependent variable inside the function. The results of your regression are summarized by applying and printing the results.summary() function.\nWhich percentage of the variance in life satisfaction can be explained with the model?\n\n\n2.3 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-04-instr.html#your-own-project",
    "href": "hw-instructions/hw-04-instr.html#your-own-project",
    "title": "Homework 04",
    "section": "3 Your own project",
    "text": "3 Your own project\nThe assessment of the Data Science Tools module will be based on a team project report.\n\n3.1 A project report in a nutshell\n\nYou pick a dataset,\ndo some interesting question-driven data analysis with it,\nwrite up well structured and nicely formatted report about the analysis, and\npresent it at the end of the semester.\n\nMore details will come soon.\n\n\n3.2 The tasks for this Homework\n\nTeam formation such that a repository at https://github.com/orgs/JU-F22-MDSSB-MET-01 can be provided.\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n\n\n\n\n\n\nTask 1: Form project teams by Sunday, Oct 30.\n\n\n\n\nTeams should have two members.\nSend your names to Jan Lorenz by email or via Teams.\nTeams of three (or more) or individual projects can be permitted on requests providing a reason.\n\n\n\n\n\n\n\n\n\nTask 2: Start a document and list data source/topic and draft questions\n\n\n\n\nYou need your repository in https://github.com/orgs/JU-F22-MDSSB-MET-01 to submit it. Please form your team early and inform us such that it can be created.\nStart a quarto markdown document in the repository. The text can be brief at this stage.\n\n\n\n\n\n3.3 Where to find data?\nThis up to you. Some entry points are:\n\nGoogle Dataset Search https://datasetsearch.research.google.com/\n\nTidy Tuesday https://github.com/rfordatascience/tidytuesday\nAwesome public datasets https://github.com/awesomedata/awesome-public-datasets\nKaggle datasets https://www.kaggle.com/datasets\nJan and Martin can provide ideas for data about polarization in Europe (based on ESS), a voting advice application for German elections, census data about Bremen’s districts.\n\nIf you have a specific topic in mind which can be approached with data but haven’t found a good dataset yet, you can also provide the topic and the questions which interest you only at this stage."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html",
    "href": "hw-instructions/hw-01-instr.html",
    "title": "Homework 01",
    "section": "",
    "text": "Note\n\n\n\nUpdated Sep 7, 2022 to prevent problems with cloning your repository.\nSep 9, 2022: Updated the individual repositories on with an updated dataset. All which have already cloned it should do a “Pull” in the Git tab of RStudio.\nThe goal of this assignment is to introduce you to R, RStudio, Git, and GitHub, which you’ll be using throughout the course both to learn the data science concepts discussed in the course and to analyze real data and come to informed conclusions."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#prerequisites",
    "href": "hw-instructions/hw-01-instr.html#prerequisites",
    "title": "Homework 01",
    "section": "1.1 Prerequisites",
    "text": "1.1 Prerequisites\nThis assignment assumes that you have reviewed the lectures of week 1 and checked all boxes on the Prerequisites Checklist."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#terminology",
    "href": "hw-instructions/hw-01-instr.html#terminology",
    "title": "Homework 01",
    "section": "1.2 Terminology",
    "text": "1.2 Terminology\nWe’ve already thrown around a few new terms, so let’s define them before we proceed.\n\nR and python: Names of the programming language we will be using throughout the course.\nRStudio: An integrated development environment developed for R, which can also work with python. In other words, a convenient interface for writing and running code.\nGit: A version control system.\nGitHub: A web platform for hosting version controlled files and facilitating collaboration among users.\nRepository: A Git repository contains all of your project’s files and stores each file’s revision history.\n\nIt’s common to refer to a repository as a repo.\n\nIn this course, each assignment you work on will be contained in a Git repo.\nFor individual assignments, only you will have access to the repo. For team assignments, all team members will have access to a single repo where they work collaboratively.\nAll repos associated with this course are housed in the course GitHub organization. The organization is set up such that students can only see repos they have access to, but the course instructors can see all of them."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#starting-slowly-step-by-step",
    "href": "hw-instructions/hw-01-instr.html#starting-slowly-step-by-step",
    "title": "Homework 01",
    "section": "1.3 Starting slowly step by step",
    "text": "1.3 Starting slowly step by step\nAs the course progresses, you are encouraged to explore beyond what the assignments dictate; a willingness to experiment will make you a much better programmer! Before we get to that stage, however, you need to build some basic fluency in the tools and the workflow we use. First, we will explore the fundamental building blocks of all of these tools.\nBefore you can get started with the analysis, you need to make sure you:\n\nhave a GitHub account\nare a member of the course GitHub organization https://github.com/JU-F22-MDSSB-MET-01\nhave the needed software stack installation on your local machine (see the Prerequisites Checklist in the slides of the Week 1 lectures.)\n\nIf you failed to confirm any of these, it means you have not yet completed the prerequisites for this assignment. Please go back to Prerequisites and complete them before continuing the assignment."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-0.-authenticate-git-to-access-your-github-content",
    "href": "hw-instructions/hw-01-instr.html#step-0.-authenticate-git-to-access-your-github-content",
    "title": "Homework 01",
    "section": "2.1 Step 0. Authenticate git to access your GitHub content",
    "text": "2.1 Step 0. Authenticate git to access your GitHub content\nBefore you can clone your repository you need to tell GitHub that you are authorized to do this, and to that end you need to make a Personal Access Token (PAT) in your GitHub account and make this available to git and RStudio on your local machine.\nThere are several ways to do this (e.g. from the command line) but as we will use RStudio anyway, we can use a convenient way provided there.\nRead more about PATs and how to use them in “Happy Git with R” Chapter 9 https://happygitwithr.com/https-pat.html (in particular the TL;DR which describes what we use).\nOpen RStudio and install the packages usethis and gitcreds if you haven’t done already: Go to the “Console” pane at the bottom left. Type in\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nand hit Enter. Now the packages should be installed.\nNow, use two commands. Copy them to the console and click Enter:\nusethis::create_github_token()\nThis opens http://github.com and you may need to log in. Then you can make the PAT (read more details in “Happy Git with R”). For today, you can go the fast way and do not think about the options and click “Generate token”. Use the clipboard icon 📋 to copy the PAT. Go back to RStudio and do in the console:\ngitcreds::gitcreds_set()\nIn the dialog in the console paste your PAT from the clipboard and press Enter. That should be it and you do not need to repeat these steps until the PAT expires. (If the PAT expires you have to make a new one in the same way.)"
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-1.-get-url-of-repo-to-be-cloned",
    "href": "hw-instructions/hw-01-instr.html#step-1.-get-url-of-repo-to-be-cloned",
    "title": "Homework 01",
    "section": "2.2 Step 1. Get URL of repo to be cloned",
    "text": "2.2 Step 1. Get URL of repo to be cloned\nOn GitHub https://github.com/JU-F22-MDSSB-MET-01, open your repository for Homework 1.\n\nOn GitHub, click on the green Code button, select HTTPS (this might already be selected by default, and if it is, you’ll see the text Use Git or checkout with SVN using the web URL). Click on the clipboard icon 📋 to copy the repo URL."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-2.-go-to-rstudio",
    "href": "hw-instructions/hw-01-instr.html#step-2.-go-to-rstudio",
    "title": "Homework 01",
    "section": "2.3 Step 2. Go to RStudio",
    "text": "2.3 Step 2. Go to RStudio\nGo to your RStudio. Select “New Project…” either from the File menu or from the special project menu on the top right of the RStudio window.\n\nIn the New Project Wizard, click on “Version Control” and then “Git”.\n\n\n\n\n\n\nImportant\n\n\n\nIf “Version Control” or “Git” is not available in your RStudio, then either you haven’t installed git on you computer or your RStudio installation has not recognized it properly. In a correct installation RStudio would recognize git on your machine when started and makes the options available automatically. You have to solve this issue first to continue.\n\n\nThen paste the repositories URL (which should still be in your clipboard) into the “Repository URL:” field. Leave directory name as it is automatically chosen, but make sure that you create the directory in a the folder where you want to store the course material on your computer via the “Browse …” button.\nWhen you click “Create Project”\n\ngit will create a new directory in the folder, copies all the files from github to it, and initializes your git repository locally\nRStudio will switch to that the new project"
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-1.-update-the-yaml",
    "href": "hw-instructions/hw-01-instr.html#step-1.-update-the-yaml",
    "title": "Homework 01",
    "section": "4.1 Step 1. Update the YAML",
    "text": "4.1 Step 1. Update the YAML\nOpen the quarto (qmd) file in your project, change the author name to your name, and “Render” the document.\n This calls quarto, and quarto renders the document. In this case, that means, quarto creates a new file “hw-01-R.md” in the gfm format as specified in the YAML. GFM stands for GitHub Flavored Markdown which is a markdown document which can be nicely shown on https://github.com.\nWhen the file was rendered successfully, RStudio shows it in the “Viewer” pane at the bottom right. At the same place you can look in the “Files” pane you can check if the file is there.\n\nIf you do not find the “Render” button in your RStudio installation, then either quarto is not installed or RStudio has not recognized. You have to fix this issue first before you can continue. Another source of error while rendering could be that you haven’t installed the tidyverse package."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-2-commit",
    "href": "hw-instructions/hw-01-instr.html#step-2-commit",
    "title": "Homework 01",
    "section": "4.2 Step 2: Commit",
    "text": "4.2 Step 2: Commit\nGo to the Git pane in your RStudio (top right corner).\nYou should see that your qmd (quarto) file and its output, your md file (Markdown), are listed there as recently changed files.\nNext, click on Diff. This will pop open a new window that shows you the difference between the last committed state of the document and its current state that includes your changes. (Click on the file “hw-01-R.qmd”.) If you’re happy with these changes, click on the checkboxes of all files in the list, and type “Update author name” in the Commit message box and hit Commit and then close the window.\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we may tell you exactly when to commit and what commit message to use. As the semester progresses we will let you make these decisions."
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#step-3.-push",
    "href": "hw-instructions/hw-01-instr.html#step-3.-push",
    "title": "Homework 01",
    "section": "4.3 Step 3. Push",
    "text": "4.3 Step 3. Push\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course instructors (your repos in this course are private to you and us, only).\nGo the Git pane and click on Push.\nThought exercise: Which of the steps “updating the YAML”, “committing”, and “pushing” involves talking to GitHub?1"
  },
  {
    "objectID": "hw-instructions/hw-01-instr.html#check-what-you-did",
    "href": "hw-instructions/hw-01-instr.html#check-what-you-did",
    "title": "Homework 01",
    "section": "4.4 Check what you did",
    "text": "4.4 Check what you did\nGo to your repository on https://github.com/JU-F22-MDSSB-MET-01 and click on the file “hw-01-R.md”. You should see a nicely rendered version with your name in the header."
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html",
    "href": "hw-instructions/hw-02-instr.html",
    "title": "Homework 02",
    "section": "",
    "text": "The goals of this assignment are\nTo that end, we provide three repositories with starter documents which you should clone and push your solutions to:"
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html#data-overview",
    "href": "hw-instructions/hw-02-instr.html#data-overview",
    "title": "Homework 02",
    "section": "1.1 Data overview",
    "text": "1.1 Data overview\n\n\n\n\n\n\nNote\n\n\n\nFirst do the following exercises in R in hw-02-R.qmd. Afterwards go through hw-02-py.qmd. This documents contains some tips how to do the same operations in python. Exercises which are not about coding are omitted there.\n\n\n\nRun the first chunk packages-data such that the lines are executed in the console to load the tidyverse functions and the ny3cflight13 data. See which dataframes are available in the Environment tab. The environment should be empty, but you can select “package:nycflights13” instead of “Global Environment” and you see values markes as <Promise>. Once you click on one or call it in the Console you see basic information there.\nReplace the “??” in the text with the actual numbers. Don’t write the numbers your self, but write inline code in which you output the number of rows of each data frame. In the document this already done for the airlines. Render the document to see if it works. More information https://quarto.org/docs/get-started/computations/rstudio.html#inline-code\nExplore the datasets. Write View(flights) in the Console to see the data as a spreadsheet. Write glimpse(flight) to see an overview of all variables, their types and first values. As these dataframes come from a package there is also a Help page for each dataframe which you access with ?flights. There you find short descriptions about each variable.\n\nWrite a nicely formatted short description about the variables origin, arr_delay, and dep_delay from flights and engine and seat from planes."
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html#data-visualizations",
    "href": "hw-instructions/hw-02-instr.html#data-visualizations",
    "title": "Homework 02",
    "section": "1.2 Data visualizations",
    "text": "1.2 Data visualizations\n\nWe first want to know the distribution of values of the categorical variable origin in flights. To that end, make a bar chart. Read the Help ?geom_bar and decide if you need to use geom_bar or geom_col. You can use the template below. Write your solution in the chunk flightsorigin. Test your line by sending it to the Console (with Ctrl + Enter). Once you are satisfied, render the document, commit the changes with message “First Visualization!” and push it. In the following, you can commit and push when you want. (Note, that we can provide help directly in your repo when you commit and push before.) This is the template.\n\nggplot(data = __________, mapping = aes(x = ________)) + \n  geom_TOSELECT()\n\nNow, we want to know the distribution of values of the numerical variable distance in flights. A common visualization is a histogram. Use geom-histogram with the same template, write the solution in the chunk flightsdistance, and test it. Notice, the red comment in the console. It advises to specify a binwidth. Test binwidth = 5, binwidth = 50, and binwidth = 500 in geom_histogram, notice the difference (consult ?geom_histogram) for details, and decide which shows the distribution best.\nIn chunk distributions you see two ways to visualize the distribution of the number of seats in airplanes - points for each observation and a boxplot. (Read ?geom_boxplot for more information). Note, that there are three ggplot objects (g1, g2, g3) which are shown combined with g1 + g2 + g3 (using the patchwork package). Make the empty g3 into a vertical histogram for the same data following the exercise before. Hint: For the vertical histogram assign distance to the y aesthetic and leave out the x aesthetic. Think about the advantages and disadvantages of each visualization.\nNow, we make the first plot which visualizes two variables, the categorical variable engine, and the numerical variable seats in airplanes. Use the template and with aesthetics x and y and the geom_boxplot and put the solution into the chunk engine-seats\nTwo numerical variables can be visualized with a scatter plot using geom_point and the aesthetics x and y. Let us look at dep_delay and arr_delay of flights. Warning: The flights is very large! So, do not use flights in data = _____ but a random sample of 10,000 flights sample_n(flights, 10000). Now, let us add information about the categorical variable origin and assign it to the color aesthetic. Put your solution into the chunk delays. Test your solution several times and observe the changes in the visualization because of the random sampling. In which region are the changes most substantial? (This is a qualitative judgment.)\nFinally, let us visualize the location of airports as points at their longitude and latitude (look up the variable names) and color them with the timezone tzone they are in. Put the solution into the chunk airportlocations."
  },
  {
    "objectID": "hw-instructions/hw-02-instr.html#data-wrangling-pipes-and-visualization",
    "href": "hw-instructions/hw-02-instr.html#data-wrangling-pipes-and-visualization",
    "title": "Homework 02",
    "section": "1.3 Data wrangling, pipes, and visualization",
    "text": "1.3 Data wrangling, pipes, and visualization\nIn the following, you have to solve some data wrangling tasks. For data wrangling, the usage of the pipe, or a chain of pipes, is convenient. You can also use the pipe to finish with a visualization.\n\nPut the code snippet below into the chunk flightsaveragespeed and test it. The mutate line makes a new variable called speed which is the distance of the flight divided by the time in the air. The select line selects variables from the dataset. In this case, it selects air_time as the first, distance as the second, and the new speed as the third variable. All other variables are dropped.\nThe values in the new speed variable do not look like speeds of airplanes in km/h. Why? Because they are in miles/minute which we know from the variable descriptions. Modify the equation in the mutate command such that the values are in km/h. To that end, you have to divide air time by 60 and multiply distance by a certain factor. Look up the factor. Be careful with the order of mathematical operations and maybe use brackets (). Test your computation. Are the speed values reasonable?\nNow, make a histogram of speed. Add another pipe after the select statement and write ggplot(mapping = aes()) in the next line. Note, that you should not put data = flights into the argument of ggplot()! It is the mission of the pipe to do this. Fill out the aes() command accordingly, and add the geom for a histogram.\n\nflights |> \n  mutate(speed = distance / air_time ) |> \n  select(air_time, distance, speed)\n\nPractice filter operations, which subsets certain observations of a dataframe. Write a line which filters the flights which\n\n\nhad an arrival delay of two or more hours\nflew to Houston (IAH or HOU)\narrived more than two hours late, but didn’t leave late\nstarted with a delay of at least an hour, but made up over 30 minutes in flight. Put all four lines into the chunk filtering.\n\n\nAnother common operation is summarizing data. Put the code below into the chunk summarizing and test it. You see the average delay at departure. (See ?mean to learn what na.rm = TRUE is doing). Now, we want to know the average delay at departue for each of the three airports of origin. This is done with a group_by applied to the flights. Find out how and modify the code in the chunk accordingly.\n\nflights |> \n  summarize(mean_dep_deplay = mean(dep_delay, na.rm = TRUE))\n\nFinally, look at the plot of airportlocations. The airport locations show the shape of the United States of America, but there are four airports on the right hand side which do not fit that pattern. Filter airports such that you only see these four airport. Write your solution into the chunk stangeairportlocations. Check with internet research where these airports are located. Why are the locations from the data as they are? List your hypotheses for each airport under the chunk."
  },
  {
    "objectID": "hw-instructions/hw-02-py-instruction.html",
    "href": "hw-instructions/hw-02-py-instruction.html",
    "title": "Additional python instruction Homework 02",
    "section": "",
    "text": "Note:\n\nRead and do the homework in R first. There you find more instructions and exercises.\nYou can also copy each chunk into a notebook cell within jupyter notebook template file and work with the .ipynb file\nquarto can also render .ipynb files. (Actually quarto converts .qmd files into .ipynb files and renders them afterwards)\nThere is also a .ipynb Template in the python Codebase provided.\n\nThis analysis works with datasets in the package nycflights13 about flights, specifically a sample of domestic flights that departed from the three major New York City airport in 2013.\n\n1 Data overview\n#| label: packages-data\n#| message: false  # We do not want to see the common tidyverse message in our document\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nycflights13 import flights, planes,airports\nRun the first chunk to import the packages, by running the command as:\n\nfrom nycflights import …\n\nYou can deliberately choose which dataset you want to import from nycflights13. The nameing convension seems to be identical with the dataset in R.\n#| output: false\nflights.info()\nExplore the datasets. If you are running the code in VSCode you can see the variable in the “JUPYTER: VARIABLES” window. Write flights.info() in the Console or in a chunk and remove it afterwards to see an overview of all variables, their types. (Write a short note about the differences to the R glimpse command)\n\n\n2 Whats the differences between R:glimpse and pandas:info\n…\n\n\n3 Data visualization\n\nWe first want to know the distribution of values of the categorical variable origin in flights. To that end, make a bar chart. Use the seaborn command displot.\n\n\nsns.displot(data=flights,x = .. y= ..,height=.., aspect=..)\ngeom_bar or geom_col is just a variation between the usage of variable x and y. Use height=.., aspect=.. to adjust the size of the plot.\n\nNow, we want to know the distribution of values of the numerical variable distance in flights. A common visualization is a histogram. Use sns.histplot with the same template, write the solution in the chunk flightsdistance, and test it. The command binwidth is equivalent to bin-width in R. Use it inside the seaborn function.\n\nfig,ax = plt.subplots(figsize=(12,6)) is used to vary the size of the plot.\nfig,ax = plt.subplots(figsize=(12,6))\nsns.histplot(data=flights,x='distance',ax=ax, ...)\nIn chunk distributions you see two ways to visualize the distribution of the number of seats in airplanes - points for each observation and a boxplot. Different to ggplot you define the output first. With ncols= .. ,nrows =.. you specify the layout and access it with  ax  similar to an array.\n#| label: distributions\nfig,ax = plt.subplots(nrows=1,ncols=3,figsize=(8,6))\n#fig,[ax1,ax2,ax3] = plt.subplots(nrows=1,ncols=3,figsize=(24,12)) ## Or define the array elements\nax[0].plot(np.zeros(len(planes)),planes['seats'],'k.')\nsns.boxplot(data=planes,y='seats',\n    boxprops={'facecolor':'None'},\n    ax=ax[1])\n#sns.boxplot(data=planes\n    #boxprops={'facecolor':'None'},\n    #ax=..)\n    # )\nNow, we make the first plot in python which visualizes two variables, the categorical variable engine, and the numerical variable seats in airplanes\n#| label: engine-seats\nfig,ax = plt.subplots(figsize=(24,12))\n\nsns.boxplot(data=planes,x = .. , y = ..\n    boxprops={'facecolor':'None'},\n    ax=ax)\nSo, do not use flights in data = .. but a random sample of 10,000 flights flights.sample(n=10000). To visualize a scatterplot use sns.scatterplot(). To add an information about the categorical variable origin and assign it to the color aesthetica variation, include  ..,hue= .. , in the scatterplot command.\n#| label: delays\nfig,ax = plt.subplots(figsize=(12,10))\nsns.scatterplot(data= flights.., x='dep_delay',y='arr_delay',..,ax=ax)\nFinally, let us visualize the location of airports as points at their longitude and latitude (look up the variable names) and color them with the timezone tzone they are in. Put the solution into the chunk airportlocations.\n#| label: airportlocations\nfig,ax = plt.subplots(figsize=(12,10))\nsns.scatterplot()\n\n\n4 Data Wrangling\nIn the following, you have to solve some data wrangling tasks. For data wrangling, you use different commands onto a DataFrame devided by a “.” . There is not equivalent to “pipes” in the pandas or python programming language. Here the easiest way is to creat a new column in our DataFrame with the average speed in it. The pandas specific .div() command allows for a fast division of two columns. Also modify the equation such that the values are in km/h.\n#| label: flightsaveragespeed\nflights['speed'] = flights['distance'].div(flights['air_time'])\nThe filter function is not available in python. IMHO the .loc function is the most comprehensive function the filter a DataFrame by using logical expressions.\nPractice .loc operations, which subsets certain observations of a dataframe. Write a line which filters the flights which\n\nhad an arrival delay of two or more hours\nflew to Houston (IAH or HOU)\narrived more than two hours late, but didn’t leave late\nstarted with a delay of at least an hour, but made up over 30 minutes in flight.\n\nuse individual lines for each filter. In the end use .describe() on the flights DataFrame. Now, we want to know the average delay at departue for each of the three airports of origin. This is done with a groupby(..) applied to the filtered DataFrame. additionally apply mean() and select the correct column. Find out how and modify the code in the chunk accordingly.\n#| label: flightsaveragespeed\nflights = flights.loc[flights['arr_delay']>=120] # 120 minutes\nNOTE: If you do not choose a new variable name for the DataFrame, data is lost, until you reload the complete DataFrame\nFinally, look at the plot of airportlocations. The airport locations show the shape of the United States of America, but there are four airports on the right hand side which do not fit that pattern. Filter airports such that you only see these four airport. Write your solution into the chunk stangeairportlocations. Check with internet research where these airports are located. Why are the locations from the data as they are? List your hypotheses for each airport under the chunk.\n#| label: strangeairportlocations\nplt.figure(figsize=(20,6))\nplt.plot( airports[..],airports[..] ,'.')"
  },
  {
    "objectID": "hw-instructions/hw-03-py-instruction.html",
    "href": "hw-instructions/hw-03-py-instruction.html",
    "title": "Additional Python Instruction Homework 03",
    "section": "",
    "text": "First we import all the relevant packages. We also need the pycountry package which we can simply install via pip in the command line.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pycountry\nThe next step is to import the Data.\ndf = pd.read_csv(...)\nIf we take a look at the countries within the dataset and we try to convert it via the pycounty package we run into an error. This error is a result of the Countrycode 'XK which refers to Kosovo but is not officially included as ISO alpha_2 code. Therefore we have to do it manually or use the following function:\ndef CountryConversion(x):\n    if x =='XK':  ## Special Case\n        return 'Kosovo'\n    else: ## otherwise use pycountry package\n        return pycountry.countries.get(alpha_2=x).name  \n        # the name is the country name within the database\n\n\n## Python function can also have multiple returns inside a function\n## if logical expressions are used.\nInstead of using the name you can also use the pycountry.countries.get(alpha_2=x).official_name.\nApply (.aaply()) this function to the cntry column and create a new column in the same step. We also need to create the ['Year'] column by using the essround column. Here round 1 is equivalent to 2002, round 2 to 2004, round 3 to 2006 and so on.\n\n\nInstead of recoding number 77, 88 and 99, you can also remove them from the dataset by utilizing loc in conjunction with a logical expression. After that we have to use the groupby function but with our newly created columns. Additionally, we apply .mean() to this pandas Series. To get a DataFrame out of the Series, we have to reset the index by reset_index() You may remember we utilized melt() in our first homework to create a long format of the DataFrame. In this case we want a wide format with the Country as index and the columns referring to the Year. The correct function is pivot(...) which you have to apply. Last but not least you sort the values by .sort_values(by=...)\n\n\n\n\nTo count the number of individuals each combination of valid values of atchctr and atcherp, you have to create a crosstable by applying the following function\n\nvaluetable = pd.crosstab(df['atchctr'],df['atcherp']).reset_index()[['atchctr',  0.0,  1.0,  2.0,  3.0,  4.0, 5.0,  6.0,  7.0,  8.0,  9.0, 10.0]].loc[0:10]\nThe table that is created looks exactly like the plot we want to recreate. But for plotting python and most other programming languages prefer a long data format. To save time and not wrangle with this table we choose a different way. Therefore we simply use the function value_counts() on both columns\nvaluedf = df[['ColA', 'ColB']].value_counts().reset_index()\nvaluedf = df.rename(columns={0:\"NumIndividuals\"})\nTake a look at the data you created.\nFinally we can plot our data with sns.relplot, define the Dataset with data= , the x and y variable x =  ,y = and use size and hue within relplot\n\nFor this we need the orignal data set. To create a lowess plot in python we simply toggle to lowess inside the regplot function with lowess=True. And don’t show the underlying data by setting scatter=False. Beware of the 77 88 and 99 that may still exist within your data\n\nsns.regplot(data=...,....)\nIf you want to have a seperate plot for each country define a facetgrid:\ng = sns.FacetGrid(Dataframe, col=\"Country\", col_wrap=5 )\n## now we can map a function to each plot in this grid\ng.map_dataframe(sns.regplot,x= .. , y= ,.., ..)\nEverything you want to have inside the regplot function you have to write individually after the function inside the map funciton.\n\n\n\nRepeat everything again for euftf\n\n\n\nYou have to again group the data by country-year combination. To get to number of observations the .size() function should be helpful. Again .reset_index() to create a DataFrame. Now you can rename the column 0 to a more suitable name by using a dictionary. Define a dictionary and apply it to rename the column.\nRenameDict = dict({SizeColumnName: 'MoreSuitableName'})\nYourDataFrame = YourDataFrame.rename(columns=RenameDict)\nNow we have to pivot() again to get wide data format. The index= should be the country and the columns= should be the Year."
  },
  {
    "objectID": "hw-instructions/hw-03-instr.html",
    "href": "hw-instructions/hw-03-instr.html",
    "title": "Homework 03",
    "section": "",
    "text": "Important\n\n\n\nHomework 03 is due Oct 16.\nThe goal of this assignment is to do steps of exploratory data analysis (EDA) with the data about corona and the European Social Survey (ESS) from Homework 02."
  },
  {
    "objectID": "hw-instructions/hw-03-instr.html#corona",
    "href": "hw-instructions/hw-03-instr.html#corona",
    "title": "Homework 03",
    "section": "1 Corona",
    "text": "1 Corona\nContinue to work on your repository corona-ind-USERNAME and make the analysis in R-chunks.\nCreate a new Quarto-document and save it as “Corona_Analysis.qmd”.\n\nCustomize the YAML with the title “Corona Analysis”, your name, and output format being a standalone html-file with embedded resources. Enable code-folding in the output html file.\nWrite a chunk labeled data where you load the tidyverse library and import the data from Our World in Data (OWiD) into a tibble owid and the data from the World Health Organization (WHO) into a tibble called who (use read_csv for both).\nDocument what you did: Write a headline ## Data before the chunk data and briefly describe the two data sources and link the data files.\n\n\n\n\n\n\n\nTip\n\n\n\n\nIn https://github.com/JU-F22-MDSSB-MET-01/corona-ind-janlorenz/blob/main/Download_corona_data.R you find a script which downloads the corona datasets from OWiD and WHO. You can copy this.\nIn https://github.com/JU-F22-MDSSB-MET-01/corona-ind-janlorenz/blob/main/Corona.qmd you find an example for the YAML and the data import chunk.\n\n\n\n\n1.1 Question 1: Is the data of OWiD and WHO the same?\nWrite a new headline ## Differences between OWiD and WHO data, make the following analysis in a chunk, and write an answer to the question. Guide for the analysis:\n\nCreate a joined tibble. This can be done with a chain of pipes |>.\n\nMake a new tibble who_owid where you left join owid to who. Use the combination of the data and the country as the key. To that end you have to identify the corresponding columns in who and owid with the named vector c(\"Date_reported\" = \"date\", \"Country\" = \"location\").\nUse select to select and rename columns in who_owid such that you have columns for Date, Country, New_cases_who, New_cases_owid, Total_cases_who, and Total_cases_owid.\n\nMake a visualization where the new cases in WHO and OWiD appear as two lines. Filter for the time before 2020-07-01 and four countries: Germany, France, India, and a country of your choice (Hint: use %in%). Then facet your visualization by country.\nRepeat the visualization for the comparison of WHO and OWiD data for the total cases.\nBelow the visualizations, explain what is visualizaed and then answer the question based on the visualizations. (Some guiding questions: What are the differences? How severe are the differences on a daily basis and in the long run? What data shows more fluctuations?) Bonus: What is the reason for the difference?\n\n\nFor comparing New_cases_who and New_cases_owid you need a to make the tibble longer (pivot_longer) such that you have one column with a categroical variable specifying the data source and one column with the actual value of the new cases in each data set. Then you can make a ggplot with geom_line where you make two lines by assigning the data source to the color aesthetic.\nFor faceting, use facet_wrap. It is useful to specify that there should be only one column and that scales are specified such that each facet has its own y-axis scale (called free_y).\nThe same advice holds for the visualization of total cases.\n\n\n\n1.2 Question 2: Are there patterns in the timeline of new cases which seem unrelated to the potential spread of the virus in the population? How could we smooth them best?\nWrite a new headline ## Smoothing new cases, make a chunk where you compute three additional columns with smoothed case numbers (see below), make a visualization and explain which is most appropriate to use and why.\n\nComputed a new tibble who_smoothed from who for this question. (There would be no essential difference with owid.) For each day compute the average of the actual day and some days directly before such that each new value is an average of 3, 7, or 10 days. Call the three new columns New_cases_smooth3, New_cases_smooth7, and New_cases_smooth10. The average is the sum of all values divided by the total number of days. Use mutate. For each column use lag several times for each previous day (see ?lag). Don’t forget to group by countries!\nVisualize the timeline for Germany using a time span between 6 and 12 month. Make one panel for New_cases_smooth3, New_cases_smooth7, and New_cases_smooth10 respectively. (Hint: Either use pivot_longer and faceting, or three ggplot objects combined using the package patchwork.)\nAnswer the question. What is the pattern which seems unrelated to the spread of the virus? Which is the most appropriate smoothing, 3, 7, or 10 days and why?\n\n\n\n1.3 Question 3: How do deaths follow cases?\nMake a new headline about this question in your report.\nIn owid, there are variables new_cases_smoothed and new_deaths_smoothed which you can use for convenience to explore this question.\nTo explore the connection between COVID-19 cases and deaths focus on the first wave in Germany. In owid, filter the data for Germany and dates before 2020-07-01. Plot new_cases_smoothed and new_deaths_smoothed in one line plot using color to distinguish both. (Hint: Use pivot_longer.) Now, produce a new variable scaled_lagged_cases where you do a shift-and-scale transformation of new_cases_smoothed: That means you scale down the number of cases by multiplying a factor \\(y\\)(< 1) and you lag the number of cases along the time axis by \\(x\\) days using y * lag(new_cases_smoothed, x).\n\nPlay with numerical values of \\(x\\) and \\(y\\) by plotting scaled_lagged_cases together with new_death_smoothed. Find values for such that both lines overlap as good as possible. Put this visualization into the report.\nDescribe what these “visually calibrated” values of \\(x\\) and \\(y\\) tell us about the relation of cases and deaths of COVID-19?\nRedo the line plot with your calibrated values for all dates in Germany. Describe how the connection between new cases and deaths changes in the course of the pandemic. List potential reasons.\nRepeat this analysis for another country of your choice. To that end find out the time range of the first wave in the country before. Do you get the same \\(x\\) and \\(y\\) by visually calibrating for this country?\n\n\n\n1.4 Question 4: How is the severity of the corona pandemic in countries related to human development?\nMake a new headline about this question in your report.\nHuman development is meant to be measured by the human development index of the United Nations Development Programme. Briefly, describe in the report how the Human Development Index (HDI) is composed.\nUse owid. Make a figure which visualizes each country as a point with human_development_index on the horizontal and total_deaths_per_million on the vertical axis. Filter the values for the date 2022-08-31. Filter out all rows which do not represent countries. (Hint: You can take all rows which have a valid value for continent.) Make the size of dots such that the area is proportional to the population. (Hint: Use the scale_size_area() as an additional layer.) Color the dots by continent.\nDescribe the visible relation of HDI and total corona related deaths per million. Emphasize what you find interesting, and hypothesize about potential reasons for the finding.\n\n\n1.5 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file and push it to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-03-instr.html#european-social-survey",
    "href": "hw-instructions/hw-03-instr.html#european-social-survey",
    "title": "Homework 03",
    "section": "2 European Social Survey",
    "text": "2 European Social Survey\nContinue to work on your repository ess-ind-USERNAME and make the analysis in python-chunks. You have to look ESS Data Documentation https://ess-search.nsd.no/.\n\nCreate a new Quarto-document and save it as “ESS-analysis.qmd” (alternatively as “ESS-analysis.ipynb” if you can render html-files similarly from it).\n\n\nCustomize the YAML with the title “European Social Survey Analysis”, your name, and output format being a standalone html-file with embedded resources. Enable code-folding in the output html file.\nWrite a chunk labeled data where you load the tidyverse library and import the dataset you downloaded for Homework 02.\n\nDocument what you did: Write a headline ## Data before the chunk data and briefly describe the data source.\nFor each of the following questions write a fitting headline to structure your report.\n\n\n2.1 Question 1: What is the ranking of European countries with respect to of average satisfaction with life?\n\nCompute the average of stflife for each country-year combination. Make a table with four columns: the country name and the average life satisfaction for the years 2006, 2012, and 2018. Order the table by the values in 2018 and print it nicely in your report.\nTo that end, you must create new variables countryname from the two-character ISO-codes in cntyr (use the library pycountry) and a new variable year.\nBeware: The ESS variables have coded missing values as large numbers like 77 or 88. Find out these values and recode them as NA before computing averages!\n\nDescribe what you find remarkable in the table. (Who is on top? who at the bottom? Any interesting trends? Any geographical patterns?)\n\n\n\n2.2 Question 2: What is the relation of the emotional attachment of Europeans to their own country and to Europe?\nEmotional attachment to Europe may diminish emotional attachment to the country. Test if this is true with the variables atchctr and atcherp. First, look up the wording of the two questions and the answering options and write it into the report.\nBuild two visualizations and put them in the report. Beware: First recode the missing values as NA! (See above.)\n\nA visualization where you first count the number of individuals each combination of valid values of atchctr and atcherp. Then plot each combination as a dot with atchctr on the x-axis, atcherp on the y-axis, and the number of individuals represented by size and color and color of the dot.\nMake a lowess (locally weighted scatterplot smoothing) line of atchctr on the x-axis, atcherp on the y-axis using the function regplot from seaborn.\n\nAnswer the question based on the visualization.\n\n\n2.3 Question 3: What is the relation of the emotional attachment to the own country and the opinion about further of European integration?\nThis is analog to Question 2 with euftf instead of atcherp. First, look up the wording of euftf and the answering options and write it into the report. Beware: First recode the missing values as NA! (See above.)\nBuild two visualizations as above.\nAnswer the question.\n\n\n2.4 Question 4: How many observations are there for each country-year combination?\nCompute the numbers and list them nicely in a table in your report.\nAre the numbers roughly proportional to the population in these countries? Discuss if the answer creates some shortcomings about the answers to Questions 2 and 3. How could these be mitigated?\n\n\n2.5 Question 5: Answer your own exploratory question\nLook up questions behind the variables in the ESS data set, formulate a question, and answer it with summarizing, and visualization."
  },
  {
    "objectID": "W9.html#precision-and-recall",
    "href": "W9.html#precision-and-recall",
    "title": "W#9 Logisitc Regression",
    "section": "Precision and Recall",
    "text": "Precision and Recall"
  },
  {
    "objectID": "W9.html#receiver-operating-characteristic",
    "href": "W9.html#receiver-operating-characteristic",
    "title": "W#9 Logisitc Regression",
    "section": "Receiver operating characteristic",
    "text": "Receiver operating characteristic"
  },
  {
    "objectID": "W9.html#recap-linear-models",
    "href": "W9.html#recap-linear-models",
    "title": "W#9 Logisitc Regression",
    "section": "Recap linear models",
    "text": "Recap linear models\nWe had linear models \\(y_i = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_n\\) with\nResponse (dependent) variable \\(y\\): Numeric\nPredictor (independent) variables \\(x_1,\\dots,x_n\\): Numeric or Binary (0 or 1)\n\nWhen a variable is categorical: We dummify it to \\(m-1\\) binary (dummy) variables (\\(m\\) is the number of categories)\n\nNote: In computer science dummifying is called one-hot encoding.\n\nWhen a variable has ordered categories (ordinal level of measurement): We may transform to a numerical variable assuming comparison of numerical distances between categories are interpretable."
  },
  {
    "objectID": "W9.html#interaction-effects",
    "href": "W9.html#interaction-effects",
    "title": "W#9 Logisitc Regression",
    "section": "Interaction effects",
    "text": "Interaction effects\nAdding products of variables in the linear model \\(y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{3}x_1x_2 + \\dots\\).\nFor \\(x_1\\) and \\(x_2\\) being dummy variables this is for example"
  },
  {
    "objectID": "W9.html#recap-interaction-effects",
    "href": "W9.html#recap-interaction-effects",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Recap: Interaction effects",
    "text": "Recap: Interaction effects\nAdding products of variables in the linear model \\(y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{3}x_1x_2 + \\dots\\).\nFor \\(x_1\\) and \\(x_2\\) being dummy variables this is for example\n\n\n\n\n\ngndr_f\nhas_kids\ngndr_f_x_has_kids\n\n\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n1\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\nCheck:\n\nWhat are the reference categories? Being male without kids.\nEstimating model on life satisfaction. How would we see if being a mother/father increases life satisfaction more? positiv/negative coefficient gndr_f_x_has_kids"
  },
  {
    "objectID": "W9.html#what-if-response-is-binary",
    "href": "W9.html#what-if-response-is-binary",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "What if response is binary?",
    "text": "What if response is binary?\n\nExample: Spam filter for emails\n\n\nlibrary(openintro)\nlibrary(tidyverse)\nglimpse(email)\n\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2012-01-01 07:16:41, 2012-01-01 08:03:59, 2012-01-01 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …"
  },
  {
    "objectID": "W9.html#variables",
    "href": "W9.html#variables",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Variables",
    "text": "Variables\n?email shows all variable descriptions. For example:\n\nspam Indicator for whether the email was spam.\nfrom Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\ncc Number of people cc’ed.\ntime Time at which email was sent.\nattach The number of attached files.\ndollar The number of times a dollar sign or the word “dollar” appeared in the email.\nnum_char The number of characters in the email, in thousands.\nre_subj Whether the subject started with “Re:”, “RE:”, “re:”, or “rE:”\n\n\n\nThe development, extraction, or discovery of such variables is called feature engineering, feature extraction or feature discovery. Usually, a combination of domain knowledge and data science skill is needed to do this."
  },
  {
    "objectID": "W9.html#multinomial",
    "href": "W9.html#multinomial",
    "title": "W#9 Logisitc Regression",
    "section": "Multinomial",
    "text": "Multinomial\n\n\nWe will not cover other categorical variables than binary ones here. However, many of the probabilistic concepts transfer."
  },
  {
    "objectID": "W9.html#does",
    "href": "W9.html#does",
    "title": "W#9 Logisitc Regression",
    "section": "Does",
    "text": "Does\n\nemail |> ggplot(aes(x = num_char, y = spam)) + geom_boxplot()"
  },
  {
    "objectID": "W9.html#data-exploration",
    "href": "W9.html#data-exploration",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Data exploration",
    "text": "Data exploration\nWould you expect spam to be longer or shorter?\n\n\nemail |> ggplot(aes(x = num_char, y = spam)) + geom_boxplot()\n\n\n\n\n\n\nWould you expect spam subject to start with “Re:” or the like?\n\n\n\nemail |> ggplot(aes(y = re_subj, fill = spam)) + geom_bar()"
  },
  {
    "objectID": "W9.html#modeling",
    "href": "W9.html#modeling",
    "title": "W#9 Logisitc Regression",
    "section": "Modeling",
    "text": "Modeling\nBoth seem to give some signal.\nHow can we model the relationship?\nWe focus first on just num_char:\n\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\nlibrary(tidymodels) \nlinear_reg() |> fit(as.numeric(spam)-1 ~ num_char, data = email)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = as.numeric(spam) - 1 ~ num_char, data = data)\n\nCoefficients:\n(Intercept)     num_char  \n   0.118214    -0.002299"
  },
  {
    "objectID": "W9.html#linear-models",
    "href": "W9.html#linear-models",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Linear models?",
    "text": "Linear models?\nBoth seem to give some signal. How can we model the relationship?\nWe focus first on just num_char:\n\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + geom_point(alpha = 0.2) + geom_smooth(method = \"lm\")\n\n\n\nlibrary(tidymodels) \nlinear_reg() |> fit(as.numeric(spam)-1 ~ num_char, data = email)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = as.numeric(spam) - 1 ~ num_char, data = data)\n\nCoefficients:\n(Intercept)     num_char  \n   0.118214    -0.002299  \n\n\nWe would like to have a better concept!"
  },
  {
    "objectID": "W9.html#a-probabilistic-concept",
    "href": "W9.html#a-probabilistic-concept",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "A probabilistic concept",
    "text": "A probabilistic concept\n\nWe treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials\n\nBernoulli trial: a random experiment with exactly two possible outcomes, success and failure, in which the probability of success is the same every time the experiment is conducted\n\n\n\n\nEach email is treated as Bernoulli trial with separate probability of success\n\n\\[ y_i ∼ \\text{Bernoulli}(p_i) \\]\n\n\n\nWe use the predictor variables to model the Bernoulli parameter \\(p_i\\)\n\n\n\n\nNow we conceptualized a continuous response, but still a linear model does not fit perfectly for \\(p_i\\) (since a probability is between 0 and 1).\nHowever, we can transform the linear model to have the appropriate range."
  },
  {
    "objectID": "W9.html#generalized-linear-models",
    "href": "W9.html#generalized-linear-models",
    "title": "W#9 Logisitc Regression",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nThis is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs)\n\n. . . - Logistic regression is just one example"
  },
  {
    "objectID": "W9.html#three-characteristics-of-glms",
    "href": "W9.html#three-characteristics-of-glms",
    "title": "W#9 Logisitc Regression",
    "section": "Three characteristics of GLMs",
    "text": "Three characteristics of GLMs\nAll GLMs have the following three characteristics:\n\nA probability distribution describing a generative model for the outcome variable\n\n. . . 2. A linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\]\n. . . 3. A link function that relates the linear model to the parameter of the outcome distribution\nclass: middle"
  },
  {
    "objectID": "W9.html#logistic-regression-1",
    "href": "W9.html#logistic-regression-1",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLogistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors.\nThe distribution is the Bernoulli distribution.\nAs link function connecting \\(\\eta_i\\) to \\(p_i\\) we use the logit function.\n\n\n\nLogit function: \\(\\text{logit}: [0,1] \\to \\mathbb{R}\\)\n\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\n\\(\\frac{p}{1-p}\\) is called the odds of a success which happens with probability \\(p\\).\nExample: Roll a six with a die has \\(p=1/6\\). Thus, the odds are \\(\\frac{1/6}{5/6} = 1/5\\). Sometimes written as 1:5. “The odds of success are one to five.”"
  },
  {
    "objectID": "W9.html#logit-function-visualised",
    "href": "W9.html#logit-function-visualised",
    "title": "W#9 Logisitc Regression",
    "section": "Logit function, visualised",
    "text": "Logit function, visualised\n\nggplot() + \n geom_function(fun = function(x) log(x/(1-x)), xlim=c(0.001,0.999), n = 500) + \n geom_function(fun = function(x) 1/(1 + exp(-x)), color = \"red\") +\n scale_x_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n scale_y_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n coord_fixed() + theme_minimal(base_size = 24) + labs(x = \"x\")"
  },
  {
    "objectID": "W9.html#properties-of-the-logit",
    "href": "W9.html#properties-of-the-logit",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Properties of the logit",
    "text": "Properties of the logit\n\nLogit takes values between 0 and 1 and returns values between \\(-\\infty\\) and \\(\\infty\\)\nThe inverse of the logit function if the logistic function (mapping values from \\(-\\infty\\) and \\(\\infty\\) to values between 0 and 1): \\[\\text{logit}^{-1}(x) = \\text{logistic}(x) = \\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}}\\]\nLogit can be interpreted as the log odds of a success – more on this later.\n\n\n\n\nThe logistic function is the solution of the differential equation \\(\\frac{d}{dx}f(x) = f(x)(1-f(x))\\) which also appears in the SI-model of epidemics (and other models of exponential growth with saturation).\nGood exercises to check your math skills:\n\nShow that \\(\\text{logit}^{-1}(x) = \\text{logistic}(x)\\) or the other way round.\nTransform \\(\\frac{e^x}{1+e^x}\\) into \\(\\frac{1}{1+e^{-x}}\\).\nCheck that \\(\\text{logistic}(x)\\) is a solution to \\(\\frac{d}{dx}f(x) = f(x)(1-f(x))\\).\n\nYou can request hints from me when you get stuck."
  },
  {
    "objectID": "W9.html#the-logistic-regression-model",
    "href": "W9.html#the-logistic-regression-model",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nBased on the three GLM criteria we have\n\n\\(y_i \\sim \\text{Bernoulli}(p_i)\\)\n\\(\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\n\\(\\text{logit}(p_i) = \\eta_i\\)\n\n\n\n\nFrom which we get\n\n\\[p_i = \\frac{e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}{1 + e^{\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}}}\\]"
  },
  {
    "objectID": "W9.html#characterising-glms",
    "href": "W9.html#characterising-glms",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Characterising GLMs",
    "text": "Characterising GLMs\n\nGeneralized linear models (GLMs) are a way of addressing many problems in regression\nLogistic regression is one example\n\nAll GLMs have the following three characteristics:\n\nA probability distribution as a generative model for the outcome variable \\(y_i \\sim \\text{Distribution}(\\text{parameter})\\)\n\n\n\nA linear model \\(\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\)\nwhere \\(\\eta\\) is related to a mean parameter of the distribution by the …\n\n\n\n\nLink function that relates the linear model to the parameter of the outcome distribution."
  },
  {
    "objectID": "W9.html#logit-and-logistic-function",
    "href": "W9.html#logit-and-logistic-function",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Logit and logistic function",
    "text": "Logit and logistic function\n\nggplot() + \n geom_function(fun = function(x) log(x/(1-x)), xlim=c(0.001,0.999), n = 500) + \n geom_function(fun = function(x) 1/(1 + exp(-x)), color = \"red\") +\n scale_x_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n scale_y_continuous(breaks = seq(-5,5,1), limits = c(-5,5)) +\n coord_fixed() + theme_minimal(base_size = 24) + labs(x = \"x\")"
  },
  {
    "objectID": "W9.html#recap-linear-models-1",
    "href": "W9.html#recap-linear-models-1",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Recap linear models",
    "text": "Recap linear models\nWe had linear models \\(y_i = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_n\\) with\nResponse (dependent) variable \\(y\\): Numeric\nPredictor (independent) variables \\(x_1,\\dots,x_n\\): Numeric or Binary (0 or 1)\n\nWhen a variable is categorical: We dummify it to \\(m-1\\) binary (dummy) variables (\\(m\\) is the number of categories)\n\nNote: In computer science dummifying is called one-hot encoding.\n\nWhen a variable has ordered categories (ordinal level of measurement): We may transform to a numerical variable assuming comparison of numerical distances between categories are interpretable."
  },
  {
    "objectID": "W9.html#modeling-spam",
    "href": "W9.html#modeling-spam",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Modeling spam",
    "text": "Modeling spam\nWith tidymodels we fit a GLM in the same way as a linear model except we\n\nspecify the model with logistic_reg()\nuse \"glm\" instead of \"lm\" as the engine\ndefine family = \"binomial\" for the link function to be used in the model\n\n\nspam_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char, data = email, family = \"binomial\")\ntidy(spam_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\n\n\n\nThe family is binomial because the Bernoulli distribution is a special case of the binomial distribution \\(\\text{Binomial}(n,p)\\) with \\(n=1\\).\nThe binomial distribution specifies the probability to have \\(k\\) successes in \\(n\\) Bernoulli trials with the same success probability \\(p\\)."
  },
  {
    "objectID": "W9.html#spam-model",
    "href": "W9.html#spam-model",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Spam model",
    "text": "Spam model\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\nModel: \\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}\\]"
  },
  {
    "objectID": "W9.html#predicted-probability-examples",
    "href": "W9.html#predicted-probability-examples",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Predicted probability: Examples",
    "text": "Predicted probability: Examples\nWe can compute the predicted probability that an email with 2000 character is spam as follows:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot 2 = -1.9242\\]\n(Note: num_char is in thousands.)\n\\[\\frac{p}{1-p} = e^{-1.9242} = 0.15 \\Rightarrow p = 0.15 \\cdot (1 - p)\\]\n\\[p = 0.15 - 0.15\\cdot p \\Rightarrow 1.15\\cdot p = 0.15\\]\n\\[p = 0.15 / 1.15 = 0.13\\]"
  },
  {
    "objectID": "W9.html#predicted-probability",
    "href": "W9.html#predicted-probability",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Predicted probability",
    "text": "Predicted probability\n\nlogistic <- function(t) 1/(1+exp(-t))\npreds <- tibble(x=c(2,15,40), y = logistic(-1.80-0.0621*x))\nemail |> ggplot(aes(x = num_char, y = as.numeric(spam)-1)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-1.80-0.0621*x),color=\"red\") +\n geom_point(data = preds, mapping = aes(x,y), color = \"blue\", size = 3)\n\n\nSpam probability 2,000 characters: 0.1273939\nSpam probability 15,000 characters: 0.06114\nSpam probability 40,000 characters: 0.0135999"
  },
  {
    "objectID": "W9.html#false-positive-and-negative",
    "href": "W9.html#false-positive-and-negative",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail labelled spam\nEmail labelled not spam\n\n\n\n\nEmail is spam\nTrue positive\nFalse negative (Type 2 error)\n\n\nEmail is not spam\nFalse positive (Type 1 error)\nTrue negative"
  },
  {
    "objectID": "W9.html#sensitivity-and-specificity-1",
    "href": "W9.html#sensitivity-and-specificity-1",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)\n\n\nFor spam:\nSensitivity: Fraction of emails labelled as spam among all emails which are spam.\nLow sensitivity \\(\\to\\) More false negatives \\(\\to\\) More spam in you inbox!\nSpecificity: Fraction of emails labelled as not spam among all emails which are not spam.\nLow specificity \\(\\to\\) More false positives \\(\\to\\) More relevant emails in spam folder!\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "W9.html#another-example",
    "href": "W9.html#another-example",
    "title": "W#9 Logisitc Regression",
    "section": "Another example",
    "text": "Another example\ndata(“PimaIndiansDiabetes2”)"
  },
  {
    "objectID": "W9.html#maximum-likelihood",
    "href": "W9.html#maximum-likelihood",
    "title": "W#9 Logisitc Regression",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nSee “More probability” for Logistic Regression."
  },
  {
    "objectID": "W9.html#another-example-penguins",
    "href": "W9.html#another-example-penguins",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Another example: Penguins",
    "text": "Another example: Penguins\n\nlibrary(palmerpenguins)\nsex_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(sex ~ body_mass_g, data = na.omit(penguins), family = \"binomial\")\ntidy(sex_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) -5.16     0.724        -7.13 1.03e-12\n2 body_mass_g  0.00124  0.000173      7.18 7.10e-13\n\n\n\nna.omit(penguins) |> ggplot(aes(x = body_mass_g, y = sex)) + \n geom_point(alpha = 0.2) + \n geom_function(fun = function(x) logistic(-5.16+0.00124*x) + 1,color=\"red\") +\n xlim(c(2000,7000))"
  },
  {
    "objectID": "W9.html#another-view-sensitivity-and-specifity",
    "href": "W9.html#another-view-sensitivity-and-specifity",
    "title": "W#9 Logisitc Regression",
    "section": "Another view: Sensitivity and specifity",
    "text": "Another view: Sensitivity and specifity"
  },
  {
    "objectID": "W9.html#another-view",
    "href": "W9.html#another-view",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Another view",
    "text": "Another view"
  },
  {
    "objectID": "W9.html#covid-19-tests",
    "href": "W9.html#covid-19-tests",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "COVID-19 tests",
    "text": "COVID-19 tests\nWhat is the sensitivity of a test?\n\nProbability to have COVID-19 when the test is positive.\n\n\nWhat is the specificity of a test?\n\n\nProbability to not have COVID-19 when the test is negative.\nOften the sensitivity is around 90% and the specificity is around 99%. What does that mean?\n\n\n\nWhen you test negative you can be more sure that you don’t have it, than you can be sure that you have it when your test is positive.\nHowever, in a larger population of testing individuals with high prevalence also 99% specificity implies a large fraction of false negatives!"
  },
  {
    "objectID": "W9.html#goal-building-a-spam-filter",
    "href": "W9.html#goal-building-a-spam-filter",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Goal: Building a spam filter",
    "text": "Goal: Building a spam filter\n\nData: Set of emails and we know if each email is spam/not and other features\nUse logistic regression to predict the probability that an incoming email is spam\nUse model selection to pick the model with the best predictive performance\n\n\n\nBuilding a model to predict the probability that an email is spam is only half of the battle! We also need a decision rule about which emails get flagged as spam (e.g. what probability should we use as out cutoff?)\n\n\n\n\nA simple approach: choose a single threshold probability and any email that exceeds that probability is flagged as spam"
  },
  {
    "objectID": "W9.html#emails-use-all-predictors",
    "href": "W9.html#emails-use-all-predictors",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Emails: Use all predictors",
    "text": "Emails: Use all predictors\n\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = email, family = \"binomial\") |>\n  tidy() |> print(n = 22)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n# A tibble: 22 × 5\n   term         estimate std.error statistic  p.value\n   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)  -9.09e+1   9.80e+3  -0.00928 9.93e- 1\n 2 to_multiple1 -2.68e+0   3.27e-1  -8.21    2.25e-16\n 3 from1        -2.19e+1   9.80e+3  -0.00224 9.98e- 1\n 4 cc            1.88e-2   2.20e-2   0.855   3.93e- 1\n 5 sent_email1  -2.07e+1   3.87e+2  -0.0536  9.57e- 1\n 6 time          8.48e-8   2.85e-8   2.98    2.92e- 3\n 7 image        -1.78e+0   5.95e-1  -3.00    2.73e- 3\n 8 attach        7.35e-1   1.44e-1   5.09    3.61e- 7\n 9 dollar       -6.85e-2   2.64e-2  -2.59    9.64e- 3\n10 winneryes     2.07e+0   3.65e-1   5.67    1.41e- 8\n11 inherit       3.15e-1   1.56e-1   2.02    4.32e- 2\n12 viagra        2.84e+0   2.22e+3   0.00128 9.99e- 1\n13 password     -8.54e-1   2.97e-1  -2.88    4.03e- 3\n14 num_char      5.06e-2   2.38e-2   2.13    3.35e- 2\n15 line_breaks  -5.49e-3   1.35e-3  -4.06    4.91e- 5\n16 format1      -6.14e-1   1.49e-1  -4.14    3.53e- 5\n17 re_subj1     -1.64e+0   3.86e-1  -4.25    2.16e- 5\n18 exclaim_subj  1.42e-1   2.43e-1   0.585   5.58e- 1\n19 urgent_subj1  3.88e+0   1.32e+0   2.95    3.18e- 3\n20 exclaim_mess  1.08e-2   1.81e-3   5.98    2.23e- 9\n21 numbersmall  -1.19e+0   1.54e-1  -7.74    9.62e-15\n22 numberbig    -2.95e-1   2.20e-1  -1.34    1.79e- 1\n\n\n\n\nWe treat the warning later."
  },
  {
    "objectID": "W9.html#the-prediction-task",
    "href": "W9.html#the-prediction-task",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "The prediction task",
    "text": "The prediction task\n\nThe mechanics of prediction is easy:\n\nPlug in values of predictors to the model equation\nCalculate the predicted value of the response variable, \\(\\hat{y}\\)\n\n\n\n\nGetting it right is harder\n\nThere is no guarantee the model estimates you have are correct\nOr that your model will perform as well with new data as it did with your sample data"
  },
  {
    "objectID": "W9.html#overfitting",
    "href": "W9.html#overfitting",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Overfitting",
    "text": "Overfitting\n\n\n\n\n\n\n\nThis is simulated data."
  },
  {
    "objectID": "W9.html#spending-our-data",
    "href": "W9.html#spending-our-data",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Spending our data",
    "text": "Spending our data\n\nSeveral steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available can lead to overfitting.\n\nSolution: We subsets our data for different tasks, as opposed to allocating all data to parameter estimation (as we have done so far)."
  },
  {
    "objectID": "W9.html#splitting-data-1",
    "href": "W9.html#splitting-data-1",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Splitting data",
    "text": "Splitting data\n\nTraining set:\n\nSandbox for model building\nSpend most of your time using the training set to develop the model\nMajority of the data (usually 80%)\n\nTesting set:\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once, otherwise it becomes part of the modeling process\nRemainder of the data (usually 20%)"
  },
  {
    "objectID": "W9.html#performing-the-split",
    "href": "W9.html#performing-the-split",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Performing the split",
    "text": "Performing the split\n\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1116)\n\n# Put 80% of the data into the training set \nemail_split <- initial_split(email, prop = 0.80)\n\n# Create data frames for the two sets:\ntrain_data <- training(email_split)\ntest_data  <- testing(email_split)"
  },
  {
    "objectID": "W9.html#peek-at-the-split",
    "href": "W9.html#peek-at-the-split",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Peek at the split",
    "text": "Peek at the split\n\n\n\nglimpse(train_data)\n\nRows: 3,136\nColumns: 21\n$ spam         <fct> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0,…\n$ sent_email   <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ time         <dttm> 2012-01-25 23:46:55, 2012-01-03 06:28:28, 2012-02-04 17:…\n$ image        <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 10, 0, 0, 0, 0, 0, 13, 0, 0, 0, 2, 0, 0, 0, 14, 0, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, yes, no, no, no, no, no, no, …\n$ inherit      <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, …\n$ num_char     <dbl> 23.308, 1.162, 4.732, 42.238, 1.228, 25.599, 16.764, 10.7…\n$ line_breaks  <int> 477, 2, 127, 712, 30, 674, 367, 226, 98, 671, 46, 192, 67…\n$ format       <fct> 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 12, 0, 2, 2, 2, 31, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 11, 1, …\n$ number       <fct> small, none, big, big, small, small, small, small, small,…\n\n\n\n\nglimpse(test_data)\n\nRows: 785\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, …\n$ sent_email   <fct> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ time         <dttm> 2012-01-01 18:55:06, 2012-01-01 20:38:32, 2012-01-02 06:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, …\n$ dollar       <dbl> 0, 0, 5, 0, 0, 0, 0, 5, 4, 0, 0, 0, 21, 0, 0, 2, 9, 0, 0,…\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, …\n$ num_char     <dbl> 4.837, 15.075, 18.037, 45.842, 11.438, 1.482, 14.431, 0.9…\n$ line_breaks  <int> 193, 354, 345, 881, 125, 24, 296, 13, 192, 14, 32, 30, 55…\n$ format       <fct> 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, …\n$ re_subj      <fct> 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, …\n$ exclaim_subj <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 1, 10, 20, 5, 2, 0, 0, 0, 6, 0, 0, 1, 3, 0, 4, 0, 1, 0, 1…\n$ number       <fct> big, small, small, big, small, none, small, small, small,…"
  },
  {
    "objectID": "W9.html#interpretation-of-coefficients",
    "href": "W9.html#interpretation-of-coefficients",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\cdot \\text{num_char}\\]\nWhat does an increase by thousand characters (num_char + 1) imply?\n\nLet us assume the predicted probability of an email is \\(p_0\\). Then an increase of num_char by one implied that the log-odds become\n\\[\\log\\left(\\frac{p_0}{1-p_0}\\right) - 0.0621 = \\log\\left(\\frac{p_0}{1-p_0}\\right) - \\log(e^{0.0621})\\]\n\\[ = \\log\\left(\\frac{p_0}{1-p_0}\\right) + \\log(\\frac{1}{e^{0.0621}}) = \\log\\left(\\frac{p_0}{1-p_0} \\frac{1}{e^{0.0621}}\\right) = \\log\\left(\\frac{p_0}{1-p_0} 0.94\\right)\\]\nThat means the odds of being spam decrease by 6%."
  },
  {
    "objectID": "W9.html#confusion-matrix",
    "href": "W9.html#confusion-matrix",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nMore general: Confusion matrix of statistical classification:"
  },
  {
    "objectID": "W9.html#fit-a-model-to-the-training-dataset",
    "href": "W9.html#fit-a-model-to-the-training-dataset",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Fit a model to the training dataset",
    "text": "Fit a model to the training dataset\n\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ ., data = train_data, family = \"binomial\")\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWe get a warning and should explore the reasons for 0 or 1 probability.\n\n\n\nA deeper looking into the predicted probabilities (not shown here) shows that 4 cases are predicted to be spam with 100% probability, as well as 864 cases are predicted to be not spam with 100% probability.\nNote: The dplyr function near was used to assess if predicted probabilities were one.\nThis is usually undesirable. Hence the warning."
  },
  {
    "objectID": "W9.html#look-at-categorical-predictors",
    "href": "W9.html#look-at-categorical-predictors",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Look at categorical predictors",
    "text": "Look at categorical predictors\n\nCloser look at from and sent_email."
  },
  {
    "objectID": "W9.html#counting-cases",
    "href": "W9.html#counting-cases",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Counting cases",
    "text": "Counting cases\n\n\nfrom: Whether the message was listed as from anyone (this is usually set by default for regular outgoing email).\n\ntrain_data |> count(spam, from)\n\n# A tibble: 3 × 3\n  spam  from      n\n  <fct> <fct> <int>\n1 0     1      2837\n2 1     0         3\n3 1     1       296\n\n\nNo non-spam mails without from.\n\nsent_mail: Indicator for whether the sender had been sent an email from the receiver in the last 30 days.\n\ntrain_data |> count(spam, sent_email)\n\n# A tibble: 3 × 3\n  spam  sent_email     n\n  <fct> <fct>      <int>\n1 0     0           1972\n2 0     1            865\n3 1     0            299\n\n\nNo spam mails with sent_email.\n\n\n\nThere is incomplete separation in the data for those variables.\nThat mean we have a sure prediction probabilities (0 or 1). (That is the warning. Also, these variables have the highest coefficients.)\nThis is not what we assume about reality. Maybe our sample is too small to see it.\nTherefore we exclude these variables."
  },
  {
    "objectID": "W9.html#look-at-numerical-variables",
    "href": "W9.html#look-at-numerical-variables",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Look at numerical variables",
    "text": "Look at numerical variables\n\n\ntrain_data |>\n group_by(spam) |>\n select(where(is.numeric)) |> \n pivot_longer(-spam) |> \n group_by(name, spam) |> \n summarize(mean = mean(value), sd = sd(value)) |> \n print(n = 22)\n\n\n# A tibble: 22 × 4\n# Groups:   name [11]\n   name         spam       mean       sd\n   <chr>        <fct>     <dbl>    <dbl>\n 1 attach       0       0.124     0.775 \n 2 attach       1       0.227     0.620 \n 3 cc           0       0.393     2.62  \n 4 cc           1       0.388     3.25  \n 5 dollar       0       1.56      5.33  \n 6 dollar       1       0.779     3.01  \n 7 exclaim_mess 0       6.68     50.2   \n 8 exclaim_mess 1       8.75     88.4   \n 9 exclaim_subj 0       0.0783    0.269 \n10 exclaim_subj 1       0.0769    0.267 \n11 image        0       0.0536    0.503 \n12 image        1       0.00334   0.0578\n13 inherit      0       0.0352    0.216 \n14 inherit      1       0.0702    0.554 \n15 line_breaks  0     247.      326.    \n16 line_breaks  1     108.      321.    \n17 num_char     0      11.4      14.9   \n18 num_char     1       5.63     15.7   \n19 password     0       0.112     0.938 \n20 password     1       0.0201    0.182 \n21 viagra       0       0         0     \n22 viagra       1       0.0268    0.463 \n\n\n\nviagra has no mentions in non-spam emails.\n\nWe should exclude this variable for the same reason."
  },
  {
    "objectID": "W9.html#fit-a-model-to-the-training-dataset-1",
    "href": "W9.html#fit-a-model-to-the-training-dataset-1",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Fit a model to the training dataset",
    "text": "Fit a model to the training dataset\n\nemail_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ . - from - sent_email - viagra, data = train_data, family = \"binomial\") \n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nemail_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = spam ~ . - from - sent_email - viagra, family = stats::binomial, \n    data = data)\n\nCoefficients:\n (Intercept)  to_multiple1            cc          time         image  \n  -9.867e+01    -2.505e+00     1.944e-02     7.396e-08    -2.854e+00  \n      attach        dollar     winneryes       inherit      password  \n   5.070e-01    -6.440e-02     2.170e+00     4.499e-01    -7.065e-01  \n    num_char   line_breaks       format1      re_subj1  exclaim_subj  \n   5.870e-02    -5.420e-03    -9.017e-01    -2.995e+00     1.002e-01  \nurgent_subj1  exclaim_mess   numbersmall     numberbig  \n   3.572e+00     1.009e-02    -8.518e-01    -1.329e-01  \n\nDegrees of Freedom: 3135 Total (i.e. Null);  3117 Residual\nNull Deviance:      1974 \nResidual Deviance: 1447     AIC: 1485\n\n\nWe still get a warning, but without very high coefficients.\n\n\nA deeper analysis shows that now only two cases are predicted not spam with 100% probability."
  },
  {
    "objectID": "W9.html#predict-outcome-on-the-testing-dataset",
    "href": "W9.html#predict-outcome-on-the-testing-dataset",
    "title": "W#9 Logisitc Regression",
    "section": "Predict outcome on the testing dataset",
    "text": "Predict outcome on the testing dataset\nPredicting the raw values (log-odds)\n\npredict(email_fit, test_data, type = \"raw\") |> head()\n\n        1         2         3         4         5         6 \n-4.942500 -6.312226 -3.938487 -6.688992 -4.399541 -1.587700 \n\n\nPredicting probabilities\n\npredict(email_fit, test_data, type = \"prob\") |> head()\n\n# A tibble: 6 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.993 0.00709\n2   0.998 0.00181\n3   0.981 0.0191 \n4   0.999 0.00124\n5   0.988 0.0121 \n6   0.830 0.170  \n\n\nPredicting spam\n\npredict(email_fit, test_data) # That is the default with type = \"class\"\n\n# A tibble: 785 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#predict-probabilities-on-the-testing-dataset",
    "href": "W9.html#predict-probabilities-on-the-testing-dataset",
    "title": "W#9 Logisitc Regression",
    "section": "Predict probabilities on the testing dataset",
    "text": "Predict probabilities on the testing dataset\n\n\n# A tibble: 785 × 4\n   .pred_0 .pred_1 spam  time               \n     <dbl>   <dbl> <fct> <dttm>             \n 1   0.993 0.00709 0     2012-01-01 18:55:06\n 2   0.998 0.00181 0     2012-01-01 20:38:32\n 3   0.981 0.0191  0     2012-01-02 06:42:16\n 4   0.999 0.00124 0     2012-01-02 16:12:51\n 5   0.988 0.0121  0     2012-01-02 17:45:36\n 6   0.830 0.170   0     2012-01-02 22:55:03\n 7   0.959 0.0410  0     2012-01-03 02:07:17\n 8   0.861 0.139   0     2012-01-03 06:41:35\n 9   0.938 0.0617  0     2012-01-03 17:02:35\n10   0.902 0.0983  0     2012-01-03 12:14:51\n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#predict-with-the-testing-dataset",
    "href": "W9.html#predict-with-the-testing-dataset",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Predict with the testing dataset",
    "text": "Predict with the testing dataset\nPredicting the raw values (log-odds)\n\npredict(email_fit, test_data, type = \"raw\") |> head() # head prints the first values\n\n        1         2         3         4         5         6 \n-4.942500 -6.312226 -3.938487 -6.688992 -4.399541 -1.587700 \n\n\n\n\nPredicting probabilities\n\npredict(email_fit, test_data, type = \"prob\") |> head()\n\n# A tibble: 6 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.993 0.00709\n2   0.998 0.00181\n3   0.981 0.0191 \n4   0.999 0.00124\n5   0.988 0.0121 \n6   0.830 0.170  \n\n\n\nPredicting spam (default)\n\npredict(email_fit, test_data) # Would be type = \"class\"\n\n# A tibble: 785 × 1\n   .pred_class\n   <fct>      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#relate-back-to-the-model-concept",
    "href": "W9.html#relate-back-to-the-model-concept",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Relate back to the model concept",
    "text": "Relate back to the model concept\n\n\nemail_pred <- \n predict(email_fit, test_data, type = \"prob\") |>\n select(spam_prob = .pred_1) |> \n mutate(spam_logodds = \n         predict(email_fit, test_data, type = \"raw\"), \n        spam_odds = exp(spam_logodds)) |> \n bind_cols(predict(email_fit, test_data)) |> \n # Append real data\n bind_cols(test_data |> select(spam)) \nemail_pred\n\n\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1   0.00709        -4.94   0.00714 0           0    \n 2   0.00181        -6.31   0.00181 0           0    \n 3   0.0191         -3.94   0.0195  0           0    \n 4   0.00124        -6.69   0.00124 0           0    \n 5   0.0121         -4.40   0.0123  0           0    \n 6   0.170          -1.59   0.204   0           0    \n 7   0.0410         -3.15   0.0427  0           0    \n 8   0.139          -1.83   0.161   0           0    \n 9   0.0617         -2.72   0.0657  0           0    \n10   0.0983         -2.22   0.109   0           0    \n# … with 775 more rows\n\n\n\n\nThe raw predictions are the log-odds.\nFrom which we can compute the odds.\nFrom which the probability is computed. Here it is done by predict.\nThe .pred_class prediction is when the probability > 0.5.\n\nWhat does it mean for the odds and the log-odds?\n\n\n\nAnswers: odds > 1, log-odds > 0"
  },
  {
    "objectID": "W9.html#another-look",
    "href": "W9.html#another-look",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Another look",
    "text": "Another look\n\nemail_pred |> arrange(desc(spam_prob)) |> print(n = 20)\n\n# A tibble: 785 × 5\n   spam_prob spam_logodds spam_odds .pred_class spam \n       <dbl>        <dbl>     <dbl> <fct>       <fct>\n 1     0.903       2.23       9.29  1           1    \n 2     0.833       1.60       4.98  1           0    \n 3     0.825       1.55       4.71  1           1    \n 4     0.733       1.01       2.75  1           1    \n 5     0.683       0.766      2.15  1           1    \n 6     0.626       0.517      1.68  1           1    \n 7     0.614       0.464      1.59  1           0    \n 8     0.597       0.392      1.48  1           1    \n 9     0.538       0.153      1.17  1           1    \n10     0.537       0.148      1.16  1           0    \n11     0.510       0.0404     1.04  1           0    \n12     0.491      -0.0345     0.966 0           0    \n13     0.490      -0.0407     0.960 0           0    \n14     0.489      -0.0453     0.956 0           1    \n15     0.483      -0.0698     0.933 0           1    \n16     0.473      -0.107      0.899 0           0    \n17     0.463      -0.150      0.861 0           0    \n18     0.457      -0.174      0.840 0           0    \n19     0.447      -0.212      0.809 0           0    \n20     0.447      -0.214      0.808 0           1    \n# … with 765 more rows\n\n\nWe see false positives and false negatives."
  },
  {
    "objectID": "W9.html#evaluate-the-performance",
    "href": "W9.html#evaluate-the-performance",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nReceiver operating characteristic (ROC) curve1 which plots true positive rate (sensitivity) vs. false positive rate (1 - specificity)\n\nemail_pred |> roc_curve(\n    truth = spam, estimate = spam_prob,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n\n\n\n\nOriginally developed for operators of military radar receivers, hence the odd name."
  },
  {
    "objectID": "W9.html#evaluate-the-performance-1",
    "href": "W9.html#evaluate-the-performance-1",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nFind the area under the curve.\nIn calculus language: \\(\\int_0^1 \\text{TPR}(\\text{FPR}) d\\text{FPR}\\) where TPR = True Positive Rate and FPR = False Positive Rate.\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.857"
  },
  {
    "objectID": "W9.html#feature-engineering-1",
    "href": "W9.html#feature-engineering-1",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are critical to the success of the model\nFeature engineering is getting creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "W9.html#modeling-workflow-revisited",
    "href": "W9.html#modeling-workflow-revisited",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Modeling workflow, revisited",
    "text": "Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\n\nThe tidymodels way (similar to ways in python).\n\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "W9.html#initiate-a-recipe",
    "href": "W9.html#initiate-a-recipe",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsummary(email_rec) |> print(n = 21)\n\n# A tibble: 21 × 4\n   variable     type    role      source  \n   <chr>        <chr>   <chr>     <chr>   \n 1 to_multiple  nominal predictor original\n 2 from         nominal predictor original\n 3 cc           numeric predictor original\n 4 sent_email   nominal predictor original\n 5 time         date    predictor original\n 6 image        numeric predictor original\n 7 attach       numeric predictor original\n 8 dollar       numeric predictor original\n 9 winner       nominal predictor original\n10 inherit      numeric predictor original\n11 viagra       numeric predictor original\n12 password     numeric predictor original\n13 num_char     numeric predictor original\n14 line_breaks  numeric predictor original\n15 format       nominal predictor original\n16 re_subj      nominal predictor original\n17 exclaim_subj numeric predictor original\n18 urgent_subj  nominal predictor original\n19 exclaim_mess numeric predictor original\n20 number       nominal predictor original\n21 spam         nominal outcome   original\n\n\nThe object email_rec only includes meta-data (columns names and types)!"
  },
  {
    "objectID": "W9.html#remove-certain-variables",
    "href": "W9.html#remove-certain-variables",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Remove certain variables",
    "text": "Remove certain variables\n\nemail_rec |>\n  step_rm(from, sent_email, viagra)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra"
  },
  {
    "objectID": "W9.html#feature-engineer-date",
    "href": "W9.html#feature-engineer-date",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Feature engineer date",
    "text": "Feature engineer date\n\nThe date-time may not be such an interesting predictor.\n\nIt could only bring in a general trend over time\n\nOften decomposing the date to the month or the day of the week (dow) is more interesting.\n\nstep_date can easily extract these\n\n\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time"
  },
  {
    "objectID": "W9.html#create-dummy-variables",
    "href": "W9.html#create-dummy-variables",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Create dummy variables",
    "text": "Create dummy variables\n\nUse helper functions like all_nominal or all_outcomes from tidymodels for column selection.\n\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) \n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time\nDummy variables from all_nominal(), -all_outcomes()"
  },
  {
    "objectID": "W9.html#remove-zero-variance-variables",
    "href": "W9.html#remove-zero-variance-variables",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Remove zero variance variables",
    "text": "Remove zero variance variables\nVariables that contain only a single value.\n\nemail_rec |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time\nDummy variables from all_nominal(), -all_outcomes()\nZero variance filter on all_predictors()"
  },
  {
    "objectID": "W9.html#full-recipe",
    "href": "W9.html#full-recipe",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Full recipe",
    "text": "Full recipe\n\nemail_rec <- recipe(\n spam ~ .,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n) |>\n step_rm(from, sent_email, viagra) |> \n step_date(time, features = c(\"dow\", \"month\")) |>\n step_rm(time) |> \n step_dummy(all_nominal(), -all_outcomes()) |> \n step_zv(all_predictors())\nemail_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         20\n\nOperations:\n\nVariables removed from, sent_email, viagra\nDate features from time\nVariables removed time\nDummy variables from all_nominal(), -all_outcomes()\nZero variance filter on all_predictors()\n\n\nThe object email_rec only includes meta-data of the data frame it shall work on (a formula, columns names and types)!"
  },
  {
    "objectID": "W9.html#define-model",
    "href": "W9.html#define-model",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Define model",
    "text": "Define model\n\nemail_mod <- logistic_reg() |> \n  set_engine(\"glm\")\n\nemail_mod\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "W9.html#define-workflow",
    "href": "W9.html#define-workflow",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Define workflow",
    "text": "Define workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nemail_wflow <- workflow() |> \n  add_model(email_mod) |> \n  add_recipe(email_rec)\nemail_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_rm()\n• step_date()\n• step_rm()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "W9.html#fit-model-to-training-data",
    "href": "W9.html#fit-model-to-training-data",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nemail_fit <- email_wflow |> \n  fit(data = train_data)\n\ntidy(email_fit) |> print(n = 27)\n\n# A tibble: 27 × 5\n   term            estimate std.error statistic  p.value\n   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)     -0.651     0.254     -2.57   1.03e- 2\n 2 cc               0.0214    0.0229     0.936  3.49e- 1\n 3 image           -2.99      1.31      -2.28   2.28e- 2\n 4 attach           0.512     0.116      4.41   1.03e- 5\n 5 dollar          -0.0651    0.0307    -2.12   3.40e- 2\n 6 inherit          0.440     0.205      2.15   3.14e- 2\n 7 password        -0.723     0.302     -2.39   1.67e- 2\n 8 num_char         0.0585    0.0240     2.43   1.50e- 2\n 9 line_breaks     -0.00548   0.00139   -3.94   8.24e- 5\n10 exclaim_subj     0.0998    0.268      0.373  7.09e- 1\n11 exclaim_mess     0.0103    0.00198    5.20   2.02e- 7\n12 to_multiple_X1  -2.56      0.339     -7.56   4.11e-14\n13 winner_yes       2.24      0.430      5.21   1.90e- 7\n14 format_X1       -0.953     0.157     -6.06   1.38e- 9\n15 re_subj_X1      -3.00      0.444     -6.76   1.39e-11\n16 urgent_subj_X1   3.69      1.15       3.20   1.37e- 3\n17 number_small    -0.840     0.162     -5.20   1.98e- 7\n18 number_big      -0.0915    0.244     -0.375  7.07e- 1\n19 time_dow_Mo     -0.326     0.303     -1.08   2.82e- 1\n20 time_dow_Di      0.0813    0.275      0.296  7.67e- 1\n21 time_dow_Mi     -0.260     0.275     -0.946  3.44e- 1\n22 time_dow_Do     -0.220     0.279     -0.788  4.31e- 1\n23 time_dow_Fr     -0.0612    0.275     -0.223  8.24e- 1\n24 time_dow_Sa      0.0646    0.292      0.221  8.25e- 1\n25 time_month_Feb   0.760     0.178      4.26   2.03e- 5\n26 time_month_Mär   0.506     0.178      2.85   4.40e- 3\n27 time_month_Apr -12.0     394.        -0.0306 9.76e- 1"
  },
  {
    "objectID": "W9.html#make-predictions-for-test-data",
    "href": "W9.html#make-predictions-for-test-data",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nemail_pred <- predict(email_fit, test_data, type = \"prob\") |> \n  bind_cols(test_data) \n\nemail_pred\n\n# A tibble: 785 × 23\n   .pred_0  .pred_1 spam  to_mul…¹ from     cc sent_…² time                image\n     <dbl>    <dbl> <fct> <fct>    <fct> <int> <fct>   <dttm>              <dbl>\n 1   0.993 0.00653  0     1        1         0 1       2012-01-01 18:55:06     0\n 2   0.998 0.00169  0     0        1         1 1       2012-01-01 20:38:32     0\n 3   0.987 0.0127   0     0        1         0 0       2012-01-02 06:42:16     0\n 4   0.999 0.000825 0     0        1         1 0       2012-01-02 16:12:51     0\n 5   0.991 0.00876  0     0        1         4 0       2012-01-02 17:45:36     0\n 6   0.878 0.122    0     0        1         0 0       2012-01-02 22:55:03     0\n 7   0.959 0.0414   0     0        1         0 0       2012-01-03 02:07:17     0\n 8   0.852 0.148    0     0        1         0 0       2012-01-03 06:41:35     0\n 9   0.938 0.0619   0     0        1         0 0       2012-01-03 17:02:35     0\n10   0.896 0.104    0     0        1         0 0       2012-01-03 12:14:51     0\n# … with 775 more rows, 14 more variables: attach <dbl>, dollar <dbl>,\n#   winner <fct>, inherit <dbl>, viagra <dbl>, password <dbl>, num_char <dbl>,\n#   line_breaks <int>, format <fct>, re_subj <fct>, exclaim_subj <dbl>,\n#   urgent_subj <fct>, exclaim_mess <dbl>, number <fct>, and abbreviated\n#   variable names ¹​to_multiple, ²​sent_email"
  },
  {
    "objectID": "W9.html#evaluate-the-performance-2",
    "href": "W9.html#evaluate-the-performance-2",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nemail_pred |>\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) |>\n  autoplot()"
  },
  {
    "objectID": "W9.html#evaluate-the-performance-3",
    "href": "W9.html#evaluate-the-performance-3",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nemail_pred |>\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.860\n\n\nThis is at least slightly better than our former model (without the feature engineering workflow), which had AUC = 0.857."
  },
  {
    "objectID": "W9.html#cutoff-probability-0.5",
    "href": "W9.html#cutoff-probability-0.5",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Cutoff probability: 0.5",
    "text": "Cutoff probability: 0.5\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5. (That is the default.)\nConfusion matrix:\n\ncutoff_prob <- 0.5\nemail_pred |>\n  mutate(\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(.pred_1 > cutoff_prob, \"Email labelled spam\", \"Email labelled not spam\")\n    ) |>\n  count(spam_pred, spam) |>\n  pivot_wider(names_from = spam, values_from = n) |>\n  knitr::kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail labelled not spam\n707\n54\n\n\nEmail labelled spam\n10\n14\n\n\n\n\n\nSensitivity: 14/(14+54) = 0.206\nSpecificity: 707/(707+10) = 0.986"
  },
  {
    "objectID": "W9.html#cutoff-probability-0.25",
    "href": "W9.html#cutoff-probability-0.25",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Cutoff probability: 0.25",
    "text": "Cutoff probability: 0.25\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25.\nConfusion matrix:\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail labelled not spam\n656\n36\n\n\nEmail labelled spam\n61\n32\n\n\n\n\n\nSensitivity: 32/(32+36) = 0.471\nSpecificity: 656/(656 + 61) = 0.915"
  },
  {
    "objectID": "W9.html#cutoff-probability-0.75",
    "href": "W9.html#cutoff-probability-0.75",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Cutoff probability: 0.75",
    "text": "Cutoff probability: 0.75\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75.\nConfusion matrix:\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail labelled not spam\n716\n65\n\n\nEmail labelled spam\n1\n3\n\n\n\n\n\nSensitivity: 3/(3+65) = 0.044\nSpecificity: 716/(716+1) = 0.999"
  },
  {
    "objectID": "W9.html#check-our-very-first-model",
    "href": "W9.html#check-our-very-first-model",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Check our very first model",
    "text": "Check our very first model\nWe make a new simple recipe and draw workflow and fitting re-using the same specified logisitc regression model email_mod.\n\nsimple_email_rec <- recipe(\n spam ~ num_char,          # formula\n data = train_data  # data to use for cataloguing names and types of variables\n)\nsimple_email_pred <- \n workflow() |> \n add_model(email_mod) |> \n add_recipe(simple_email_rec) |> \n fit(data = train_data) |> \n predict(test_data, type = \"prob\") |> \n bind_cols(test_data |> select(spam,num_char,time)) \nsimple_email_pred \n\n# A tibble: 785 × 5\n   .pred_0 .pred_1 spam  num_char time               \n     <dbl>   <dbl> <fct>    <dbl> <dttm>             \n 1   0.889  0.111  0        4.84  2012-01-01 18:55:06\n 2   0.936  0.0644 0       15.1   2012-01-01 20:38:32\n 3   0.945  0.0547 0       18.0   2012-01-02 06:42:16\n 4   0.989  0.0113 0       45.8   2012-01-02 16:12:51\n 5   0.922  0.0784 0       11.4   2012-01-02 17:45:36\n 6   0.868  0.132  0        1.48  2012-01-02 22:55:03\n 7   0.933  0.0667 0       14.4   2012-01-03 02:07:17\n 8   0.864  0.136  0        0.978 2012-01-03 06:41:35\n 9   0.905  0.0953 0        7.79  2012-01-03 17:02:35\n10   0.864  0.136  0        0.978 2012-01-03 12:14:51\n# … with 775 more rows"
  },
  {
    "objectID": "W9.html#evaluate-the-performance-4",
    "href": "W9.html#evaluate-the-performance-4",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\n\nsimple_email_pred |> roc_curve(\n    truth = spam, estimate = .pred_1,\n    event_level = \"second\" # this adjusts the location above the diagonal\n  ) |> autoplot()\n\n\n\n\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.753\n\n\nConclusion: It is not as good compared to AUC 0.86\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W9.html#multinomial-response-variable",
    "href": "W9.html#multinomial-response-variable",
    "title": "W#9 Classification Problems, Logistic Regression, Prediction",
    "section": "Multinomial response variable?",
    "text": "Multinomial response variable?\n\nWe will not cover other categorical variables than binary ones here.\nHowever, many of the probabilistic concepts transfer."
  },
  {
    "objectID": "index.html#week-9-oct-27-classification-problems-logisitc-regression-prediction",
    "href": "index.html#week-9-oct-27-classification-problems-logisitc-regression-prediction",
    "title": "Data Science Concepts / Tools",
    "section": "Week 9, Oct 27: Classification Problems, Logisitc Regression, Prediction",
    "text": "Week 9, Oct 27: Classification Problems, Logisitc Regression, Prediction\nSlides Week 9\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 due in 10 days."
  },
  {
    "objectID": "W10.html#model-1-predict-eu-attitudes",
    "href": "W10.html#model-1-predict-eu-attitudes",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Model 1: Predict EU attitudes",
    "text": "Model 1: Predict EU attitudes\n\ness <- ess_raw |> filter(essround == 9) |> \n select(cntry, euftf, atchctr, atcherp, imueclt, lrscale) |> \n mutate(euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99), \n        atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),\n        atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),\n        imueclt = imueclt |> na_if(77) |> na_if(88) |> na_if(99),\n        lrscale = lrscale |> na_if(77) |> na_if(88) |> na_if(99))\n\nFor the ESS dataset\n\nwe filter for people from round 9 (2018)\nselect 5 attitude variabels and ntry with 29 countries: AT, BE, BG, CH, CY, CZ, DE, DK, EE, ES, FI, FR, GB, HR, HU, IE, IS, IT, LT, LV, ME, NL, NO, PL, PT, RS, SE, SI, SK\nrecode NA’s properly for five variables:\neuftf: European Union: European unification go further (=10) or gone too far (=0)\natchctr: How emotionally attached to [country] (0 to 10)\natcherp: How emotionally attached to Europe (0 to 10)\nimueclt: Country’s cultural life undermined (=0) or enriched (=10) by immigrants\nlrscale: Placement on left (=0) right (=10) scale"
  },
  {
    "objectID": "W10.html#model-1.a-1.b-and-1.c",
    "href": "W10.html#model-1.a-1.b-and-1.c",
    "title": "W#10: Cross validation",
    "section": "Model 1.a, 1.b, and 1.c",
    "text": "Model 1.a, 1.b, and 1.c\n\nCreate an initial split with 80% training data\nCreate three recipes\n\ness_rec1 without using the country variable\ness_rec2 with main effects for all 29 countries\ness_rec3 with additional interaction effects for all 29 countries\n\n\n\ness_split <- initial_split(ess, prop = 0.80)\ness_train <- training(ess_split)\ness_test <- testing(ess_split)\n\ness_model <- linear_reg() |> set_engine(\"lm\")\ness_rec1 <- ess_train |> \n recipe(euftf ~ .) |> \n step_rm(cntry)\ness_rec2 <- ess_train |> \n recipe(euftf ~ .)\ness_rec3 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(all_nominal()) |> \n step_interact(~starts_with(\"cntry\"):c(atchctr,atcherp,imueclt,lrscale))\n\ness_wflow <- workflow() |> \n add_model(ess_model)\n\ness_fit1 <- ess_wflow |> \n add_recipe(ess_rec1) |> \n fit(ess_train)\ness_fit2 <- ess_wflow |> \n add_recipe(ess_rec2) |> \n fit(ess_train)\ness_fit3 <- ess_wflow |> \n add_recipe(ess_rec3) |> \n fit(ess_train)"
  },
  {
    "objectID": "W10.html#model-1-fits",
    "href": "W10.html#model-1-fits",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Model 1: Fits",
    "text": "Model 1: Fits\n\n\n\ntidy(ess_fit1) |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 5 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   2.92  \n2 atchctr      -0.0952\n3 atcherp       0.285 \n4 imueclt       0.287 \n5 lrscale      -0.0401\n\n\n\n\ntidy(ess_fit2)  |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 33 × 2\n   term        estimate\n   <chr>          <dbl>\n 1 (Intercept)  2.10   \n 2 atchctr     -0.101  \n 3 atcherp      0.294  \n 4 imueclt      0.299  \n 5 lrscale     -0.0196 \n 6 cntry_BE     0.747  \n 7 cntry_BG     1.20   \n 8 cntry_CH    -0.0476 \n 9 cntry_CY     1.39   \n10 cntry_CZ     0.184  \n11 cntry_DE     1.32   \n12 cntry_DK     1.01   \n13 cntry_EE     0.532  \n14 cntry_ES     1.14   \n15 cntry_FI     0.00996\n16 cntry_FR     0.743  \n17 cntry_GB    -0.0705 \n18 cntry_HR     0.729  \n19 cntry_HU     0.216  \n20 cntry_IE     0.294  \n21 cntry_IS     0.0371 \n22 cntry_IT     0.593  \n23 cntry_LT     2.15   \n24 cntry_LV     0.426  \n25 cntry_ME     2.41   \n26 cntry_NL     0.600  \n27 cntry_NO    -0.170  \n28 cntry_PL     1.19   \n29 cntry_PT     1.42   \n30 cntry_RS     1.21   \n31 cntry_SE    -0.0761 \n32 cntry_SI     1.53   \n33 cntry_SK     0.178  \n\n\n\n\ntidy(ess_fit3)  |> select(term, estimate) |> print(n = 145)\n\n# A tibble: 145 × 2\n    term               estimate\n    <chr>                 <dbl>\n  1 (Intercept)         2.65   \n  2 atchctr            -0.135  \n  3 atcherp             0.259  \n  4 imueclt             0.412  \n  5 lrscale            -0.141  \n  6 cntry_BE            0.0202 \n  7 cntry_BG           -0.264  \n  8 cntry_CH            1.70   \n  9 cntry_CY            0.242  \n 10 cntry_CZ           -0.914  \n 11 cntry_DE            0.248  \n 12 cntry_DK            0.637  \n 13 cntry_EE           -0.709  \n 14 cntry_ES            1.15   \n 15 cntry_FI            0.143  \n 16 cntry_FR            0.136  \n 17 cntry_GB            0.486  \n 18 cntry_HR            0.177  \n 19 cntry_HU           -0.190  \n 20 cntry_IE            0.186  \n 21 cntry_IS            0.881  \n 22 cntry_IT           -0.582  \n 23 cntry_LT            0.925  \n 24 cntry_LV           -1.60   \n 25 cntry_ME            0.542  \n 26 cntry_NL           -0.804  \n 27 cntry_NO            0.763  \n 28 cntry_PL            2.24   \n 29 cntry_PT            2.18   \n 30 cntry_RS            0.220  \n 31 cntry_SE           -0.507  \n 32 cntry_SI            1.19   \n 33 cntry_SK           -0.604  \n 34 cntry_BE_x_atchctr -0.0249 \n 35 cntry_BE_x_atcherp  0.128  \n 36 cntry_BE_x_imueclt -0.130  \n 37 cntry_BE_x_lrscale  0.148  \n 38 cntry_BG_x_atchctr  0.220  \n 39 cntry_BG_x_atcherp -0.0614 \n 40 cntry_BG_x_imueclt -0.159  \n 41 cntry_BG_x_lrscale  0.151  \n 42 cntry_CH_x_atchctr -0.0286 \n 43 cntry_CH_x_atcherp  0.0194 \n 44 cntry_CH_x_imueclt -0.197  \n 45 cntry_CH_x_lrscale -0.106  \n 46 cntry_CY_x_atchctr  0.0956 \n 47 cntry_CY_x_atcherp  0.0496 \n 48 cntry_CY_x_imueclt -0.336  \n 49 cntry_CY_x_lrscale  0.300  \n 50 cntry_CZ_x_atchctr -0.0375 \n 51 cntry_CZ_x_atcherp -0.0267 \n 52 cntry_CZ_x_imueclt  0.0735 \n 53 cntry_CZ_x_lrscale  0.273  \n 54 cntry_DE_x_atchctr  0.0299 \n 55 cntry_DE_x_atcherp  0.159  \n 56 cntry_DE_x_imueclt -0.127  \n 57 cntry_DE_x_lrscale  0.0836 \n 58 cntry_DK_x_atchctr -0.0884 \n 59 cntry_DK_x_atcherp  0.0752 \n 60 cntry_DK_x_imueclt  0.0109 \n 61 cntry_DK_x_lrscale  0.0970 \n 62 cntry_EE_x_atchctr  0.0895 \n 63 cntry_EE_x_atcherp  0.0464 \n 64 cntry_EE_x_imueclt -0.137  \n 65 cntry_EE_x_lrscale  0.180  \n 66 cntry_ES_x_atchctr  0.0735 \n 67 cntry_ES_x_atcherp -0.0706 \n 68 cntry_ES_x_imueclt -0.0704 \n 69 cntry_ES_x_lrscale  0.0210 \n 70 cntry_FI_x_atchctr -0.106  \n 71 cntry_FI_x_atcherp  0.0602 \n 72 cntry_FI_x_imueclt -0.104  \n 73 cntry_FI_x_lrscale  0.176  \n 74 cntry_FR_x_atchctr -0.0254 \n 75 cntry_FR_x_atcherp  0.185  \n 76 cntry_FR_x_imueclt -0.195  \n 77 cntry_FR_x_lrscale  0.139  \n 78 cntry_GB_x_atchctr -0.0968 \n 79 cntry_GB_x_atcherp  0.161  \n 80 cntry_GB_x_imueclt -0.196  \n 81 cntry_GB_x_lrscale  0.0630 \n 82 cntry_HR_x_atchctr  0.145  \n 83 cntry_HR_x_atcherp -0.0138 \n 84 cntry_HR_x_imueclt -0.213  \n 85 cntry_HR_x_lrscale  0.120  \n 86 cntry_HU_x_atchctr  0.0531 \n 87 cntry_HU_x_atcherp -0.0656 \n 88 cntry_HU_x_imueclt -0.0588 \n 89 cntry_HU_x_lrscale  0.160  \n 90 cntry_IE_x_atchctr  0.0630 \n 91 cntry_IE_x_atcherp -0.0230 \n 92 cntry_IE_x_imueclt -0.138  \n 93 cntry_IE_x_lrscale  0.0874 \n 94 cntry_IS_x_atchctr -0.186  \n 95 cntry_IS_x_atcherp  0.111  \n 96 cntry_IS_x_imueclt -0.0756 \n 97 cntry_IS_x_lrscale  0.0596 \n 98 cntry_IT_x_atchctr  0.0381 \n 99 cntry_IT_x_atcherp -0.0405 \n100 cntry_IT_x_imueclt  0.114  \n101 cntry_IT_x_lrscale  0.111  \n102 cntry_LT_x_atchctr  0.145  \n103 cntry_LT_x_atcherp -0.113  \n104 cntry_LT_x_imueclt -0.0917 \n105 cntry_LT_x_lrscale  0.219  \n106 cntry_LV_x_atchctr  0.130  \n107 cntry_LV_x_atcherp -0.0431 \n108 cntry_LV_x_imueclt -0.161  \n109 cntry_LV_x_lrscale  0.365  \n110 cntry_ME_x_atchctr  0.198  \n111 cntry_ME_x_atcherp  0.136  \n112 cntry_ME_x_imueclt -0.179  \n113 cntry_ME_x_lrscale  0.0701 \n114 cntry_NL_x_atchctr  0.0416 \n115 cntry_NL_x_atcherp  0.138  \n116 cntry_NL_x_imueclt -0.130  \n117 cntry_NL_x_lrscale  0.182  \n118 cntry_NO_x_atchctr -0.0643 \n119 cntry_NO_x_atcherp -0.0283 \n120 cntry_NO_x_imueclt -0.202  \n121 cntry_NO_x_lrscale  0.187  \n122 cntry_PL_x_atchctr -0.00472\n123 cntry_PL_x_atcherp  0.0737 \n124 cntry_PL_x_imueclt -0.231  \n125 cntry_PL_x_lrscale -0.0290 \n126 cntry_PT_x_atchctr  0.0377 \n127 cntry_PT_x_atcherp -0.114  \n128 cntry_PT_x_imueclt -0.178  \n129 cntry_PT_x_lrscale  0.126  \n130 cntry_RS_x_atchctr  0.139  \n131 cntry_RS_x_atcherp  0.0996 \n132 cntry_RS_x_imueclt -0.285  \n133 cntry_RS_x_lrscale  0.167  \n134 cntry_SE_x_atchctr  0.0493 \n135 cntry_SE_x_atcherp -0.00505\n136 cntry_SE_x_imueclt -0.138  \n137 cntry_SE_x_lrscale  0.160  \n138 cntry_SI_x_atchctr  0.199  \n139 cntry_SI_x_atcherp -0.165  \n140 cntry_SI_x_imueclt -0.256  \n141 cntry_SI_x_lrscale  0.204  \n142 cntry_SK_x_atchctr  0.0955 \n143 cntry_SK_x_atcherp -0.0549 \n144 cntry_SK_x_imueclt -0.0424 \n145 cntry_SK_x_lrscale  0.137  \n\n\n\n\nNote: We omit std.error, p-values and so on in the display here because they are usually small in this large dataset, and we will not look at them now."
  },
  {
    "objectID": "W10.html#recap-interpreting-with-interaction-effects",
    "href": "W10.html#recap-interpreting-with-interaction-effects",
    "title": "W#10: Cross validation",
    "section": "Recap interpreting with interaction effects",
    "text": "Recap interpreting with interaction effects\n\ness_train |> \n filter(cntry==\"AT\") |> \n select(-cntry) |> \n lm(formula = euftf ~ . ) |> \n tidy()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.92     0.377       7.74 1.68e-14\n2 atchctr       -0.176    0.0369     -4.77 1.98e- 6\n3 atcherp        0.276    0.0309      8.95 9.16e-19\n4 imueclt        0.402    0.0274     14.6  8.05e-46\n5 lrscale       -0.135    0.0354     -3.82 1.36e- 4"
  },
  {
    "objectID": "W10.html#ne",
    "href": "W10.html#ne",
    "title": "W#10: Cross validation",
    "section": "Ne",
    "text": "Ne\n\n\n# A tibble: 785 × 2\n   .pred_0   .pred_1\n     <dbl>     <dbl>\n 1   0.931 0.0688   \n 2   0.996 0.00444  \n 3   0.994 0.00555  \n 4   0.914 0.0856   \n 5   0.775 0.225    \n 6   0.670 0.330    \n 7   1.00  0.000330 \n 8   1.00  0.0000793\n 9   0.997 0.00339  \n10   0.900 0.0995   \n# … with 775 more rows\n\n\n\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [169/19]> Fold01\n 2 <split [169/19]> Fold02\n 3 <split [169/19]> Fold03\n 4 <split [169/19]> Fold04\n 5 <split [169/19]> Fold05\n 6 <split [169/19]> Fold06\n 7 <split [169/19]> Fold07\n 8 <split [169/19]> Fold08\n 9 <split [170/18]> Fold09\n10 <split [170/18]> Fold10\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#model-1.1-1.2-and-1.3",
    "href": "W10.html#model-1.1-1.2-and-1.3",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Model 1.1, 1.2, and 1.3",
    "text": "Model 1.1, 1.2, and 1.3\n\n\n\nCreate an initial split with 80% training data\nCreate a linear model ess_mod\nCreate three recipes\n\ness_rec1 without using the country variable\ness_rec2 with main effects for all 29 countries\ness_rec3 with additional interaction effects for all 29 countries\n\nCreate the workflow\nFit 3 models by adding the 3 recipes\n\n\n\nset.seed(7)\ness_split <- initial_split(ess, prop = 0.80)\ness_train <- training(ess_split)\ness_test <- testing(ess_split)\n\ness_model <- linear_reg() |> set_engine(\"lm\")\ness_rec1 <- ess_train |> \n recipe(euftf ~ .) |> \n step_rm(cntry)\ness_rec2 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry)\ness_rec3 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry) |> \n step_interact(~starts_with(\"cntry\"):c(atchctr,atcherp,imueclt,lrscale))\n\ness_wflow <- workflow() |> \n add_model(ess_model)\n\ness_fit1 <- ess_wflow |> \n add_recipe(ess_rec1) |> \n fit(ess_train)\ness_fit2 <- ess_wflow |> \n add_recipe(ess_rec2) |> \n fit(ess_train)\ness_fit3 <- ess_wflow |> \n add_recipe(ess_rec3) |> \n fit(ess_train)"
  },
  {
    "objectID": "W10.html#recap-interpreting-interaction-effects",
    "href": "W10.html#recap-interpreting-interaction-effects",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Recap: Interpreting interaction effects",
    "text": "Recap: Interpreting interaction effects\nIn Model 3 the reference country is Austria (AT) therefore the intercept and main coefficient are valid for Austria and all interaction coefficients have to be added to these to be interpreted.\nCross check: A linear model with the data filtered for Austria only without a country effect:\n\ness_train |> \n filter(cntry==\"AT\") |> \n select(-cntry) |> \n lm(formula = euftf ~ . ) |> \n tidy()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    2.65     0.364       7.28 5.06e-13\n2 atchctr       -0.135    0.0353     -3.83 1.34e- 4\n3 atcherp        0.259    0.0296      8.76 4.51e-18\n4 imueclt        0.412    0.0268     15.3  6.49e-50\n5 lrscale       -0.141    0.0338     -4.16 3.34e- 5\n\n\nThe coefficients are identical to the full model with all interaction effects."
  },
  {
    "objectID": "W10.html#make-predictions-for-test-data",
    "href": "W10.html#make-predictions-for-test-data",
    "title": "W#10: Cross validation",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#make-predictions-for-training-data",
    "href": "W10.html#make-predictions-for-training-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\ness_train_pred1 <- predict(ess_fit1, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred1\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.27     3 HR          8       7       5       8\n 2  4.93     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.20    NA BG         10       7       5       5\n 6  5.67     8 BG          9       6       8      10\n 7  5.52     3 AT         10      10       3       4\n 8  7.22     3 FR          9       8      10       0\n 9  4.17     1 FI          8       5       3       7\n10  4.47     4 BG         10       7       3       9\n# … with 39,605 more rows\n\n\nNote:\n\nWe can make predictions when the response in NA\nWe cannot make predictions when on predictor is NA"
  },
  {
    "objectID": "W10.html#make-predictions-for-training-data-1",
    "href": "W10.html#make-predictions-for-training-data-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\n\n\ness_train_pred2 <- predict(ess_fit2, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred2\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.42     3 HR          8       7       5       8\n 2  5.53     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.75    NA BG         10       7       5       5\n 6  6.36     8 BG          9       6       8      10\n 7  4.86     3 AT         10      10       3       4\n 8  7.28     3 FR          9       8      10       0\n 9  3.54     1 FI          8       5       3       7\n10  5.08     4 BG         10       7       3       9\n# … with 39,605 more rows\n\n\n\n\ness_train_pred3 <- predict(ess_fit3, ess_train) |> \n bind_cols(ess_train |> select(euftf, everything()))\ness_train_pred3\n\n# A tibble: 39,615 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.45     3 HR          8       7       5       8\n 2  5.22     6 BG          6       5       5       7\n 3 NA        3 IE         10       9       3      NA\n 4 NA       NA EE         10      10       6      NA\n 5  5.94    NA BG         10       7       5       5\n 6  6.47     8 BG          9       6       8      10\n 7  4.56     3 AT         10      10       3       4\n 8  7.06     3 FR          9       8      10       0\n 9  3.63     1 FI          8       5       3       7\n10  5.47     4 BG         10       7       3       9\n# … with 39,605 more rows"
  },
  {
    "objectID": "W10.html#r-squared",
    "href": "W10.html#r-squared",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "R-squared",
    "text": "R-squared\nRecap R-squared: Percentage of variability in euftf explained by the model\n\n\nrsq(ess_train_pred1, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.178\n\n\n\n\n\nrsq(ess_train_pred2, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.230\n\n\n\n\n\nrsq(ess_train_pred3, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.253\n\n\n\nWhich model is better in prediction?"
  },
  {
    "objectID": "W10.html#root-mean-squared-error-rmse",
    "href": "W10.html#root-mean-squared-error-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Root mean squared error (RMSE)",
    "text": "Root mean squared error (RMSE)\nRMSE is an alternative measure of performance.\n\\[\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}\\]\nwhere \\(\\hat{y}_i\\) is the predicted value and \\(y_i\\) the true value.\n(The name RMSE pretty much describes what the measure does.)\n\n\nrmse(ess_train_pred1, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.44\n\n\n\n\n\nrmse(ess_train_pred2, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.36\n\n\n\n\n\nrmse(ess_train_pred3, \n     truth = euftf, \n     estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.32"
  },
  {
    "objectID": "W10.html#interpreting-rmse",
    "href": "W10.html#interpreting-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\nIn contrast to R-squared, RMSE can only be interpreted with knowledge about the range and of the response variable.\nThe values of euftf range from 0 to 10\n\nThe RMSE of 2.3244529 shows how much predicted values deviate from the true value on average. (Taking the squaring of differences and root of the average into account.)"
  },
  {
    "objectID": "W10.html#make-predictions-for-testing-data",
    "href": "W10.html#make-predictions-for-testing-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\ness_test_pred3 <- predict(ess_fit3, ess_test) |> \n bind_cols(ess_test |> select(euftf, everything()))\ness_test_pred3\n\n# A tibble: 9,904 × 7\n   .pred euftf cntry atchctr atcherp imueclt lrscale\n   <dbl> <dbl> <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  3.93     5 AT          9       6       4       5\n 2 NA        2 AT         10       3       7      NA\n 3  4.49     4 AT          5       5       4       3\n 4  2.56     3 AT          8       6       0       4\n 5  7.73    10 AT         10      10      10       2\n 6  6.66     8 AT          5       7       8       3\n 7  2.87     2 AT         10       4       3       5\n 8  3.43     5 AT         10       3       5       5\n 9  7.19    10 AT          7       9       8       1\n10  3.24     8 AT         10       6       3       6\n# … with 9,894 more rows"
  },
  {
    "objectID": "W10.html#training-vs.-testing-data-prediction",
    "href": "W10.html#training-vs.-testing-data-prediction",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Training vs. testing data prediction",
    "text": "Training vs. testing data prediction\n\n\n\n\n\nModel\nMetric\nTrain\nTest\n\n\n\n\n1\nR-squared\n0.178\n0.169\n\n\n1\nRMSE\n2.440\n2.451\n\n\n2\nR-squared\n0.230\n0.216\n\n\n2\nRMSE\n2.360\n2.381\n\n\n3\nR-squared\n0.253\n0.242\n\n\n3\nRMSE\n2.324\n2.340\n\n\n\n\n\n\nR-squared is a little lower in the test data, RMSE a bit higher (both mean lower performance)\nOften, metrics are worse for the testing data, as here.\nHowever, in it can also be the other way round by chance."
  },
  {
    "objectID": "W10.html#how-to-evaluate-performance-on-training-data",
    "href": "W10.html#how-to-evaluate-performance-on-training-data",
    "title": "W#10: Cross validation",
    "section": "How to evaluate performance on training data?",
    "text": "How to evaluate performance on training data?\n\nModel performance changes with the random selection of the training data. How can we then reliably compare models?\nAnyway, the training data is not a good source for model performance. It is not an independent piece of information. Predicting the training data only reveals what the model already “knows”.\nAlso, we should save the testing data only for the final validation, so we should not use it systematically to compare models.\n\nA solution: Cross validation"
  },
  {
    "objectID": "W10.html#cross-validation-1",
    "href": "W10.html#cross-validation-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, \\(v\\)-fold cross validation:\n\nShuffle your data and make a partition with \\(v\\) parts\n\nRecall from set theory: A partition is a division of a set into mutually disjoint parts which union cover the whole set. Here applied to observations (rows) in a data frame.\n\nUse 1 part for validation, and the remaining \\(v-1\\) parts for training\nRepeat \\(v\\) times"
  },
  {
    "objectID": "W10.html#split-data-into-folds",
    "href": "W10.html#split-data-into-folds",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Split data into folds",
    "text": "Split data into folds\nWe split the ess data into ten parts.\n\n\n\nfolds <- vfold_cv(ess_train, v = 10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits               id    \n   <list>               <chr> \n 1 <split [35653/3962]> Fold01\n 2 <split [35653/3962]> Fold02\n 3 <split [35653/3962]> Fold03\n 4 <split [35653/3962]> Fold04\n 5 <split [35653/3962]> Fold05\n 6 <split [35654/3961]> Fold06\n 7 <split [35654/3961]> Fold07\n 8 <split [35654/3961]> Fold08\n 9 <split [35654/3961]> Fold09\n10 <split [35654/3961]> Fold10"
  },
  {
    "objectID": "W10.html#cross-validation-2",
    "href": "W10.html#cross-validation-2",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "W10.html#fit-resamples",
    "href": "W10.html#fit-resamples",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Fit resamples",
    "text": "Fit resamples\nWe use the workflow (model plus formula and recipe) we have on the folds with fit_resamples.\n\ness_fit3_rs <- ess_wflow |> add_recipe(ess_rec3) |> \n fit_resamples(folds)\ness_fit3_rs\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits               id     .metrics         .notes          \n   <list>               <chr>  <list>           <list>          \n 1 <split [35653/3962]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]>\n 2 <split [35653/3962]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]>\n 3 <split [35653/3962]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]>\n 4 <split [35653/3962]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]>\n 5 <split [35653/3962]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]>\n 6 <split [35654/3961]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]>\n 7 <split [35654/3961]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]>\n 8 <split [35654/3961]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]>\n 9 <split [35654/3961]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]>\n10 <split [35654/3961]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]>\n\n\nThis computes a set of performance metrics for each folds. For linear models the defaults are R-squared and RMSE."
  },
  {
    "objectID": "W10.html#collect-the-metrics",
    "href": "W10.html#collect-the-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Collect the metrics",
    "text": "Collect the metrics\n\ness_fit3_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   2.34     10 0.00794 Preprocessor1_Model1\n2 rsq     standard   0.245    10 0.00667 Preprocessor1_Model1\n\n\nThese values are indeed closer to the values we got for the test data."
  },
  {
    "objectID": "W10.html#deeper-look-into-the-metrics",
    "href": "W10.html#deeper-look-into-the-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Deeper look into the metrics",
    "text": "Deeper look into the metrics\n\n\n\ness_fit3_rs |> collect_metrics(summarize = FALSE)\n\n# A tibble: 20 × 5\n   id     .metric .estimator .estimate .config             \n   <chr>  <chr>   <chr>          <dbl> <chr>               \n 1 Fold01 rmse    standard       2.29  Preprocessor1_Model1\n 2 Fold01 rsq     standard       0.278 Preprocessor1_Model1\n 3 Fold02 rmse    standard       2.35  Preprocessor1_Model1\n 4 Fold02 rsq     standard       0.238 Preprocessor1_Model1\n 5 Fold03 rmse    standard       2.33  Preprocessor1_Model1\n 6 Fold03 rsq     standard       0.258 Preprocessor1_Model1\n 7 Fold04 rmse    standard       2.38  Preprocessor1_Model1\n 8 Fold04 rsq     standard       0.249 Preprocessor1_Model1\n 9 Fold05 rmse    standard       2.34  Preprocessor1_Model1\n10 Fold05 rsq     standard       0.250 Preprocessor1_Model1\n11 Fold06 rmse    standard       2.32  Preprocessor1_Model1\n12 Fold06 rsq     standard       0.261 Preprocessor1_Model1\n13 Fold07 rmse    standard       2.36  Preprocessor1_Model1\n14 Fold07 rsq     standard       0.215 Preprocessor1_Model1\n15 Fold08 rmse    standard       2.35  Preprocessor1_Model1\n16 Fold08 rsq     standard       0.214 Preprocessor1_Model1\n17 Fold09 rmse    standard       2.33  Preprocessor1_Model1\n18 Fold09 rsq     standard       0.257 Preprocessor1_Model1\n19 Fold10 rmse    standard       2.35  Preprocessor1_Model1\n20 Fold10 rsq     standard       0.226 Preprocessor1_Model1\n\n\n\n\n\n\n\n\nid\nrmse\nrsq\n\n\n\n\nFold01\n2.286402\n0.2780252\n\n\nFold02\n2.352520\n0.2378150\n\n\nFold03\n2.334960\n0.2579689\n\n\nFold04\n2.376705\n0.2491427\n\n\nFold05\n2.338995\n0.2503009\n\n\nFold06\n2.315747\n0.2607719\n\n\nFold07\n2.362725\n0.2147322\n\n\nFold08\n2.347958\n0.2137093\n\n\nFold09\n2.331361\n0.2568661\n\n\nFold10\n2.345771\n0.2258870"
  },
  {
    "objectID": "W10.html#cross-validation-for-logistic-regression",
    "href": "W10.html#cross-validation-for-logistic-regression",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation for logistic regression",
    "text": "Cross validation for logistic regression\nTake 2 simple models predicting the sex of penguins.\n\nlibrary(palmerpenguins)\npenguins <- na.omit(penguins)\nset.seed(9999)\npeng_split <- initial_split(penguins, prob = 0.8)\npeng_train <- training(peng_split)\npeng_test <- testing(peng_split)\npeng_folds <- vfold_cv(peng_train, v = 5)\n\npeng_rec1 <- peng_train |> \n recipe(sex ~ flipper_length_mm + body_mass_g, family = \"binomial\")\npeng_rec2 <- peng_train |> \n recipe(sex ~ bill_depth_mm + bill_length_mm, family = \"binomial\")  \npeng_mod <- logistic_reg() |> set_engine(\"glm\")\npeng_wflow1 <- workflow() |> add_model(peng_mod) |> add_recipe(peng_rec1)\npeng_wflow2 <- workflow() |> add_model(peng_mod) |> add_recipe(peng_rec2)\n\npeng_fit1 <- peng_wflow1 |> fit(peng_train)\npeng_fit2 <- peng_wflow2 |> fit(peng_train)\n\n\n\n\n\n# A tibble: 3 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        8.72     3.02          2.89 3.90e- 3\n2 flipper_length_mm -0.0988   0.0225       -4.38 1.16e- 5\n3 body_mass_g        0.00267  0.000437      6.12 9.59e-10\n\n\n\n\n\n# A tibble: 3 × 5\n  term           estimate std.error statistic  p.value\n  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     -24.4      3.29       -7.40 1.33e-13\n2 bill_depth_mm     0.791    0.111       7.13 1.01e-12\n3 bill_length_mm    0.249    0.0407      6.13 8.97e-10"
  },
  {
    "objectID": "W10.html#accuracy",
    "href": "W10.html#accuracy",
    "title": "W#10: Cross validation",
    "section": "Accuracy",
    "text": "Accuracy\n\nAccuracy is the fraction of correct predictions: (TP + TN) / (TP + FP + FN + TN)\nRecall:\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)"
  },
  {
    "objectID": "W10.html#how-to-evaluate-performance-on-training-data-only",
    "href": "W10.html#how-to-evaluate-performance-on-training-data-only",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "How to evaluate performance on training data only?",
    "text": "How to evaluate performance on training data only?\n\nModel performance changes with the random selection of the training data. How can we then reliably compare models?\nAnyway, the training data is not a good source for model performance. It is not an independent piece of information. Predicting the training data only reveals what the model already “knows”.\nAlso, we should save the testing data only for the final validation, so we should not use it systematically to compare models.\n\nA solution: Cross validation"
  },
  {
    "objectID": "W10.html#cross-validation-for-logistic-regression-1",
    "href": "W10.html#cross-validation-for-logistic-regression-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Cross validation for logistic regression",
    "text": "Cross validation for logistic regression\n\npeng_fit1_rs <- peng_wflow1 |> fit_resamples(peng_folds)\npeng_fit2_rs <- peng_wflow2 |> fit_resamples(peng_folds)\npeng_fit1_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.654     5  0.0298 Preprocessor1_Model1\n2 roc_auc  binary     0.755     5  0.0268 Preprocessor1_Model1\n\npeng_fit2_rs |> collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.783     5  0.0240 Preprocessor1_Model1\n2 roc_auc  binary     0.857     5  0.0123 Preprocessor1_Model1\n\n\n\nFor the logistic regression fit_resamples has two performance measures per default: AUC (area under the ROC-curve) and accuracy.\nThe model using the two variables about penguin bills performs better than the model using body mass and flipper length."
  },
  {
    "objectID": "W10.html#accuracy-for-classifiers",
    "href": "W10.html#accuracy-for-classifiers",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Accuracy for classifiers",
    "text": "Accuracy for classifiers\n\n\nAccuracy is the fraction of correct predictions:\n(TP + TN) / (TP + FP + FN + TN)\n\nAccuracy is a good overall performance measure but it does not specify if errors are false positives or false negatives.\n\nFor logistic regression:\n\n\n\n\n\nAccuracy relies a decision threshold (default 50%). It has an easy interpretation.\nAUC is the area under the ROC-curve sweeping over all possible thresholds.\n\nRecall:\nSensitivity is the true positive rate: TP / (TP + FN)\nSpecificity is the true negative rate: TN / (TN + FP)"
  },
  {
    "objectID": "W10.html#comparison-of-metrics",
    "href": "W10.html#comparison-of-metrics",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\n\n\nFlipper length, body mass\n\npeng_fit1_rs |> collect_metrics(summarize = FALSE) |> select(-.config) |> arrange(.metric)\n\n# A tibble: 10 × 4\n   id    .metric  .estimator .estimate\n   <chr> <chr>    <chr>          <dbl>\n 1 Fold1 accuracy binary         0.76 \n 2 Fold2 accuracy binary         0.6  \n 3 Fold3 accuracy binary         0.62 \n 4 Fold4 accuracy binary         0.68 \n 5 Fold5 accuracy binary         0.612\n 6 Fold1 roc_auc  binary         0.851\n 7 Fold2 roc_auc  binary         0.689\n 8 Fold3 roc_auc  binary         0.745\n 9 Fold4 roc_auc  binary         0.728\n10 Fold5 roc_auc  binary         0.761\n\npeng_test_pred1 <- predict(peng_fit1, new_data = peng_test, type = \"prob\") |> \n mutate(.pred_class_0.5 = if_else(.pred_female > 0.5, \"female\", \"male\") |> factor(),\n        .pred_class_0.45 = if_else(.pred_female > 0.45, \"female\", \"male\") |> factor(),\n        .pred_class_0.55 = if_else(.pred_female > 0.55, \"female\", \"male\") |> factor()\n        ) |> \n bind_cols(peng_test |> select(sex, everything())) \npeng_test_pred1 |> roc_auc(truth = sex, estimate = .pred_female)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.830\n\n\n\nBill length and depth\n\npeng_fit2_rs |> collect_metrics(summarize = FALSE) |> select(-.config) |> arrange(.metric)\n\n# A tibble: 10 × 4\n   id    .metric  .estimator .estimate\n   <chr> <chr>    <chr>          <dbl>\n 1 Fold1 accuracy binary         0.8  \n 2 Fold2 accuracy binary         0.86 \n 3 Fold3 accuracy binary         0.78 \n 4 Fold4 accuracy binary         0.76 \n 5 Fold5 accuracy binary         0.714\n 6 Fold1 roc_auc  binary         0.851\n 7 Fold2 roc_auc  binary         0.889\n 8 Fold3 roc_auc  binary         0.881\n 9 Fold4 roc_auc  binary         0.840\n10 Fold5 roc_auc  binary         0.825\n\npeng_test_pred2 <- predict(peng_fit2, new_data = peng_test, type = \"prob\") |> \n mutate(.pred_class_0.5 = if_else(.pred_female > 0.5, \"female\", \"male\") |> factor(),\n        .pred_class_0.45 = if_else(.pred_female > 0.45, \"female\", \"male\") |> factor(),\n        .pred_class_0.55 = if_else(.pred_female > 0.55, \"female\", \"male\") |> factor()\n        ) |> \n bind_cols(peng_test |> select(sex, everything())) \npeng_test_pred2 |> roc_auc(truth = sex, estimate = .pred_female)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.839"
  },
  {
    "objectID": "W10.html#galtons-data",
    "href": "W10.html#galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Galton’s data",
    "text": "Galton’s data\nWhat is the weight of the meat of this ox?\n\nlibrary(readxl)\ngalton <- read_excel(\"data/galton_data.xlsx\")\ngalton |> ggplot(aes(Estimate)) + geom_histogram(binwidth = 5) + geom_vline(xintercept = 1198, color = \"green\") + \n geom_vline(xintercept = mean(galton$Estimate), color = \"red\")\n\n\n\n\n787 estimates, true value 1198, mean 1196.7\n\n\nWe focus on the arithmetic mean as aggregation function for the wisdom of the crowd here."
  },
  {
    "objectID": "W10.html#rmse-galtons-data",
    "href": "W10.html#rmse-galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "RMSE Galton’s data",
    "text": "RMSE Galton’s data\nDescribe the estimation game as a predictive model:\n\nAll estimates are made to predict the same value: the truth.\n\nIn contrast to the regression model, the estimate come from people and not from a regression formula.\n\nThe truth is the same for all.\n\nIn contrast to the regression model, the truth is one value and not a value for each prediction\n\n\n\nrmse_galton <- galton |> \n mutate(true_value = 1198) |>\n rmse(truth = true_value, estimate = Estimate)\nrmse_galton\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        73.6"
  },
  {
    "objectID": "W10.html#the-diversity-prediction-theorem",
    "href": "W10.html#the-diversity-prediction-theorem",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "The diversity prediction theorem1",
    "text": "The diversity prediction theorem1\n\nMSE is a measure the average individuals error\nBias-squared is a measure the collective error\nVariance is a measure for the diversity of estimates around the mean\n\nThe mathematical relation \\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\] can be formulated as\nCollective error = Individual error - Diversity\nInterpretation: The higher the diversity the lower the collective error!\nNotion from: Page, S. E. (2007). The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies. Princeton University Press."
  },
  {
    "objectID": "W10.html#mse-variance-and-bias-of-estimates",
    "href": "W10.html#mse-variance-and-bias-of-estimates",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "MSE, Variance, and Bias of estimates",
    "text": "MSE, Variance, and Bias of estimates\nIn a crowd estimation, \\(n\\) estimators delivered the estimates \\(\\hat{y}_1,\\dots,\\hat{y}_n\\). Let us look at the following measures\n\n\\(\\bar{y} = \\frac{1}{n}\\sum_{i = 1}^n \\hat{y}_i^2\\) is the mean estimate, it is the aggregated estimate of the crowd\n\\(\\text{MSE} = \\text{RMSE}^2 = \\frac{1}{n}\\sum_{i = 1}^n (\\text{truth} - \\hat{y}_i)^2\\)\n\\(\\text{Variance} = \\frac{1}{n}\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)\n\\(\\text{Bias-squared} = (\\bar{y} - \\text{truth})^2\\) which is the square difference between truth and mean estimate.\n\nThere is a mathematical relation (a math exercise to check):\n\\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\]"
  },
  {
    "objectID": "W10.html#testing-for-galtons-data",
    "href": "W10.html#testing-for-galtons-data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Testing for Galton’s data",
    "text": "Testing for Galton’s data\n\\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\]\n\nMSE <- (rmse_galton$.estimate)^2 \nMSE\n\n[1] 5409.795\n\nVariance <- var(galton$Estimate)*(nrow(galton)-1)/nrow(galton)\n# Note, we had to correct for the divisor (n-1) in the classical statistical definition\n# to get the sample variance instead of the estimate for the population variance\nVariance\n\n[1] 5408.132\n\nBias_squared <- (mean(galton$Estimate) - 1198)^2\nBias_squared\n\n[1] 1.663346\n\nBias_squared + Variance\n\n[1] 5409.795\n\n\n\n\nSuch nice mathematical properties are probably one reason why these squared measures are so popular."
  },
  {
    "objectID": "W10.html#why-is-this-message-a-bit-suggestive",
    "href": "W10.html#why-is-this-message-a-bit-suggestive",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Why is this message a bit suggestive?",
    "text": "Why is this message a bit suggestive?\nThe mathematical relation \\[\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\] can be formulated as\nCollective error = Individual error - Diversity\nInterpretation: The higher the diversity the lower the collective error!\n\n\n\\(\\text{MSE}\\) and \\(\\text{Variance}\\) are not independent!\nActivities to increase diversity (Variance) typically also increase the average individual error (MSE).\nFor example, if we just add more random estimates with same mean but wild variance to our sample we increase both and do not gain any decrease of the collective error."
  },
  {
    "objectID": "W10.html#accuracy-for-numerical-estimate",
    "href": "W10.html#accuracy-for-numerical-estimate",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Accuracy for numerical estimate",
    "text": "Accuracy for numerical estimate\n\nFor binary classifiers accuracy has a simple definition: Fraction of correct classifications.\n\nIt can be further informed by other more specific measures taken from the confusion matrix (sensitivity, specificity)\n\n\nHow about numerical estimators?\nFor example outcomes of estimation games, or linear regression models.\n\nAccuracy is for example measured by (R)MSE\n\\(\\text{MSE} = \\text{Bias-squared} + \\text{Variance}\\) shows us that we can make a\nbias-variance decomposition\nThat means some part of the error is a systematic (the bias) and another part due to random variation (the variance).\nLearn more about the bias-variance tradeoff in statistical learning independently! It is an important concept to understand predictive models."
  },
  {
    "objectID": "W10.html#d-accuracy-trueness-and-precision",
    "href": "W10.html#d-accuracy-trueness-and-precision",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "2-d Accuracy: Trueness and Precision",
    "text": "2-d Accuracy: Trueness and Precision\nAccording to ISO 5725-1 Standard: Accuracy (trueness and precision) of measurement methods and results - Part 1: General principles and definitions. there are two dimension of accuracy of numerical measurement."
  },
  {
    "objectID": "W10.html#section",
    "href": "W10.html#section",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "",
    "text": "JU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#what-is-a-wise-crowd",
    "href": "W10.html#what-is-a-wise-crowd",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "What is a wise crowd?",
    "text": "What is a wise crowd?\nAssume the dots are estimates. Which is a wise crowd?\n\n\n\nOf course, high trueness and high precision! But, …\nFocusing on the crowd being wise instead of its individuals: High trueness, low precision."
  },
  {
    "objectID": "W10.html#organ-donors",
    "href": "W10.html#organ-donors",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Organ donors",
    "text": "Organ donors\nPeople providing an organ for donation sometimes seek the help of a special “medical consultant”. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients.\nOne consultant tried to attract patients by noting that the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!)."
  },
  {
    "objectID": "W10.html#data",
    "href": "W10.html#data",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Data",
    "text": "Data\n\norgan_donor <- tibble(\n  outcome = c(rep(\"complication\", 3), rep(\"no complication\", 59))\n)\n\n\norgan_donor |>\n  count(outcome)\n\n# A tibble: 2 × 2\n  outcome             n\n  <chr>           <int>\n1 complication        3\n2 no complication    59"
  },
  {
    "objectID": "W10.html#parameter-vs.-statistic",
    "href": "W10.html#parameter-vs.-statistic",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Parameter vs. statistic",
    "text": "Parameter vs. statistic\nA parameter for a hypothesis test is the “true” value of interest. We typically estimate the parameter using a sample statistic as a point estimate.\n\\(p\\): true rate of complication, here 0.1 (10% complication rate in US)\n\\(\\hat{p}\\): rate of complication in the sample = \\(\\frac{3}{62}\\) = 0.048"
  },
  {
    "objectID": "W10.html#correlation-vs.-causation",
    "href": "W10.html#correlation-vs.-causation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Correlation vs. causation",
    "text": "Correlation vs. causation\nIs it possible to assess the consultant’s claim using the data?\nNo. The claim is: There is a causal connection, but the data are observational. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate (for example).\nWhile it is not possible to assess the causal claim, it is still possible to test for an association using these data. For this question we ask, could the low complication rate of \\(\\hat{p}\\) = 0.048 be due to chance?"
  },
  {
    "objectID": "W10.html#two-claims",
    "href": "W10.html#two-claims",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Two claims",
    "text": "Two claims\n\nNull hypothesis: “There is nothing going on”\n\nComplication rate for this consultant is no different than the US average of 10%\n\nAlternative hypothesis: “There is something going on”\n\nComplication rate for this consultant is lower than the US average of 10%"
  },
  {
    "objectID": "W10.html#hypothesis-testing-as-a-court-trial",
    "href": "W10.html#hypothesis-testing-as-a-court-trial",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "W10.html#hypothesis-testing-framework",
    "href": "W10.html#hypothesis-testing-framework",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\), that represents the status quo\nSet an alternative hypothesis, \\(H_A\\), that represents the research question, i.e. what we are testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value.\nDefinition: Probability of observed or more extreme outcome given that the null hypothesis is true.\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "W10.html#setting-the-hypotheses",
    "href": "W10.html#setting-the-hypotheses",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Setting the hypotheses",
    "text": "Setting the hypotheses\nWhich of the following is the correct set of hypotheses for the claim that the consultant has lower complication rates?\n\n\\(H_0: p = 0.10\\); \\(H_A: p \\ne 0.10\\)\n\\(H_0: p = 0.10\\); \\(H_A: p > 0.10\\)\n\\(H_0: p = 0.10\\); \\(H_A: p < 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} \\ne 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} > 0.10\\)\n\\(H_0: \\hat{p} = 0.10\\); \\(H_A: \\hat{p} < 0.10\\)\n\n\nCorrect is c. Hypotheses are be about the true rate of complication \\(p\\) not the observed ones \\(\\hat{p}\\)"
  },
  {
    "objectID": "W10.html#simulating-the-null-distribution",
    "href": "W10.html#simulating-the-null-distribution",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Simulating the null distribution",
    "text": "Simulating the null distribution\nSince \\(H_0: p = 0.10\\), we need to simulate a null distribution where the probability of success (complication) for each trial (patient) is 0.10.\nHow should we simulate the null distribution for this study using a bag of chips?\n\nHow many chips? For example 10 which makes 10% choices possible\nHow many colors? 2\nWhat should colors represent? “complication”, “no complication”\nHow many draws? 62 as the data\nWith replacement or without replacement? With replacement\n\nWhen sampling from the null distribution, what would be the expected proportion of “complications”? 0.1"
  },
  {
    "objectID": "W10.html#what-do-we-expect",
    "href": "W10.html#what-do-we-expect",
    "title": "W#10: Cross validation",
    "section": "What do we expect?",
    "text": "What do we expect?\n.question[ When sampling from the null distribution, what is the expected proportion of success (complications)?]"
  },
  {
    "objectID": "W10.html#simulation",
    "href": "W10.html#simulation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Simulation!",
    "text": "Simulation!\n\nset.seed(1234)\noutcomes <- c(\"complication\", \"no complication\")\nsim1 <- sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\nsim1\n\n [1] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [5] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n [9] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[13] \"no complication\" \"complication\"    \"no complication\" \"no complication\"\n[17] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[21] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[25] \"no complication\" \"no complication\" \"no complication\" \"complication\"   \n[29] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[33] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[37] \"no complication\" \"no complication\" \"complication\"    \"no complication\"\n[41] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[45] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[49] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[53] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[57] \"no complication\" \"no complication\" \"no complication\" \"no complication\"\n[61] \"no complication\" \"no complication\"\n\nsum(sim1 == \"complication\")/62\n\n[1] 0.0483871\n\n\nOh OK, this was is pretty close to the consultant’s rate. But maybe it was a rare event?"
  },
  {
    "objectID": "W10.html#more-simulation",
    "href": "W10.html#more-simulation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "More simulation!",
    "text": "More simulation!\n\none_sim <- function() sample(outcomes, size = 62, prob = c(0.1, 0.9), replace = TRUE)\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1290323\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1290323\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.09677419\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.09677419\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1774194\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1129032\n\nsum(one_sim() == \"complication\")/62\n\n[1] 0.1129032"
  },
  {
    "objectID": "W10.html#automating-with-tidymodels",
    "href": "W10.html#automating-with-tidymodels",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Automating with tidymodels1",
    "text": "Automating with tidymodels1\n\n\n\norgan_donor\n\n# A tibble: 62 × 1\n   outcome        \n   <chr>          \n 1 complication   \n 2 complication   \n 3 complication   \n 4 no complication\n 5 no complication\n 6 no complication\n 7 no complication\n 8 no complication\n 9 no complication\n10 no complication\n# … with 52 more rows\n\n\n\n\nset.seed(10)\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 100, type = \"draw\") |> \n  calculate(stat = \"prop\")\nnull_dist\n\nResponse: outcome (factor)\nNull Hypothesis: point\n# A tibble: 100 × 2\n   replicate   stat\n   <fct>      <dbl>\n 1 1         0.0323\n 2 2         0.0645\n 3 3         0.0968\n 4 4         0.0161\n 5 5         0.161 \n 6 6         0.0968\n 7 7         0.0645\n 8 8         0.129 \n 9 9         0.161 \n10 10        0.0968\n# … with 90 more rows\n\n\n\n\nOf course, you can also do it in your own way without packages."
  },
  {
    "objectID": "W10.html#visualizing-the-null-distribution",
    "href": "W10.html#visualizing-the-null-distribution",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Visualizing the null distribution",
    "text": "Visualizing the null distribution\n\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(title = \"Null distribution\")"
  },
  {
    "objectID": "W10.html#calculating-the-p-value-visually",
    "href": "W10.html#calculating-the-p-value-visually",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Calculating the p-value, visually",
    "text": "Calculating the p-value, visually\nWhat is the p-value: How often was the simulated sample proportion at least as extreme as the observed sample proportion?"
  },
  {
    "objectID": "W10.html#calculating-the-p-value-directly",
    "href": "W10.html#calculating-the-p-value-directly",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Calculating the p-value, directly",
    "text": "Calculating the p-value, directly\n\nnull_dist |>\n summarise(p_value = sum(stat <= 3/62)/n())\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1    0.13\n\n\nThis is the fraction of simulations where complications was equal or below 0.0483871."
  },
  {
    "objectID": "W10.html#significance-level",
    "href": "W10.html#significance-level",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Significance level",
    "text": "Significance level\n\nA significance level \\(\\alpha\\) is a threshold we make up to make our judgment about the plausibility of the null hypothesis being true given the observed data.\nWe often use \\(\\alpha = 0.05 = 5\\%\\) as the cutoff for whether the p-value is low enough that the data are unlikely to have come from the null model.\nIf p-value < \\(\\alpha\\), reject \\(H_0\\) in favor of \\(H_A\\): The data provide convincing evidence for the alternative hypothesis.\nIf p-value > \\(\\alpha\\), fail to reject \\(H_0\\) in favor of \\(H_A\\): The data do not provide convincing evidence for the alternative hypothesis.\n\nWhat is the conclusion of the hypothesis test?\nSince the p-value is greater than the significance level, we fail to reject the null hypothesis. These data do not provide convincing evidence that this consultant incurs a lower complication rate than the 10% overall US complication rate."
  },
  {
    "objectID": "W10.html#conclusion",
    "href": "W10.html#conclusion",
    "title": "W#10: Cross validation",
    "section": "Conclusion",
    "text": "Conclusion\n.question[ What is the conclusion of the hypothesis test?]\n–\nSince the p-value is greater than the significance level, we fail to reject the null hypothesis. These data do not provide convincing evidence that this consultant incurs a lower complication rate than 10% (overall US complication rate)."
  },
  {
    "objectID": "W10.html#lets-get-real",
    "href": "W10.html#lets-get-real",
    "title": "W#10: Cross validation",
    "section": "Let’s get real",
    "text": "Let’s get real\n\n100 simulations is not sufficient\nWe usually simulate around 15,000 times to get an accurate distribution, but we’ll do 1,000 here for efficiency."
  },
  {
    "objectID": "W10.html#run-the-test",
    "href": "W10.html#run-the-test",
    "title": "W#10: Cross validation",
    "section": "Run the test",
    "text": "Run the test\n.small[\n\n\n\n]"
  },
  {
    "objectID": "W10.html#visualize-and-calculate",
    "href": "W10.html#visualize-and-calculate",
    "title": "W#10: Cross validation",
    "section": "Visualize and calculate",
    "text": "Visualize and calculate\n.small[\n\n\n\n]\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#simulations-is-not-sufficient",
    "href": "W10.html#simulations-is-not-sufficient",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "100 simulations is not sufficient",
    "text": "100 simulations is not sufficient\n\nWe simulate 15,000 times to get an accurate distribution.\n\n\nnull_dist <- organ_donor |>\n  specify(response = outcome, success = \"complication\") |>\n  hypothesize(null = \"point\", \n              p = c(\"complication\" = 0.10, \"no complication\" = 0.90)) |> \n  generate(reps = 15000, type = \"simulate\") |> \n  calculate(stat = \"prop\")\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.01) +\n  geom_vline(xintercept = 3/62, color = \"red\")"
  },
  {
    "objectID": "W10.html#xkcd-on-p-values",
    "href": "W10.html#xkcd-on-p-values",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "xkcd on p-values",
    "text": "xkcd on p-values\n\n\n \n\n\n\nSignificance levels are fairly arbitrary. Sometimes they are used (wrongly) as definitive judgments\nThey can even be used to do p-hacking: Searching for “significant” effects in observational data\nIn parts of science it has become a “gamed” performance metric.\nThe p-value syas nothing about effect size!"
  },
  {
    "objectID": "W10.html#our-more-robust-p-value",
    "href": "W10.html#our-more-robust-p-value",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Our more robust p-value",
    "text": "Our more robust p-value\nFor the null distribution with 15,000 simulations\n\nnull_dist |>\n  filter(stat <= 3/62) |>\n  summarise(p_value = n()/nrow(null_dist))\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.125\n\n\nOh OK, our fist p-value was much more borderline in favor of the alternative hypothesis."
  },
  {
    "objectID": "W10.html#model-purpose-predict-eu-attitudes",
    "href": "W10.html#model-purpose-predict-eu-attitudes",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model purpose: Predict EU attitudes",
    "text": "Model purpose: Predict EU attitudes\n\ness <- ess_raw |> filter(essround == 9) |> \n select(cntry, euftf, atchctr, atcherp, imueclt, lrscale) |> \n mutate(euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99), \n        atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),\n        atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),\n        imueclt = imueclt |> na_if(77) |> na_if(88) |> na_if(99),\n        lrscale = lrscale |> na_if(77) |> na_if(88) |> na_if(99))\n\nFor the ESS dataset\n\nwe filter for people from round 9 (2018)\nselect 5 attitude variables and cntry with 29 countries: AT, BE, BG, CH, CY, CZ, DE, DK, EE, ES, FI, FR, GB, HR, HU, IE, IS, IT, LT, LV, ME, NL, NO, PL, PT, RS, SE, SI, SK\nrecode NA’s properly for five variables:\neuftf: European Union: European unification go further (=10) or gone too far (=0)\natchctr: How emotionally attached to [country] (0 to 10)\natcherp: How emotionally attached to Europe (0 to 10)\nimueclt: Country’s cultural life undermined (=0) or enriched (=10) by immigrants\nlrscale: Placement on left (=0) right (=10) scale"
  },
  {
    "objectID": "W10.html#model-1-2-and-3",
    "href": "W10.html#model-1-2-and-3",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model 1, 2, and 3",
    "text": "Model 1, 2, and 3\n\n\n\nCreate an initial split with 80% training data\nCreate a linear model ess_mod\nCreate three recipes\n\ness_rec1 without using the country variable\ness_rec2 with main effects for all 29 countries\ness_rec3 with additional interaction effects for all 29 countries\n\nCreate the workflow\nFit 3 models by adding the 3 recipes\n\n\n\nset.seed(7)\ness_split <- initial_split(ess, prop = 0.80)\ness_train <- training(ess_split)\ness_test <- testing(ess_split)\n\ness_model <- linear_reg() |> set_engine(\"lm\")\ness_rec1 <- ess_train |> \n recipe(euftf ~ .) |> \n step_rm(cntry)\ness_rec2 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry)\ness_rec3 <- ess_train |> \n recipe(euftf ~ .) |> \n step_dummy(cntry) |> \n step_interact(~starts_with(\"cntry\"):c(atchctr,atcherp,imueclt,lrscale))\n\ness_wflow <- workflow() |> \n add_model(ess_model)\n\ness_fit1 <- ess_wflow |> \n add_recipe(ess_rec1) |> \n fit(ess_train)\ness_fit2 <- ess_wflow |> \n add_recipe(ess_rec2) |> \n fit(ess_train)\ness_fit3 <- ess_wflow |> \n add_recipe(ess_rec3) |> \n fit(ess_train)"
  },
  {
    "objectID": "W10.html#model-fits",
    "href": "W10.html#model-fits",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Model fits",
    "text": "Model fits\n\n\n\ntidy(ess_fit1) |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 5 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   2.92  \n2 atchctr      -0.0952\n3 atcherp       0.285 \n4 imueclt       0.287 \n5 lrscale      -0.0401\n\n\n\n\ntidy(ess_fit2)  |> select(term, estimate) |> print(n = 33)\n\n# A tibble: 33 × 2\n   term        estimate\n   <chr>          <dbl>\n 1 (Intercept)  2.10   \n 2 atchctr     -0.101  \n 3 atcherp      0.294  \n 4 imueclt      0.299  \n 5 lrscale     -0.0196 \n 6 cntry_BE     0.747  \n 7 cntry_BG     1.20   \n 8 cntry_CH    -0.0476 \n 9 cntry_CY     1.39   \n10 cntry_CZ     0.184  \n11 cntry_DE     1.32   \n12 cntry_DK     1.01   \n13 cntry_EE     0.532  \n14 cntry_ES     1.14   \n15 cntry_FI     0.00996\n16 cntry_FR     0.743  \n17 cntry_GB    -0.0705 \n18 cntry_HR     0.729  \n19 cntry_HU     0.216  \n20 cntry_IE     0.294  \n21 cntry_IS     0.0371 \n22 cntry_IT     0.593  \n23 cntry_LT     2.15   \n24 cntry_LV     0.426  \n25 cntry_ME     2.41   \n26 cntry_NL     0.600  \n27 cntry_NO    -0.170  \n28 cntry_PL     1.19   \n29 cntry_PT     1.42   \n30 cntry_RS     1.21   \n31 cntry_SE    -0.0761 \n32 cntry_SI     1.53   \n33 cntry_SK     0.178  \n\n\n\n\ntidy(ess_fit3)  |> select(term, estimate) |> print(n = 145)\n\n# A tibble: 145 × 2\n    term               estimate\n    <chr>                 <dbl>\n  1 (Intercept)         2.65   \n  2 atchctr            -0.135  \n  3 atcherp             0.259  \n  4 imueclt             0.412  \n  5 lrscale            -0.141  \n  6 cntry_BE            0.0202 \n  7 cntry_BG           -0.264  \n  8 cntry_CH            1.70   \n  9 cntry_CY            0.242  \n 10 cntry_CZ           -0.914  \n 11 cntry_DE            0.248  \n 12 cntry_DK            0.637  \n 13 cntry_EE           -0.709  \n 14 cntry_ES            1.15   \n 15 cntry_FI            0.143  \n 16 cntry_FR            0.136  \n 17 cntry_GB            0.486  \n 18 cntry_HR            0.177  \n 19 cntry_HU           -0.190  \n 20 cntry_IE            0.186  \n 21 cntry_IS            0.881  \n 22 cntry_IT           -0.582  \n 23 cntry_LT            0.925  \n 24 cntry_LV           -1.60   \n 25 cntry_ME            0.542  \n 26 cntry_NL           -0.804  \n 27 cntry_NO            0.763  \n 28 cntry_PL            2.24   \n 29 cntry_PT            2.18   \n 30 cntry_RS            0.220  \n 31 cntry_SE           -0.507  \n 32 cntry_SI            1.19   \n 33 cntry_SK           -0.604  \n 34 cntry_BE_x_atchctr -0.0249 \n 35 cntry_BE_x_atcherp  0.128  \n 36 cntry_BE_x_imueclt -0.130  \n 37 cntry_BE_x_lrscale  0.148  \n 38 cntry_BG_x_atchctr  0.220  \n 39 cntry_BG_x_atcherp -0.0614 \n 40 cntry_BG_x_imueclt -0.159  \n 41 cntry_BG_x_lrscale  0.151  \n 42 cntry_CH_x_atchctr -0.0286 \n 43 cntry_CH_x_atcherp  0.0194 \n 44 cntry_CH_x_imueclt -0.197  \n 45 cntry_CH_x_lrscale -0.106  \n 46 cntry_CY_x_atchctr  0.0956 \n 47 cntry_CY_x_atcherp  0.0496 \n 48 cntry_CY_x_imueclt -0.336  \n 49 cntry_CY_x_lrscale  0.300  \n 50 cntry_CZ_x_atchctr -0.0375 \n 51 cntry_CZ_x_atcherp -0.0267 \n 52 cntry_CZ_x_imueclt  0.0735 \n 53 cntry_CZ_x_lrscale  0.273  \n 54 cntry_DE_x_atchctr  0.0299 \n 55 cntry_DE_x_atcherp  0.159  \n 56 cntry_DE_x_imueclt -0.127  \n 57 cntry_DE_x_lrscale  0.0836 \n 58 cntry_DK_x_atchctr -0.0884 \n 59 cntry_DK_x_atcherp  0.0752 \n 60 cntry_DK_x_imueclt  0.0109 \n 61 cntry_DK_x_lrscale  0.0970 \n 62 cntry_EE_x_atchctr  0.0895 \n 63 cntry_EE_x_atcherp  0.0464 \n 64 cntry_EE_x_imueclt -0.137  \n 65 cntry_EE_x_lrscale  0.180  \n 66 cntry_ES_x_atchctr  0.0735 \n 67 cntry_ES_x_atcherp -0.0706 \n 68 cntry_ES_x_imueclt -0.0704 \n 69 cntry_ES_x_lrscale  0.0210 \n 70 cntry_FI_x_atchctr -0.106  \n 71 cntry_FI_x_atcherp  0.0602 \n 72 cntry_FI_x_imueclt -0.104  \n 73 cntry_FI_x_lrscale  0.176  \n 74 cntry_FR_x_atchctr -0.0254 \n 75 cntry_FR_x_atcherp  0.185  \n 76 cntry_FR_x_imueclt -0.195  \n 77 cntry_FR_x_lrscale  0.139  \n 78 cntry_GB_x_atchctr -0.0968 \n 79 cntry_GB_x_atcherp  0.161  \n 80 cntry_GB_x_imueclt -0.196  \n 81 cntry_GB_x_lrscale  0.0630 \n 82 cntry_HR_x_atchctr  0.145  \n 83 cntry_HR_x_atcherp -0.0138 \n 84 cntry_HR_x_imueclt -0.213  \n 85 cntry_HR_x_lrscale  0.120  \n 86 cntry_HU_x_atchctr  0.0531 \n 87 cntry_HU_x_atcherp -0.0656 \n 88 cntry_HU_x_imueclt -0.0588 \n 89 cntry_HU_x_lrscale  0.160  \n 90 cntry_IE_x_atchctr  0.0630 \n 91 cntry_IE_x_atcherp -0.0230 \n 92 cntry_IE_x_imueclt -0.138  \n 93 cntry_IE_x_lrscale  0.0874 \n 94 cntry_IS_x_atchctr -0.186  \n 95 cntry_IS_x_atcherp  0.111  \n 96 cntry_IS_x_imueclt -0.0756 \n 97 cntry_IS_x_lrscale  0.0596 \n 98 cntry_IT_x_atchctr  0.0381 \n 99 cntry_IT_x_atcherp -0.0405 \n100 cntry_IT_x_imueclt  0.114  \n101 cntry_IT_x_lrscale  0.111  \n102 cntry_LT_x_atchctr  0.145  \n103 cntry_LT_x_atcherp -0.113  \n104 cntry_LT_x_imueclt -0.0917 \n105 cntry_LT_x_lrscale  0.219  \n106 cntry_LV_x_atchctr  0.130  \n107 cntry_LV_x_atcherp -0.0431 \n108 cntry_LV_x_imueclt -0.161  \n109 cntry_LV_x_lrscale  0.365  \n110 cntry_ME_x_atchctr  0.198  \n111 cntry_ME_x_atcherp  0.136  \n112 cntry_ME_x_imueclt -0.179  \n113 cntry_ME_x_lrscale  0.0701 \n114 cntry_NL_x_atchctr  0.0416 \n115 cntry_NL_x_atcherp  0.138  \n116 cntry_NL_x_imueclt -0.130  \n117 cntry_NL_x_lrscale  0.182  \n118 cntry_NO_x_atchctr -0.0643 \n119 cntry_NO_x_atcherp -0.0283 \n120 cntry_NO_x_imueclt -0.202  \n121 cntry_NO_x_lrscale  0.187  \n122 cntry_PL_x_atchctr -0.00472\n123 cntry_PL_x_atcherp  0.0737 \n124 cntry_PL_x_imueclt -0.231  \n125 cntry_PL_x_lrscale -0.0290 \n126 cntry_PT_x_atchctr  0.0377 \n127 cntry_PT_x_atcherp -0.114  \n128 cntry_PT_x_imueclt -0.178  \n129 cntry_PT_x_lrscale  0.126  \n130 cntry_RS_x_atchctr  0.139  \n131 cntry_RS_x_atcherp  0.0996 \n132 cntry_RS_x_imueclt -0.285  \n133 cntry_RS_x_lrscale  0.167  \n134 cntry_SE_x_atchctr  0.0493 \n135 cntry_SE_x_atcherp -0.00505\n136 cntry_SE_x_imueclt -0.138  \n137 cntry_SE_x_lrscale  0.160  \n138 cntry_SI_x_atchctr  0.199  \n139 cntry_SI_x_atcherp -0.165  \n140 cntry_SI_x_imueclt -0.256  \n141 cntry_SI_x_lrscale  0.204  \n142 cntry_SK_x_atchctr  0.0955 \n143 cntry_SK_x_atcherp -0.0549 \n144 cntry_SK_x_imueclt -0.0424 \n145 cntry_SK_x_lrscale  0.137  \n\n\n\n\nNote: We omit std.error, p-values and so on in the display here because they are usually small in this large dataset, and we will not look at them now."
  },
  {
    "objectID": "W10.html#some-probability-topics-for-data-science",
    "href": "W10.html#some-probability-topics-for-data-science",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing",
    "section": "Some Probability Topics for Data Science",
    "text": "Some Probability Topics for Data Science\n\nThe concept of probability form the mathematical perspective.\nWhat is the difference between probability theory and statistics?\nMake precise what are probabilistic events, a probability functions and random variables.\nHow do random variables relate to data?\nProbabilistic simulations. For example bootstrapping.\nConditional probabilities and their relation to the confusion matrix.\nContinuous random variables and some theoretical distributions to link the theory with the practical treatment.\nThe central limit theorem.\nWhat is the difference between probability theory and statistics?\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#probability-topics-for-data-science",
    "href": "W10.html#probability-topics-for-data-science",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\nSome concepts and topics\n\n\nThe concept of probability form the mathematical perspective.\nWhat are probabilistic events, probability functions and random variables.\nHow do random variables relate to data?\nProbabilistic simulations. For example bootstrapping.\nConditional probabilities and their relation to the confusion matrix.\nContinuous random variables and some theoretical distributions.\nThe central limit theorem.\nWhat is the difference between probability theory and statistics?"
  },
  {
    "objectID": "W10.html#what-is-probability",
    "href": "W10.html#what-is-probability",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "What is probability",
    "text": "What is probability\n\n\nThe systematic and rigorous treatment of uncertainty.\nWe have a certain intuition of probability visible in sentences like:\n\n“That’s not very probable.”\n“That is likely.”\n“I don’t have a prior for that.”\n\nWe can call it a model for uncertainty: A simplified but formalized way to think about uncertain events.\nThe model of probability is one of the most successful mathematical models. It is used in many domains."
  },
  {
    "objectID": "W10.html#two-different-flavors",
    "href": "W10.html#two-different-flavors",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Two different flavors",
    "text": "Two different flavors\n\nModel for uncertainty as subjective or objective probability of an uncertain event.\nThey are also called Bayesian vs. Frequentist interpretation of probability. * They differ in the way of reasoning, the interpretation what is random, and in terminology."
  },
  {
    "objectID": "W10.html#objective-interpretation-of-probability",
    "href": "W10.html#objective-interpretation-of-probability",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Objective interpretation of probability",
    "text": "Objective interpretation of probability\nThe objective interpretation of probability is relative frequency in the limit of indefinite sampling. It is the long run behavior of non-deterministic outcomes.\n\nIf we flip the same coin several times the relative frequency of the number of HEADS converges to the probability of HEADS.\nIf we draw people at random the relative frequency of people with heights between 1.70m and 1.75m converges to the probability that a person is in this range.\nIn this frequentist philosophy the parameters of the population we sample from is fixed and the data is a random selection."
  },
  {
    "objectID": "W10.html#subjective-interpretation-of-probability",
    "href": "W10.html#subjective-interpretation-of-probability",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Subjective interpretation of probability",
    "text": "Subjective interpretation of probability\nThe subjective interpretation of probability is the belief a person has about the likelihood that an event occurs. This can be formalized by the condition under which a person would make a bet.\n\nIf say we flip a coin, we can offer a person a bet for HEADS, e.g. you gain 1€ when the outcome is heads and you lose 2€ when it is TAILS. The person would be indifferent between accepting and rejecting the bet if their subjective belief is that HEADS will come two times more likely than TAILS (odds 2:1 or probability 2/3). The subjective probability is the probability where the person is indifferent.\nWe can make up similar bets for the heights of randomly drawn drawn people.\nIn Bayesian philosophy the data we know is fixed but the parameters of the population are random and associated with probabilities.\n\nThe objective and subjective views are not mutually exclusive and it is not important to take a side.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W10.html#objective-interpretation",
    "href": "W10.html#objective-interpretation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Objective interpretation",
    "text": "Objective interpretation\nProbability is relative frequency in the limit of indefinite sampling. It is the long run behavior of non-deterministic outcomes.\n\nIf we flip the same coin several times the relative frequency of the number of HEADS converges to the probability of HEADS.\nIf we draw people at random the relative frequency of people with heights between 1.70m and 1.75m converges to the probability that a person is in this range.\nIn this frequentist philosophy the parameters of the population we sample from is fixed and the data is a random selection."
  },
  {
    "objectID": "W10.html#subjective-interpretation",
    "href": "W10.html#subjective-interpretation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Subjective interpretation",
    "text": "Subjective interpretation\nProbability is a belief a person has about the likelihood that an event occurs. This can be formalized by the condition under which a person would make a bet.\n\nIf say we flip a coin, we can offer a person a bet for HEADS, e.g. you gain 1€ when the outcome is heads and you lose 2€ when it is TAILS. The person would be indifferent between accepting and rejecting the bet if their subjective belief is that HEADS will come two times more likely than TAILS (odds 2:1 or probability 2/3). The subjective probability is the probability where the person is indifferent.\nWe can make up similar bets for the heights of randomly drawn people.\nIn Bayesian philosophy the data we know is fixed but the parameters of the population are random and associated with probabilities.\n\nThe objective and subjective views are not mutually exclusive and it is not important to take a side.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "index.html#core-resources",
    "href": "index.html#core-resources",
    "title": "Data Science Concepts / Tools",
    "section": "2.1 Core resources",
    "text": "2.1 Core resources\nA large part of the course will build on or be inspired by material in Data Science in the Box https://datasciencebox.org/ by Mine Çetinkaya-Rundel and the data science education community around. You can also use the website for accompanying self-study on selected topics.\nA core resource for self-learning data science with R is R for Data Science https://r4ds.had.co.nz/ by Hadley Wickham and the R data science community around. This is a valuable resource for self-studying particular topics using tidyverse tools. Just out: Tidy Modeling with R https://www.tmwr.org/ by Max Kuhn and Julia Silge on using tidymodels.\nA core resource for data science basics with python is the Python Data Science Handbook https://jakevdp.github.io/PythonDataScienceHandbook/ by Jake VanderPlas. Also there is a good video playlist https://www.youtube.com/playlist?list=PLWKjhJtqVAbkmRvnFmOd4KhDdlK1oIq23 and a 4-hour “full-course” video https://youtu.be/rfscVS0vtbw."
  },
  {
    "objectID": "index.html#mathematics",
    "href": "index.html#mathematics",
    "title": "Data Science Concepts / Tools",
    "section": "2.2 Mathematics",
    "text": "2.2 Mathematics\nSome helpful YouTube videos for those recaping maths:\nHow to read math https://www.youtube.com/watch?v=Kp2bYWRQylk"
  },
  {
    "objectID": "index.html#week-8-oct-20-homework-topics-typical-data-issues-more-linear-models-and-interpretation",
    "href": "index.html#week-8-oct-20-homework-topics-typical-data-issues-more-linear-models-and-interpretation",
    "title": "Data Science Concepts / Tools",
    "section": "Week 8, Oct 20: Homework topics: Typical Data Issues, More Linear Models and Interpretation",
    "text": "Week 8, Oct 20: Homework topics: Typical Data Issues, More Linear Models and Interpretation\nTopics:\n\nErrors, Differences, Missings in Data\nMore Linear Models\n\nMore predictors\nMain effects and interaction effects\n\nTo be finished\n\nSlides Week 8\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 should come. Due Nov 6."
  },
  {
    "objectID": "index.html#week-10-nov-3-performance-metrics-cross-validation-hypothesis-testing-math-probability",
    "href": "index.html#week-10-nov-3-performance-metrics-cross-validation-hypothesis-testing-math-probability",
    "title": "Data Science Concepts / Tools",
    "section": "Week 10, Nov 3: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "text": "Week 10, Nov 3: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability\nSlides Week 10\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 due in 3 days\nHomework 05 should come. Due Nov 20."
  },
  {
    "objectID": "W10.html#comparison-of-metrics-1",
    "href": "W10.html#comparison-of-metrics-1",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\n\n\nFlipper length, body mass\n\npeng_test_pred1 |> \n accuracy(truth = sex, estimate = .pred_class_0.5)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.774\n\npeng_test_pred1 |> \n accuracy(truth = sex, estimate = .pred_class_0.55)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.726\n\npeng_test_pred1 |> \n accuracy(truth = sex, estimate = .pred_class_0.45)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.821\n\n\n\nBill length and depth\n\npeng_test_pred2 |> \n accuracy(truth = sex, estimate = .pred_class_0.5)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.738\n\npeng_test_pred2 |> \n accuracy(truth = sex, estimate = .pred_class_0.55)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.774\n\npeng_test_pred2 |> \n accuracy(truth = sex, estimate = .pred_class_0.45)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary          0.75"
  },
  {
    "objectID": "W10.html#comparison-of-metrics-2",
    "href": "W10.html#comparison-of-metrics-2",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Comparison of metrics",
    "text": "Comparison of metrics\nFlipper length, body mass\n\npeng_test_pred1 |> select(1:6)\n\n# A tibble: 84 × 6\n   .pred_female .pred_male .pred_class_0.5 .pred_class_0.45 .pred_class_…¹ sex  \n          <dbl>      <dbl> <fct>           <fct>            <fct>          <fct>\n 1       0.756       0.244 female          female           female         fema…\n 2       0.373       0.627 male            male             male           fema…\n 3       0.286       0.714 male            male             male           male \n 4       0.352       0.648 male            male             male           fema…\n 5       0.365       0.635 male            male             male           male \n 6       0.451       0.549 male            female           male           fema…\n 7       0.0491      0.951 male            male             male           male \n 8       0.700       0.300 female          female           female         fema…\n 9       0.532       0.468 female          female           male           male \n10       0.824       0.176 female          female           female         fema…\n# … with 74 more rows, and abbreviated variable name ¹​.pred_class_0.55\n\n\nBill length and depth\n\npeng_test_pred2 |> select(1:6)\n\n# A tibble: 84 × 6\n   .pred_female .pred_male .pred_class_0.5 .pred_class_0.45 .pred_class_…¹ sex  \n          <dbl>      <dbl> <fct>           <fct>            <fct>          <fct>\n 1        0.487      0.513 male            female           male           fema…\n 2        0.642      0.358 female          female           female         fema…\n 3        0.278      0.722 male            male             male           male \n 4        0.614      0.386 female          female           female         fema…\n 5        0.543      0.457 female          female           male           male \n 6        0.556      0.444 female          female           female         fema…\n 7        0.339      0.661 male            male             male           male \n 8        0.736      0.264 female          female           female         fema…\n 9        0.410      0.590 male            male             male           male \n10        0.855      0.145 female          female           female         fema…\n# … with 74 more rows, and abbreviated variable name ¹​.pred_class_0.55"
  },
  {
    "objectID": "W10.html#overview-of-predictions",
    "href": "W10.html#overview-of-predictions",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Overview of predictions",
    "text": "Overview of predictions\nBill length and depth\n\npeng_test_pred2 |> select(1:6)\n\n# A tibble: 84 × 6\n   .pred_female .pred_male .pred_class_0.5 .pred_class_0.45 .pred_class_…¹ sex  \n          <dbl>      <dbl> <fct>           <fct>            <fct>          <fct>\n 1        0.487      0.513 male            female           male           fema…\n 2        0.642      0.358 female          female           female         fema…\n 3        0.278      0.722 male            male             male           male \n 4        0.614      0.386 female          female           female         fema…\n 5        0.543      0.457 female          female           male           male \n 6        0.556      0.444 female          female           female         fema…\n 7        0.339      0.661 male            male             male           male \n 8        0.736      0.264 female          female           female         fema…\n 9        0.410      0.590 male            male             male           male \n10        0.855      0.145 female          female           female         fema…\n# … with 74 more rows, and abbreviated variable name ¹​.pred_class_0.55"
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html",
    "href": "hw-instructions/hw-05-instr.html",
    "title": "Homework 05",
    "section": "",
    "text": "Important\n\n\n\nHomework 05 is due Sunday, Nov 20. When you get stuck, you are encouraged to push intermediate steps, and contact us.\nIn this homework you are to continue to do data analysis with data from the European Social Survey (ESS) in python, and with the nycflight13 in R."
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html#european-social-survey",
    "href": "hw-instructions/hw-05-instr.html#european-social-survey",
    "title": "Homework 05",
    "section": "1 European Social Survey",
    "text": "1 European Social Survey\nContinue to work on your repository ess-ind-USERNAME. ESS Data Documentation is here https://ess-search.nsd.no/.\nContinue to work on the Quarto-document “ESS-analysis.qmd” (alternatively “ESS-analysis.ipynb” if you can render html-files from it) from Homework 03 and add new headlines and new code chunks for each new question."
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html#question-8-how-well-can-life-satisfaction-predict-voting-behavior",
    "href": "hw-instructions/hw-05-instr.html#question-8-how-well-can-life-satisfaction-predict-voting-behavior",
    "title": "Homework 05",
    "section": "2 Question 8: How well can life satisfaction predict voting behavior?",
    "text": "2 Question 8: How well can life satisfaction predict voting behavior?\nLike in our last tools session you should repeat the build of a logistic regression model using both statsmodels and sklearn packages. We want to predict voters with their satisfaction with life. We start simple and try to modify and enhance the model afterwards.\n\nClean the data set, so you remove all values above 10.\nRearrange the values from the vote columns. The default data from ESS is 1=voter and 2=non-voter. Modify to: 0= non-voter , 1=voter.\nCreate a model with vote ~ stflife\nUse sklearn, statsmodel.api (mind: Intercept), or the statsmodel.formula.api\nPlot the logistic regression function of the model, either build the function from scratch with numpy or use expit in scipy.special.\n\nDescribe in text what we can see in the function and what does it say about the predictions?\nPlot the crosstab (pd.crosstab(..)) as an barplot as validation."
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html#question-9-how-can-we-improve-the-prediction-of-voters",
    "href": "hw-instructions/hw-05-instr.html#question-9-how-can-we-improve-the-prediction-of-voters",
    "title": "Homework 05",
    "section": "3 Question 9: How can we improve the prediction of voters?",
    "text": "3 Question 9: How can we improve the prediction of voters?\nNow we want to enhance the model with many variables 'polintr','euftf','stflife', 'trstplc', 'imueclt', 'atchctr'. Also we want to use a training data set and a test data set.\nWrite a short sentence why we use a split of training and test data.\n\nLook up the range for polintr to clean the data set.\nCreate a train and a test data set with train_test_split from sklearn.model_selection.\nTrain the model (Train data set) and then test it (Test data set). Use the predict function on the model.\nPrint out the coefficients, accuracy, and the confusion matrix (metrics from sklearn)\n\nDescribe what the model is predicting?\n\n3.1 Question 9.1: How do country variables change the prediction?\nNow we integrate countries in the model. We go back to the model in Question 1 vote ~ stflife but add the country values.\n\nSplit the data in a training and a test set that includes the Country column.\nCreate a simple model with statsmodel.formula.api.\nFit the model and print the confusion matrix\nSave the coefficients for later.\n\n\n\n3.2 Question 9.2: As Question 9.1 but with sklearn\nFor comparison:\n\nCreate dummy variables pd.get_dummies() for the country column (Attention: after cleaning the data)\nUse pd.concat(..,axis=1) to attach the stflife and vote variables back to the country dummies data set.\nSplit the training and test data set.\nCreate a LogisticRegression model from sklearn.linear_model and fit the data to it.\nPrint the confusion matrix\nDescribe how do both models (9.1 and 9.2) perform. What are the differences? If there are differences, where do they come from?\nFinally create 2 horizontal bar plots (matplotlib.pyplot.barh) with the coefficients from Question 9.1 and 9.2 and compare them.\n\n\n\n3.3 Submit the new version of your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html#corona",
    "href": "hw-instructions/hw-05-instr.html#corona",
    "title": "Homework 05",
    "section": "4 Corona",
    "text": "4 Corona\nContinue to work on your repository corona-ind-USERNAME.\nContinue to work on the Quarto-document “Corona_Analysis.qmd” from Homework 03 and add new headlines and new code chunks for each new question.\n\n4.1 Question 5: Do the cumulative cases reported by the WHO for Germany, France, India and the country of your choice coincide with the cumulative sum of new cases?\n\nFilter the who dataset for the four countries. Group by these countries and compute a new variable called Total_cases as the cumulative sum (cumsum) of the new cases for each each country.\nNow, test if this variable coincides with the variable Cumulative_cases which is already present in the dataset. To that end, compute a new variable with the difference of the two time lines and check if the difference is zero in every time step. (You can use count on the the difference variable to count different values.)\nWrite down the answer to the question.\n\n\n\n4.2 Question 6: What can the visualization of the daily change of new cases in Germany tell us about the evolution of the pandemic?\n\n\n\n\n\n\nTip\n\n\n\nDownload the two datasets (who and owid) anew to get the most recent values.\n\n\n\nUse the dataset of the WHO, filter for Germany and all dates after August 15, 2022.\nMake a new variable New_cases_smoothed where you smooth New_cases with a 7-day lagged window.\nCreate another variable called Change which is the difference of in the smoothed new cases with the day before.\nDevelop two ggplots (see below), one for the smoothed new cases and one for the change. Once you have the code for each plot save each plot as a ggplot object (e.g., g1 and g2) and show them in your rendered document with new cases above the change. (Hint: Use the package patchwork and g1 / g2 to show the two plots.)\n\nUse a bar-chart for both plots. Use geom_col.\nFor the plot of the change make the bars for negative numbers filled with blue and the bars for positive numbers filled with red. The way to do it is to add two geom_cols to the same plot. Both have different y-aesthetics but both based on the Change variable. In one the negative values are set to zero in the other the positive values. For example you can create such a variable like ifelse(Change >= 0, Change, 0).\nLet us make these plots look nice. Use + labs() to create meaningful axis labels. Use scale_x_date to make the x-axis-ticks look nice. For example, use date_breaks = ... to specify the main labelled ticks, date_minor_breaks = ... for smaller unlabelled ticks, and use date_labels = ... to make the labels shorter by omitting the year in the dates. You have to look up the specification in the help of scale_x_date.\n\nDescribe what the blue and the red regions in the change plot tell us about the recent evolution of corona cases in Germany.\nFrom the total case, the new cases and the change of new cases. Which timeline is the derivative an the anti-derivative of which?\n\n\n\n4.3 Question 7: Which variable explains more variance of total deaths per million, the human development index (HDI) or the median age?\nUse the owid dataset and filter the rows with data Oct 15, 2022 only and with a valid continent (this should be 223 rows).\nEstimate two linear model with this dataset. Both have total_death_per_million as dependent variable.\n\nUse human_development_index as independent variable and save the model object as mod_hdi.\nUse median_age as independent variable and save the model object as mod_age.\n\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html. Answer the question.\n\n\n4.4 Question 8: How do the two models improve when the continent is added as an interaction effect?\nEstimate two more linear models, mod_hdicont and mod_agecont, by adding continent as an interaction effect to the two models from Question 8 (human_development_index * continent and median_age * continent).\n\nPrint the coefficients of mod_agecont in tidy form and interpret the coefficient of the intercept, the main effects, and the interaction effects.\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html.\n\nAccess, print, and interpret the R-squared of both models (glance(mod)$r.squared) in the rendered html. Answer the question. In which case is the improvement stronger? Why is the improvement different?\n\n\n4.5 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html#european-social-survey-1",
    "href": "hw-instructions/hw-05-instr.html#european-social-survey-1",
    "title": "Homework 05",
    "section": "5 European Social Survey",
    "text": "5 European Social Survey\nContinue to work on your repository ess-ind-USERNAME. You have to look ESS Data Documentation https://ess-search.nsd.no/.\nContinue to work on the Quarto-document “ESS-analysis.qmd” (alternatively “ESS-analysis.ipynb” if you can render html-files from it) from Homework 03 and add new headlines and new code chunks for each new question.\n\n5.1 Question 6: What are the correlations between life satisfaction, trust in the police, religiosity, emotional attachment to Europe, and social activity?\nFilter the ess dataset for essround 9 and select the variables stflife, trstplc, rlgdgr, atcherp, and sclact.\nCompute the correlation matrix and visualize correlation coefficients with color.\nFor calculating the correlation of a dataframe use .corr() which returns the correlation matrix. Visualizing a value with a certain color is called heatmap. Use your created correlation matrix in the seaborn function sns.heatmap(). Look up the function and vary the parameters for better visibility.\nExplain the two highest and two lowest correlations.\n\n\n5.2 Question 7: Which variables can explain the variance of life satisfaction in a linear model how well?\nWith the same dataset compute a linear model where life satisfaction is explained as a linear combination of all other four variables.\nPrint and interpret the coefficients. (Look up and report the variable descriptions and their scales at https://ess-search.nsd.no for this purpose!)\nThere are many python packages that include linear models. For similarity with R we use the (statsmodel package)[https://www.statsmodels.org/stable/index.html].\nimport statsmodel.api as sm\nThe default model within statsmodel does not have an intercept defined. Therefore, add a constant to the predictors with sm.add_constant(). Now apply the sm.OLS() to your data with the predictors and dependent variable inside the function. The results of your regression are summarized by applying and printing the results.summary() function.\nWhich percentage of the variance in life satisfaction can be explained with the model?\n\n\n5.3 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html#your-own-project",
    "href": "hw-instructions/hw-05-instr.html#your-own-project",
    "title": "Homework 05",
    "section": "6 Your own project",
    "text": "6 Your own project\nThe assessment of the Data Science Tools module will be based on a team project report.\n\n6.1 A project report in a nutshell\n\nYou pick a dataset,\ndo some interesting question-driven data analysis with it,\nwrite up well structured and nicely formatted report about the analysis, and\npresent it at the end of the semester.\n\nMore details will come soon.\n\n\n6.2 The tasks for this Homework\n\nTeam formation such that a repository at https://github.com/orgs/JU-F22-MDSSB-MET-01 can be provided.\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n\n\n\n\n\n\nTask 1: Form project teams by Sunday, Oct 30.\n\n\n\n\nTeams should have two members.\nSend your names to Jan Lorenz by email or via Teams.\nTeams of three (or more) or individual projects can be permitted on requests providing a reason.\n\n\n\n\n\n\n\n\n\nTask 2: Start a document and list data source/topic and draft questions\n\n\n\n\nYou need your repository in https://github.com/orgs/JU-F22-MDSSB-MET-01 to submit it. Please form your team early and inform us such that it can be created.\nStart a quarto markdown document in the repository. The text can be brief at this stage.\n\n\n\n\n\n6.3 Where to find data?\nThis up to you. Some entry points are:\n\nGoogle Dataset Search https://datasetsearch.research.google.com/\n\nTidy Tuesday https://github.com/rfordatascience/tidytuesday\nAwesome public datasets https://github.com/awesomedata/awesome-public-datasets\nKaggle datasets https://www.kaggle.com/datasets\nJan and Martin can provide ideas for data about polarization in Europe (based on ESS), a voting advice application for German elections, census data about Bremen’s districts.\n\nIf you have a specific topic in mind which can be approached with data but haven’t found a good dataset yet, you can also provide the topic and the questions which interest you only at this stage."
  },
  {
    "objectID": "hw-instructions/hw-05-instr.html#new-york-city-flights-2013",
    "href": "hw-instructions/hw-05-instr.html#new-york-city-flights-2013",
    "title": "Homework 05",
    "section": "4 New York City Flights 2013",
    "text": "4 New York City Flights 2013\nUse your coding and modelling skills to create a model to predict flight delays with nycflights2013 in R. You find hints in the codebase of the “Data Science Tools with R” course.\nTo that end, continue to work in the repository hw-02-ind-USERNAME on the file “hw-02-R.qmd”.\nFirst make some modifications to the file:\n\nIn the YAML (header of the file) change the title to “Homework 02/05 - New York City Flights 2013”\nAt the end of the file make a new main headline “# Predicting flight delays”. Test if the file renders to html nicely. Write all you analysis below, and structure your report with second order headlines “## YOUR HEADLINE”.\n\n\n4.1 Data Preparation\nCreate a big data set from the data frames in the New York City flights package. Join the data sets so that you get as little missing values as possible. (Use inner_join().)\nRename the variables so that you can distinguish them in the final data set. (Ignore the airport data set though, we don’t need it.)\nCreate a dummy variable for the arrival delay being bigger than 15 minutes. This will be the response in the analysis below.\nCreate factors for weekdays and months.\nConvert all character variables into factors, and drop factors with more than 100 observations for faster computation.\nEliminate variables with more than 10,000 missing values, and eliminate observations with missing values. (Use na.omit().)\n\n\n4.2 Split the dataset\nSplit the dataset into the training set (60% of observations) and a test set (40% of observations).\n\n\n4.3 Exploratory analysis\nIn this stage, make an informed selection of eight predictor variables. Select 8 predictor variables that you think have an impact on flight delays, and formulate expectations about how they may influence flight delays.\nThen, take a random sample of 10,000 observations from the training dataset to conduct exploratory analysis.\nMake plots that show the connection between arr_delay, or the dummy variable you created, and the potential predictors.\nThink about and document whether you want to change one predictor after looking at the plots.\n\n\n4.4 Model development\nTrain your model on the training dataset imitating a simple algorithm called forward selection.\nFit one model with a single predictor for all eight variables selected respectively. Use the code from the slides, and save each of the eight models as an object.\nCompare the models in one integrated table, and use AIC or BIC as the main performance criterion. The lower the value of the criterion, the better the performance of the model.\nSelect the best model with a single predictor, and test the remaining seven variables as the second predictor. Again, choose the model that performs best, and try the remaining six variables as predictor number three. Continue until there are no further decreases in AIC or BIC.\n(AIC and BIC are somewhat similar to adjusted R-squared in linear regression: They point at the performance of the model in describing the dataset, while punishing model complexity to avoid overfitting. BIC has a stricter penalty for complexity than AIC, and this will lead to models with fewer predictors.)\nOnce you found the best-performing model, interpret the coefficients against the background of your previous expectations.\n\n\n4.5 Prediction\nUse the best-performing model to predict delays of 15 minutes or more in the test data set.\nCreate the confusion matrix.\nCalculate accuracy, sensitivity, and specificity; interpret the values. Create the ROC plot and interpret it.\n\n\n4.6 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "index.html#data-visualization",
    "href": "index.html#data-visualization",
    "title": "Data Science Concepts / Tools",
    "section": "2.3 Data Visualization",
    "text": "2.3 Data Visualization\nggplot2: Elegant Graphics for Data Analysis https://ggplot2-book.org is a resource on understanding the logic of ggplot better.\nWebsites on how to decide for what visualization to choose:\nhttps://www.data-to-viz.com/\nhttps://datavizcatalogue.com/search.html\nThe number of freely available learning resources is increasing rapidly.\nEveryone has a different learning style and a different background. Thus, different material may be most helpful.\nThis list shall be extended. Good self-study resources for data science with python, mathematics, statistics, data visualization, etc. shall be added here. Contact me if you find something useful to share. Ideally, with a short description when it may be most helpful."
  },
  {
    "objectID": "W10.html#should-we-prefer-larger-or-lower-rmse",
    "href": "W10.html#should-we-prefer-larger-or-lower-rmse",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Should we prefer larger or lower RMSE?",
    "text": "Should we prefer larger or lower RMSE?\nLower. The lower the error, the better the model’s prediction.\nNotes:\n\nThe common method to fit a linear model is the ordinary least squares (OLS) method\nThat means the fitted parameters should deliver the lowest possible sum of squared errors (SSE) between predicted and observed values.\nMinimizing the sum of squared errors (SSE) is identical to minimizing the mean of squared errors (MSE) because it only adds the factor \\(1/n\\).\nMinimizing the mean of squared errors (MSE) is identical to minimizing the root mean of squared errors (RMSE) because the square root is strictly monotone function.\n\nConclusion: RMSE can be seen as a definition of the OLS optimization goal."
  },
  {
    "objectID": "W10.html#regression-vs.-crowd-estimation",
    "href": "W10.html#regression-vs.-crowd-estimation",
    "title": "W#10: Performance Metrics, Cross validation, Hypothesis testing, Math: Probability",
    "section": "Regression vs. crowd estimation",
    "text": "Regression vs. crowd estimation\nThe linear regression and the crowd estimation problems are similar but not identical!\n\n\n\n\n\n\n\n\nVariable\nLinear Regression Model\nCrowd estimation\n\n\n\n\n\\(y_i\\)\nData point of response variable\nTrue value, uniform for all estimators \\(y_i = y\\)\n\n\n\\(\\hat{y}_i\\)\nPredicted value \\(\\hat{y}_i=b_0+b_1+x_1+\\dots\\)\nEstimate of one estimator\n\n\n\\(\\bar{y}\\)\nMean of response variable\nMean of estimates\n\n\n\nNote that"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Motivation: What is the question you want to answer and why?\nProject plan: Identify the tasks to be done to answer the question\nData retrieval: Use interfaces and resources to collect all data necessary for the project\nData processing: Select and filter data, normalize or standardize values if appropriate, and compute additional variables if useful\nAnalysis: Perform statistical analyses and visualizations that assess the question\nConclusion: Evaluate answers to the question and their reliability\nCritique: Identify limitations and alternative explanations"
  },
  {
    "objectID": "W11.html#data-science-tools-module",
    "href": "W11.html#data-science-tools-module",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Data Science Tools Module",
    "text": "Data Science Tools Module"
  },
  {
    "objectID": "W11.html#a-project-report-in-a-nutshell",
    "href": "W11.html#a-project-report-in-a-nutshell",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "A project report in a nutshell",
    "text": "A project report in a nutshell\n\nYou pick a dataset,\ndo some interesting question-driven data analysis with it,\nwrite up a well structured and nicely formatted report about the analysis, and\npresent it at the end of the semester."
  },
  {
    "objectID": "W11.html#hw-04-task-1-form-project-teams",
    "href": "W11.html#hw-04-task-1-form-project-teams",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "HW 04 Task 1: Form Project Teams",
    "text": "HW 04 Task 1: Form Project Teams\n✔️ Team formation is mostly complete\n8 repositories are created in\nhttps://github.com/orgs/JU-F22-MDSSB-MET-01/.\nYou are to deliver your project reports in these repositories."
  },
  {
    "objectID": "W11.html#from-homework-04",
    "href": "W11.html#from-homework-04",
    "title": "W#11: Data Analysis Projects, Probability",
    "section": "From Homework 04",
    "text": "From Homework 04\n\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n\n\n\n\ness <- ess_raw |> filter(essround == 9) |> \n select(cntry, euftf, atchctr, atcherp, imueclt, lrscale) |> \n mutate(euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99), \n        atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),\n        atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),\n        imueclt = imueclt |> na_if(77) |> na_if(88) |> na_if(99),\n        lrscale = lrscale |> na_if(77) |> na_if(88) |> na_if(99))\n\nlibrary(readxl) galton <- read_excel(“data/galton_data.xlsx”) |> mutate(true_value = 1198)"
  },
  {
    "objectID": "W11.html#probability-topics-for-data-science",
    "href": "W11.html#probability-topics-for-data-science",
    "title": "W#11: Probability",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\nToday concepts and topics\n\nThe concept of probability form the mathematical perspective.\nWhat are probabilistic events, probability functions and random variables.\nHow do random variables relate to data?\n(Binomial distribution)"
  },
  {
    "objectID": "W11.html#what-is-probability",
    "href": "W11.html#what-is-probability",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "What is probability",
    "text": "What is probability\n\n\nThe systematic and rigorous treatment of uncertainty.\nWe have a certain intuition of probability visible in sentences like:\n\n“That’s not very probable.”\n“That is likely.”\n“I don’t have a prior for that.”\n\nWe can call it a model for uncertainty: A simplified but formalized way to think about uncertain events.\nThe model of probability is one of the most successful mathematical models. It is used in many domains."
  },
  {
    "objectID": "W11.html#two-different-flavors",
    "href": "W11.html#two-different-flavors",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Two different flavors",
    "text": "Two different flavors\n\nModel for uncertainty as subjective or objective probability of an uncertain event.\nThey are also called Bayesian vs. Frequentist interpretation of probability. * They differ in the way of reasoning, the interpretation what is random, and in terminology."
  },
  {
    "objectID": "W11.html#objective-interpretation",
    "href": "W11.html#objective-interpretation",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Objective interpretation",
    "text": "Objective interpretation\nProbability is relative frequency in the limit of indefinite sampling. It is the long run behavior of non-deterministic outcomes.\n\nIf we flip the same coin several times the relative frequency of the number of HEADS converges to the probability of HEADS.\nIf we draw people at random the relative frequency of people with heights between 1.70m and 1.75m converges to the probability that a person is in this range.\nIn this frequentist philosophy the parameters of the population we sample from is fixed and the data is a random selection."
  },
  {
    "objectID": "W11.html#subjective-interpretation",
    "href": "W11.html#subjective-interpretation",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Subjective interpretation",
    "text": "Subjective interpretation\nProbability is a belief a person has about the likelihood that an event occurs. This can be formalized by the condition under which a person would make a bet.\n\nIf say we flip a coin, we can offer a person a bet for HEADS, e.g. you gain 1€ when the outcome is heads and you lose 2€ when it is TAILS. The person would be indifferent between accepting and rejecting the bet if their subjective belief is that HEADS will come two times more likely than TAILS (odds 2:1 or probability 2/3). The subjective probability is the probability where the person is indifferent.\nWe can make up similar bets for the heights of randomly drawn people.\nIn Bayesian philosophy the data we know is fixed but the parameters of the population are random and associated with probabilities.\n\nThe objective and subjective views are not mutually exclusive and it is not important to take a side.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W11.html#hw-04-task-2-start-a-document-and-list-data-sourcetopic-and-draft-questions",
    "href": "W11.html#hw-04-task-2-start-a-document-and-list-data-sourcetopic-and-draft-questions",
    "title": "W#11: Data Analysis Projects, Probability",
    "section": "HW 04 Task 2: Start a document and list data source/topic and draft questions",
    "text": "HW 04 Task 2: Start a document and list data source/topic and draft questions\n\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n❌ Only two teams pushed a document to the repository. No quarto draft yet."
  },
  {
    "objectID": "W11.html#ok-lets-do-it-a-bit-more-directive-now",
    "href": "W11.html#ok-lets-do-it-a-bit-more-directive-now",
    "title": "W#11: Data Analysis Projects, Probability",
    "section": "OK, let’s do it a bit more directive now",
    "text": "OK, let’s do it a bit more directive now"
  },
  {
    "objectID": "W11.html#hw-04-task-2",
    "href": "W11.html#hw-04-task-2",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "HW 04 Task 2",
    "text": "HW 04 Task 2\nStart a document and list data source/topic and draft questions\n\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n❌ Only two teams pushed a document to the repository. No quarto draft yet."
  },
  {
    "objectID": "W11.html#ok-lets-do-it-together-now",
    "href": "W11.html#ok-lets-do-it-together-now",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "OK, let’s do it together now",
    "text": "OK, let’s do it together now\nPlan:\n\nWe clone project repositories now\nWe go through a push and pull cycle\nWe create and solve a merge conflict\n\nLearning goal:\n\nFirst experiences with collaborative Data Science work with git"
  },
  {
    "objectID": "W11.html#git-pull",
    "href": "W11.html#git-pull",
    "title": "W#11: Data Analysis Projects, Probability",
    "section": "git pull",
    "text": "git pull"
  },
  {
    "objectID": "W11.html#step-1-git-clone-your-project-teamname-repo-url",
    "href": "W11.html#step-1-git-clone-your-project-teamname-repo-url",
    "title": "W#11: Data Analysis Projects, Probability",
    "section": "Step 1: git clone your project-TEAMname repo url",
    "text": "Step 1: git clone your project-TEAMname repo url\n\nGo to: https://github.com/orgs/JU-F22-MDSSB-MET-01/\nFind your project repository\nCopy the URL"
  },
  {
    "objectID": "W11.html#step-2",
    "href": "W11.html#step-2",
    "title": "W#11: Data Analysis Projects, Probability",
    "section": "Step 2:",
    "text": "Step 2:\n\ncopy a template quarto file there\none team member commits and pushes the file\nthe other pulls the latest commits"
  },
  {
    "objectID": "W11.html#step-1-git-clone-project-teamname",
    "href": "W11.html#step-1-git-clone-project-teamname",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 1: git clone project-TEAMname",
    "text": "Step 1: git clone project-TEAMname\n\nGo to: https://github.com/orgs/JU-F22-MDSSB-MET-01/\nFind your project repository project-TEAMname\nCopy the URL\n\nGo to RStudio\n\nNew Project > Form Version Control > Git > Paste your URL\nThe project is created but empty in most cases"
  },
  {
    "objectID": "W11.html#step-2-copy-the-template-file",
    "href": "W11.html#step-2-copy-the-template-file",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 2: Copy the template file",
    "text": "Step 2: Copy the template file\n\nGo to: https://github.com/JU-F22-MDSSB-MET-01/project-TEMPLATE\nClick on the file report.qmd\nClick on “Raw” to see only the file content itself. Usually this also happens in the browser.\nSelect “Save as” from your browser’s menu and save the file in the just created project folder on your computer: YOURPATH/project-TEAMname/\ncopy a template quarto file there\none team member commits and pushes the file\nthe other pulls the latest commits"
  },
  {
    "objectID": "W11.html#step-3-first-team-member-commits-and-pushes",
    "href": "W11.html#step-3-first-team-member-commits-and-pushes",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 3: First Team member commits and pushes",
    "text": "Step 3: First Team member commits and pushes\n\nWho is the first team member? The first one mentioned in the TEAMname.\nExample: In the project project-PoroJan Poro is the first team member\nAs the first team member in RStudio:\n\nSelect the document report.qmd in the Git pane to perform git add of the file\nDo a git commit in the RStudio interface. Enter “Add report.qmd” as commit message.\nDo git push in the RStudio interface."
  },
  {
    "objectID": "W11.html#step-4-second-and-third-team-member-pulls",
    "href": "W11.html#step-4-second-and-third-team-member-pulls",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 4: Second (and third) Team member pulls",
    "text": "Step 4: Second (and third) Team member pulls\n\nWho is the second team member? The first one mentioned in the TEAMname.\nExample: In the project project-PoroJan Jan is the second team member\nAs the second team member in RStudio:\n\nDo git push in the RStudio interface."
  },
  {
    "objectID": "W11.html#what-does-git-pull-do",
    "href": "W11.html#what-does-git-pull-do",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "What does git pull do?",
    "text": "What does git pull do?\n\nIt first does git fetch which gets the commit from the remote repository (GitHub).\n\nThen it git merges the commit with the latest commit on your local machine.\nWhen we are lucky this works with no problems. (Should be the case with new files.)\n\n\nSource: https://mastodon.social/@allison_horst/109303149552034159"
  },
  {
    "objectID": "W11.html#step-5-now-both-do-independent-changes",
    "href": "W11.html#step-5-now-both-do-independent-changes",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 5: Now both do independent changes",
    "text": "Step 5: Now both do independent changes\nGit can merge changes in the same file when there are no conflicts. Let’s try.\n\nThe second team member:\n\nChange the title in the YAML to something meaningful.\nSave, add the file in the Git pane, commit with message “Title”.\nPush.\n\nThe first team member:\n\nAdd your name in the author section of the YAML, save the file.\nPull. This should create an error. Let’s look at it. Get back to your local file.\nAdd the file in the Git pane, commit with message “Author name”.\nPull. This should result in a successful merge where both changes are there:\n\nYour local change of the name\nThe change of the title of your colleague"
  },
  {
    "objectID": "W11.html#step-5-merge-independent-changes",
    "href": "W11.html#step-5-merge-independent-changes",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 5: Merge independent changes",
    "text": "Step 5: Merge independent changes\nGit can merge changes in the same file when there are no conflicts. Let’s try.\n\nThe second team member:\n\nChange the title in the YAML to something meaningful.\nSave, add the file in the Git pane, commit with message “Title”.\nPush.\n\nThe first team member:\n\nAdd your name in the author section of the YAML, save the file.\nPull. This should create an error. Let’s look at it. Get back to your local file.\nAdd the file in the Git pane, commit with message “Author name”.\nPull. This should result in a successful merge where both changes are there:\n\nYour local change of the name and the title of your colleague\n\nNow, git add, git commit, and git push.\n\n\nThe second team member: git pull (That should work.)"
  },
  {
    "objectID": "W11.html#step-6-a-merge-conflict",
    "href": "W11.html#step-6-a-merge-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 6: A merge conflict",
    "text": "Step 6: A merge conflict\n\nBoth team members:\n\nChange a word in the title (but different ones)\ngit add and git commit on your local machine.\n\nFirst member: git push\nSecond member:\n\ngit pull. That should result in a conflict. Maybe you are first asked to config git properly, with proposals for three options. Use the default one (merge). Copy the line. Close the dialog and enter the command in the Terminal pane (ATTENTION: Not the console, the terminal right beside.)\nThe conflict should show directly in the file with markings >>>>>, one option of text afterwards, a separator =======, the other option, and <<<<<<<.\nYou have to solve this conflict now!\n\nDecide for an option or make a new text and remove the >>>>>,=====,<<<<<<\ngit add, git commit, and git push"
  },
  {
    "objectID": "W11.html#proposal-of-one-team",
    "href": "W11.html#proposal-of-one-team",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Proposal of one team",
    "text": "Proposal of one team\nTitle: Marijuana related Crime\nData: https://www.kaggle.com/datasets/jinbonnie/crime-and-weed\nTopic: Causes and correlations of crimes in the capital of Colorado\nQuestions:\n\nGeographical issues. How much location of the district contribute into crime features (like offense_type and offense_category by neighbourhoods)? Most interestingly we may put it onto the city map and understand does airport nearby help planting weed, do people do it in their flats or rather in countryhouses etc.\nCriminology issues. Relation between MJ and other types of crimes (e. g. are they against property or rather violent?).\nMachine learning problems: classify whether (i) certrain crime is industry or non-industry type and (ii) certain crime is more likely to be a larceny or a burglary.\n\nBONUS Temporal issues of the crimes by type. E. g. do some kinds of crimes tend to be discovered later or to be proved as lasting one with more difficulty? Days of week?\nMore BONUS available in case we’re allowed to pull some data from outside"
  },
  {
    "objectID": "W11.html#project-idea-of-one-team",
    "href": "W11.html#project-idea-of-one-team",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Project idea of one team",
    "text": "Project idea of one team\nTitle: Marijuana related Crime\nData: https://www.kaggle.com/datasets/jinbonnie/crime-and-weed\nTopic: Causes and correlations of crimes in the capital of Colorado\nQuestions:\n\nGeographical issues. Does location of the district contribute to crime features (like offense_type and offense_category by neighbourhoods)? We may put it onto the city map and understand: Does airport nearby help planting weed, do people do it in their flats or rather in countryhouses etc.\nCriminology issues. Relation between MJ and other types of crimes (e.g. against property or rather violent).\nMachine learning problems: classify whether (i) certrain crime is industry or non-industry type and (ii) certain crime is more likely to be a larceny or a burglary.\n\nBONUS: Temporal issues of the crimes by type. E.g. do some kinds of crimes tend to be discovered later or to be proved as lasting one with more difficulty? Days of week?\nMore BONUS available in case we’re allowed to pull some data from outside"
  },
  {
    "objectID": "W11.html#another-teams-project-idea",
    "href": "W11.html#another-teams-project-idea",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Another team’s project idea",
    "text": "Another team’s project idea\nTitle: The climate crisis as seen in temperature rise across the world\nData: https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\nQuestions:\n\nHow have temperatures risen across the world since 1750?\nWhich regions, countries and cities have seen the most extreme changes in temperature? (E.g. Which cities have registered the hottest temperatures, what is the impact of temperature rising in the Arctic region?)\nHow are fossil fuel consumption and CO2 emissions correlated to temperature changes in different regions? (Note: We may add an additional dataset on fossil fuel and CO2 data per country for this)\nHow closely does a predictive model based on historical data match the temperatures in subsequent years (2016 onwards)"
  },
  {
    "objectID": "W11.html#my-project",
    "href": "W11.html#my-project",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "My project",
    "text": "My project\nQuestion: Do summer school vacations (in Germany) spur or mitigate a pandemic like corona?\nIdea: Summer School vacations differ substantially in time between the 16 Germany federal states. Maybe this helps to isolate a vacation effect from other things like general seasonality.\nDiscuss:\nHypothesis?\nWhat data do I need?\nWhich visualizations would help?\nWhat analysis could I try?"
  },
  {
    "objectID": "W11.html#what-are-good-questions",
    "href": "W11.html#what-are-good-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "What are good questions?",
    "text": "What are good questions?\n\nIs the question specific or too broad?\nCan it be answered (or at least approached) with data?\nDoes the question clearly call for one of the different types of data analysis: descriptive, explanatory, inferential, predictive, causal, or mechanistic? (Note: Questions for causal and mechanistic explanations are not the typical choices for a short term data analysis projects. We have not discussed much on these. They should not be central but may be touched.)"
  },
  {
    "objectID": "W11.html#six-types-of-questions",
    "href": "W11.html#six-types-of-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Six types of questions",
    "text": "Six types of questions\n\n\n\nDescriptive: summarize a characteristic of data\nExploratory: analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\nInferential: analyze patterns, trends, or relationships in representative data from a population\nPredictive: make predictions for individuals or groups of individuals\nCausal: whether changing one factor will change another factor, on average, in a population\nMechanistic: explore “how” as opposed to whether\n\n\n\n\n\n\n\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146."
  },
  {
    "objectID": "W11.html#data-analysis-flowchart",
    "href": "W11.html#data-analysis-flowchart",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Data Analysis Flowchart",
    "text": "Data Analysis Flowchart"
  },
  {
    "objectID": "W11.html#other-teams",
    "href": "W11.html#other-teams",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Other teams",
    "text": "Other teams\n\nLet us know your first thought\nWhat were your obstacles?"
  },
  {
    "objectID": "W11.html#different-routes-to-setup-your-project",
    "href": "W11.html#different-routes-to-setup-your-project",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Different routes to setup your project",
    "text": "Different routes to setup your project\n\nRandom datasets from the sources mentioned in the homework.\n\nPick a data source (at random) and investigate deeper: What are the cases? What are the variables?\nStart to think about potential exploratory and predictive questions on this. If you feel some curiosity – take it. If not, repeat. Try with at least three datasets.\n\nTopical data search\n\nSelect a topic you are interested in and search for datasets in the sources\nContinue with investigating deeper and thinking about questions as above\n\nQuestion driven\n\nMaybe you have a fairly precise question (Like vacation and corona.)\nThink about out what data you would need\nFind that data (investigation!). Adjust the question when seeing what is available.\n\nDepart from Homework on Corona or ESS (you may download new variables yourself) and develop your own question"
  },
  {
    "objectID": "W11.html#advice",
    "href": "W11.html#advice",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Advice",
    "text": "Advice\n\nA manageable dataset: at least 50 cases, good is 10-20 variables with a mix of categorical and numeric\nGo through the visualizations, statistics, and models we had in lectures and homework and think if similar things would be interesting.\nThe goal is not an exhaustive data analysis. Do not calculate every statistic and procedure you have learned for every variable, but rather show that you can ask meaningful questions and answer them with results of data analysis and proficient interpretation and presentation of the results.\nDo NOT blindly do all visualization and all statistic on all variables in the data set! You can do that in your data exploration but that should not go into your report as is.\nYou are invited to use other data analysis and visualization methods (from other courses or packages which you self-learn)! We are happy to give advice if we can.\nA single high quality visualization which shows a point clearly will receive a much higher appreciation than a large number of poor quality visualizations without an explanation what they should communicate."
  },
  {
    "objectID": "W11_projects.html#data-science-tools-module",
    "href": "W11_projects.html#data-science-tools-module",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Data Science Tools Module",
    "text": "Data Science Tools Module"
  },
  {
    "objectID": "W11_projects.html#hw-04-task-1-form-project-teams",
    "href": "W11_projects.html#hw-04-task-1-form-project-teams",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "HW 04 Task 1: Form Project Teams",
    "text": "HW 04 Task 1: Form Project Teams\n✔️ Team formation is mostly complete\n8 repositories are created in\nhttps://github.com/orgs/JU-F22-MDSSB-MET-01/.\nYou are to deliver your project reports in your repository."
  },
  {
    "objectID": "W11_projects.html#hw-04-task-2",
    "href": "W11_projects.html#hw-04-task-2",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "HW 04 Task 2:",
    "text": "HW 04 Task 2:\nStart a document and list data source/topic and draft questions\n\nAn initial document in the repository which lists either\n\na link and brief description to a data source you want to build your project on, or\na topic and three potential questions on that topic you like to answer within your project report.\n\nIdeally, you provide both.\nNone, of these binds you. It can be changed.\nIt is possible to build on either ESS or Corona data. If you want to do this, make sure to find new questions not those covered in Homework or Lectures.\n\n❌ Only two teams pushed a document to the repository. No quarto draft yet."
  },
  {
    "objectID": "W11_projects.html#ok-lets-do-it-together-now",
    "href": "W11_projects.html#ok-lets-do-it-together-now",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "OK, let’s do it together now",
    "text": "OK, let’s do it together now\nThere is a template quarto file here: https://github.com/JU-F22-MDSSB-MET-01/project-TEMPLATE/blob/main/report.qmd\nPlan:\n\nWe clone project repositories now and add the template\nWe go through a push and pull cycle\nWe create and solve a merge conflict\n\nLearning goal:\n\nFirst experiences with collaborative data science work with git"
  },
  {
    "objectID": "W11_projects.html#step-1-git-clone-project-teamname",
    "href": "W11_projects.html#step-1-git-clone-project-teamname",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 1: git clone project-TEAMname",
    "text": "Step 1: git clone project-TEAMname\n\nGo to: https://github.com/orgs/JU-F22-MDSSB-MET-01/\nFind your project repository project-TEAMname\nCopy the URL\n\nGo to RStudio\n\nNew Project > Form Version Control > Git > Paste your URL\nThe project is created, but it is empty"
  },
  {
    "objectID": "W11_projects.html#step-2-copy-the-template-file",
    "href": "W11_projects.html#step-2-copy-the-template-file",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 2: Copy the template file",
    "text": "Step 2: Copy the template file\nOnly the first team member needs to do the following:\n\nWho is the first team member? The first one mentioned in the TEAMname.\nExample: In the project project-PoroJan Poro is the first team member\n\nThe first team member copies the template file to project folder locally.\n\nGo to: https://github.com/JU-F22-MDSSB-MET-01/project-TEMPLATE\nClick on the file report.qmd\nClick on “Raw” to see the file content only. Usually, this happens in the browser.\nSelect “Save as” from your browser’s menu and save the file in the project folder on your computer: YOURPATH/project-TEAMname/\ncopy a template quarto file there\none team member commits and pushes the file\nthe other pulls the latest commits"
  },
  {
    "objectID": "W11_projects.html#step-3-first-team-member-commits-and-pushes",
    "href": "W11_projects.html#step-3-first-team-member-commits-and-pushes",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 3: First Team member commits and pushes",
    "text": "Step 3: First Team member commits and pushes\nNow the first team member propagates the template file to the others.\n\nAs the first team member in RStudio:\n\nSelect the document report.qmd in the Git pane to perform git add of the file\nDo a git commit in the RStudio interface. Enter “Add report.qmd” as commit message.\nDo git push in the RStudio interface."
  },
  {
    "objectID": "W11_projects.html#step-4-second-and-third-team-member-pulls",
    "href": "W11_projects.html#step-4-second-and-third-team-member-pulls",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 4: Second (and third) team member pulls",
    "text": "Step 4: Second (and third) team member pulls\n\nWho is the second team member? The first one mentioned in the TEAMname.\nExample: In the project project-PoroJan Jan is the second team member\nAs the second team member in RStudio:\n\nDo git pull in the RStudio interface."
  },
  {
    "objectID": "W11_projects.html#what-does-git-pull-do",
    "href": "W11_projects.html#what-does-git-pull-do",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "What does git pull do?",
    "text": "What does git pull do?\n\nIt first does git fetch which gets the commit from the remote repository (GitHub) to the local machine.\n\nThen it git merges the commit with the latest commit on your local machine.\nWhen we are lucky this works with no problems. (Should be the case with new files.)\n\n\nSource: https://mastodon.social/@allison_horst/109303149552034159"
  },
  {
    "objectID": "W11_projects.html#step-5-merge-independent-changes",
    "href": "W11_projects.html#step-5-merge-independent-changes",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 5: Merge independent changes",
    "text": "Step 5: Merge independent changes\nGit can merge changes in the same file when there are no conflicts. Let’s try.\n\nThe second team member:\n\nChange the title in the YAML to something meaningful.\nSave, add the file in the Git pane, commit with message “Title”, push.\n\nThe first team member:\n\nAdd your name in the author section of the YAML, save the file, add the file in the Git pane and make a commit.\nTry to push. You should receive an error. Read it carefully, often it tells you what to do. Here: Do git pull first. You cannot push because remotely there is a newer commit (the one your colleague just made).\nPull.\nThis should result in message about a successfull auto-merge. Check that both changes are there. If you receive several hints instead, first read the next slide!"
  },
  {
    "objectID": "W11_projects.html#step-6-a-merge-conflict",
    "href": "W11_projects.html#step-6-a-merge-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 6: A merge conflict",
    "text": "Step 6: A merge conflict\n\nBoth team members:\n\nChange a word in the title (but different ones)\ngit add and git commit on your local machine.\n\nFirst member: git push\nSecond member:\n\ngit pull. That should result in a conflict. Maybe you are first asked to config git properly, with proposals for three options. Use the default one (merge). Copy the line. Close the dialog and enter the command in the Terminal pane (ATTENTION: Not the console, the terminal right beside.)\nThe conflict should show directly in the file with markings >>>>>, one option of text afterwards, a separator =======, the other option, and <<<<<<<.\nYou have to solve this conflict now!\n\nDecide for an option or make a new text and remove the >>>>>,=====,<<<<<<\ngit add, git commit, and git push"
  },
  {
    "objectID": "W11_projects.html#a-project-report-in-a-nutshell",
    "href": "W11_projects.html#a-project-report-in-a-nutshell",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "A project report in a nutshell",
    "text": "A project report in a nutshell\n\nYou pick a dataset,\ndo some interesting question-driven data analysis with it,\nwrite up a well structured and nicely formatted report about the analysis, and\npresent it at the end of the semester."
  },
  {
    "objectID": "W11_projects.html#what-are-good-questions",
    "href": "W11_projects.html#what-are-good-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "What are good questions?",
    "text": "What are good questions?\n\nIs the question specific or too broad?\nCan it be answered (or at least approached) with data?\nDoes the question clearly call for one of the different types of data analysis: descriptive, explanatory, inferential, predictive, causal, or mechanistic? (Note: Questions for causal and mechanistic explanations are not the typical choices for a short term data analysis projects. We have not discussed much on these. They should not be central but may be touched.)"
  },
  {
    "objectID": "W11_projects.html#six-types-of-questions",
    "href": "W11_projects.html#six-types-of-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Six types of questions",
    "text": "Six types of questions\n\n\n\nDescriptive: summarize a characteristic of data\nExploratory: analyze to see if there are patterns, trends, or relationships between variables (hypothesis generating)\nInferential: analyze patterns, trends, or relationships in representative data from a population\nPredictive: make predictions for individuals or groups of individuals\nCausal: whether changing one factor will change another factor, on average, in a population\nMechanistic: explore “how” as opposed to whether\n\n\n\n\n\n\n\nLeek, Jeffery T., and Roger D. Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15. https://doi.org/10.1126/science.aaa6146."
  },
  {
    "objectID": "W11_projects.html#project-idea-of-one-team",
    "href": "W11_projects.html#project-idea-of-one-team",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Project idea of one team",
    "text": "Project idea of one team\nTitle: Marijuana related Crime\nData: https://www.kaggle.com/datasets/jinbonnie/crime-and-weed\nTopic: Causes and correlations of crimes in the capital of Colorado\n\nGeographical issues. Does location of the district contribute to crime features (like offense_type and offense_category by neighbourhoods)? We may put it onto the city map and understand: Does airport nearby help planting weed, do people do it in their flats or rather in countryhouses etc.\nCriminology issues. Relation between MJ and other types of crimes (e.g. against property or rather violent).\nMachine learning problems: classify whether (i) certrain crime is industry or non-industry type and (ii) certain crime is more likely to be a larceny or a burglary.\n\nBONUS: Temporal issues of the crimes by type. E.g. do some kinds of crimes tend to be discovered later or to be proved as lasting one with more difficulty? Days of week?"
  },
  {
    "objectID": "W11_projects.html#another-teams-project-idea",
    "href": "W11_projects.html#another-teams-project-idea",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Another team’s project idea",
    "text": "Another team’s project idea\nTitle: The climate crisis as seen in temperature rise across the world\nData: https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\nQuestions:\n\nHow have temperatures risen across the world since 1750?\nWhich regions, countries and cities have seen the most extreme changes in temperature? (E.g. Which cities have registered the hottest temperatures, what is the impact of temperature rising in the Arctic region?)\nHow are fossil fuel consumption and CO2 emissions correlated to temperature changes in different regions? (Note: We may add an additional dataset on fossil fuel and CO2 data per country for this)\nHow closely does a predictive model based on historical data match the temperatures in subsequent years (2016 onwards)"
  },
  {
    "objectID": "W11_projects.html#my-project",
    "href": "W11_projects.html#my-project",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "My project",
    "text": "My project\nQuestion: Do summer school vacations (in Germany) spur or mitigate a pandemic like corona?\nIdea: Summer School vacations differ substantially in time between the 16 Germany federal states. Maybe this helps to isolate a vacation effect from other things like general seasonality.\nDiscuss:\nHypothesis?\nWhat data do I need?\nWhich visualizations would help?\nWhat analysis could I try?"
  },
  {
    "objectID": "W11_projects.html#other-teams",
    "href": "W11_projects.html#other-teams",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Other teams",
    "text": "Other teams\n\nLet us know your first thought\nWhat were your obstacles?"
  },
  {
    "objectID": "W11_projects.html#different-routes-to-setup-your-project",
    "href": "W11_projects.html#different-routes-to-setup-your-project",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Different routes to setup your project",
    "text": "Different routes to setup your project\n\nRandom datasets from the sources mentioned in the homework.\n\n\nPick a data source (at random) and investigate deeper: What are the cases? What are the variables?\nStart to think about potential exploratory and predictive questions on this. If you feel some curiosity – take it. If not, repeat. Try with at least three datasets.\n\n\nTopical data search\n\n\nSelect a topic you are interested in and search for datasets in the sources\nContinue with investigating deeper and thinking about questions as above"
  },
  {
    "objectID": "W11_projects.html#advice",
    "href": "W11_projects.html#advice",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Advice",
    "text": "Advice\n\nA manageable dataset: at least 50 cases, good is 10-20 variables with a mix of categorical and numeric\nGo through the visualizations, statistics, and models we had in lectures and homework and think if similar things would be interesting.\nThe goal is not an exhaustive data analysis. Do not calculate every statistic and procedure you have learned for every variable, but rather show that you can ask meaningful questions and answer them with results of data analysis and proficient interpretation and presentation of the results.\nDo NOT blindly do all visualization and all statistic on all variables in the data set! You do that in your data exploration but that should not go into your report as is.\nA single high quality visualization which shows a point clearly will receive a much higher appreciation than a large number of poor quality visualizations without an explanation what they should communicate."
  },
  {
    "objectID": "W11_projects.html#probability-topics-for-data-science",
    "href": "W11_projects.html#probability-topics-for-data-science",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\nSome concepts and topics\n\n\nThe concept of probability form the mathematical perspective.\nWhat are probabilistic events, probability functions and random variables.\nHow do random variables relate to data?\nProbabilistic simulations. For example bootstrapping.\nConditional probabilities and their relation to the confusion matrix.\nContinuous random variables and some theoretical distributions.\nThe central limit theorem.\nWhat is the difference between probability theory and statistics?\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W11_projects.html#potential-questions",
    "href": "W11_projects.html#potential-questions",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Potential questions",
    "text": "Potential questions\n\nDo we need to stick to methods and visualizations treated in lectures?\n\nNo, you are invited to use other data analysis and visualization methods (from other courses or packages which you self-learn)! We are happy to give advice if we can.\n\nAre we only allowed to use one dataset?\n\nNo, you can also merge data from different sources. This is a more challenging project because the data wrangling work would be a bit more. If your question calls for it we encourage you to use another data source. The additional effort will be recognized.\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W11_projects.html#different-routes-to-setup-your-project-1",
    "href": "W11_projects.html#different-routes-to-setup-your-project-1",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Different routes to setup your project",
    "text": "Different routes to setup your project\n\nQuestion driven\n\n\nMaybe you have a fairly precise question (Like vacation and corona.)\nThink about out what data you would need\nFind that data (investigation!). Adjust the question when seeing what is available.\n\n\nDepart from Homework on Corona or ESS (you may download new variables yourself) and develop your own question"
  },
  {
    "objectID": "W11_projects.html#step-6-create-a-merge-conflict",
    "href": "W11_projects.html#step-6-create-a-merge-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 6: Create a merge conflict",
    "text": "Step 6: Create a merge conflict\n\nBoth team members:\n\nChange a word in the title (but different ones)\ngit add and git commit on your local machine.\n\nFirst member: git push\nSecond member:\n\ngit pull. That should result in a conflict. Maybe you are first asked to config git properly, with proposals for three options. Use the default one (merge). Copy the line. Close the dialog and enter the command in the Terminal pane (ATTENTION: Not the console, the terminal right beside.)\nThe conflict should show directly in the file with markings >>>>>, one option of text afterwards, a separator =======, the other option, and <<<<<<<."
  },
  {
    "objectID": "W11_projects.html#step-7-solve-the-conflict",
    "href": "W11_projects.html#step-7-solve-the-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 7: Solve the conflict",
    "text": "Step 7: Solve the conflict\n\nThe second member\n\nYou have to solve this conflict now!\nSolve is by editing the text\nDecide for an option or make a new text\nThereby, remove the >>>>>,=====,<<<<<<\nWhen you are done: git add, git commit, and git push"
  },
  {
    "objectID": "index.html#week-11-nov-10-data-analysis-projects-collaborative-git",
    "href": "index.html#week-11-nov-10-data-analysis-projects-collaborative-git",
    "title": "Data Science Concepts / Tools",
    "section": "Week 11, Nov 10: Data Analysis Projects, Collaborative Git",
    "text": "Week 11, Nov 10: Data Analysis Projects, Collaborative Git\nSlides Week 11 Projects\nThese slides are a main reference for your data science project.\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf"
  },
  {
    "objectID": "W11.html#example-events-for-one-coin-toss",
    "href": "W11.html#example-events-for-one-coin-toss",
    "title": "W#11: Probability",
    "section": "Example events for one coin toss",
    "text": "Example events for one coin toss\n\nThe set with one atomic event is a subset \\(\\{H\\} \\subset \\{H,T\\}\\).\nAlso the sample space \\(S = \\{H,T\\} \\subset \\{H,T\\}\\) is an event. It is called the sure event.\nAlso the empty set \\(\\{\\} = \\emptyset \\subset \\{H,T\\}\\) is an event. It is called the impossible event.\nIn interpretation, the event \\(\\{H,T\\}\\) means: The coin comes up HEAD or TAIL.\nThe empty set is interpreted as the event that it comes up neither HEADS nor TAILS."
  },
  {
    "objectID": "W11.html#example-events-for-two-or-three-coin-tosses",
    "href": "W11.html#example-events-for-two-or-three-coin-tosses",
    "title": "W#11: Probability",
    "section": "Example events for two or three coin tosses",
    "text": "Example events for two or three coin tosses\n\nThe event \\({HH, TH}\\) means “The first toss comes up HEAD or TAIL and the second is HEADS.”\n\nThe event \\({HT, TH, HH}\\) means that “We have HEAD once or twice and it does not matter what coins.”\nThe event \\({TT, HH}\\) means “Both coins show the same side.”\n\nQuiz questions for three coin tosses:\n\nWhat is the event “The coins show one HEAD”? \\(\\{HTT, THT, TTH\\}\\)\nWhat is the event “The first and the third coin are not HEAD? \\(\\{THT, TTT\\}\\)\nHow many atomic events exist? \\(2^3=8\\)\n\n\nFor selecting a random person:\nThe event \\(\\{2,5,6\\}\\) means that the selected person is either 2, 5, or 6.\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W11.html#more-example",
    "href": "W11.html#more-example",
    "title": "W#11: Probability",
    "section": "More example",
    "text": "More example\n\nThe set with one atomic event is a subset \\(\\{H\\} \\subset \\{H,T\\}\\).\nAlso the sample space \\(S = \\{H,T\\} \\subset \\{H,T\\}\\) is an event. It is called the sure event.\nAlso the empty set \\(\\{\\} = \\emptyset \\subset \\{H,T\\}\\) is an event. It is called the impossible event.\nIn interpretation, the event \\(\\{H,T\\}\\) means: The coin comes up HEAD or TAIL.\nThe empty set is interpreted as the event that it comes up neither HEADS nor TAILS."
  },
  {
    "objectID": "W11.html#more-example-events",
    "href": "W11.html#more-example-events",
    "title": "W#11: Probability",
    "section": "More example events",
    "text": "More example events\n\n2 coin tosses: The event \\({HH, TH}\\) means “The first toss comes up HEAD or TAIL and the second is HEADS.”\n\nThe event \\({HT, TH, HH}\\) means that “We have HEAD once or twice and it does not matter what coins.”\nThe event \\({TT, HH}\\) means “Both coins show the same side.”\n\nQuiz questions for three coin tosses:\n\nWhat is the event “The coins show one HEAD”? \\(\\{HTT, THT, TTH\\}\\)\nWhat is the event “The first and the third coin are not HEAD? \\(\\{THT, TTT\\}\\)\nHow many atomic events exist? \\(2^3=8\\)\n\n\nFor selecting a random person:\nThe event \\(\\{2,5,6\\}\\) means that the selected person is either 2, 5, or 6."
  },
  {
    "objectID": "W11.html#the-set-of-all-events",
    "href": "W11.html#the-set-of-all-events",
    "title": "W#11: Probability",
    "section": "The set of all events",
    "text": "The set of all events\nThe collection of all events is called a sigma-algebra. (This is a mathematical term which linguistic meaning we do not analyze deeper here.)\nDefinition: A sigma-algebra \\(\\mathcal{F}(S)\\) is a collection of subsets of a sample space \\(S\\) when it has the following properties\n\nThe empty set (the impossible event) is part of it \\(\\emptyset \\in \\mathcal{F}(S)\\)\nWhen \\(A \\in \\mathcal{F}(S)\\) then its complement \\(A^c \\in \\mathcal{F}(S)\\). That means: For any event \\(A\\) also its opposite \\(A^c = S \\setminus A\\) (read \\(S\\) minus the elements of \\(A\\)) is an event.\n\\(\\mathcal{F}(S)\\) is closed under the countable set union of its members. That means if \\(A_1,A_2,A_3, \\dots \\in \\mathcal{F}(S)\\) the \\(\\bigcup_{i}^\\infty A_i = A_1 \\cup A_2 \\cup A_3 \\cup \\dots \\in \\mathcal{F}(S)\\). The mathematical technicality is not central here. Import is: The sigma-algebra is the set of all possible events and this is large / more complex then one may naively think."
  },
  {
    "objectID": "W11.html#the-power-set",
    "href": "W11.html#the-power-set",
    "title": "W#11: Probability",
    "section": "The power set",
    "text": "The power set\n\n\n\nA sigma-algebra \\(\\mathcal{F}(S)\\) is a subset of the set of all subsets (also called power set) of the sample space sometimes denoted \\(\\mathcal{P}(S)\\) or \\(2^S\\).\nThe notation \\(2^S\\) matches the fact that the power set of a set with \\(n\\) elements has \\(2^n\\) elements.\n\n\n\nExample powerset of a three element set."
  },
  {
    "objectID": "W11.html#example-for-the-set-of-all-events",
    "href": "W11.html#example-for-the-set-of-all-events",
    "title": "W#11: Probability",
    "section": "Example for the set of all events",
    "text": "Example for the set of all events\n\nFor 3 coin tosses: How many events exist? \\(2^3=8\\) atomic events \\(\\to\\) \\(2^8=256\\) event\nHow is it for four coin tosses? \\(2^{(2^4)} = 65536\\)\nWe select two out of five people at random (without replacement). How many atomic events? How many events?\n\n\nThis can be computed by “n choose k” \\({n \\choose k} =\\frac{n!}{(n-k)!k!}\\). Here: \\({5\\choose 2}\\).\n\nchoose(5,2)\n\n[1] 10\n\n\nThus there are \\(2^{10} = 1024\\) event.\nExample event: “Person 1 is among the selected.” = \\(\\{12, 21, 13, 31, 14, 41, 15, 51\\}\\)\nThese are typical problems of combinatorics, the theory of counting, which is basic for many probability models. We do not go deeper into it here."
  },
  {
    "objectID": "W11.html#probability-function",
    "href": "W11.html#probability-function",
    "title": "W#11: Probability",
    "section": "Probability function",
    "text": "Probability function\nDefinition: For a collection of events (in a sigma-algebra \\(\\mathcal{F}(S)\\)) a function \\(\\text{Pr}: \\mathcal{F}(S) \\to \\mathbb{R}\\) is a probability function when\n\nThe probability of any event is between 0 and 1: \\(0\\leq \\text{Pr}(A) \\leq 1\\). (So, actually a probability function is a function \\(\\text{Pr}: \\mathcal{F}(S) \\to [0,1]\\).)\nThe probability of the event coinciding with the whole sample space (the sure event) is 1: \\(\\text{Pr}(S) = 1\\).\nFor events \\(A_1, A_2, \\dots, A_n \\in \\mathcal{F}(S)\\) which are pairwise disjoint we can sum up their probabilities:\n\\[\\text{Pr}(A_1 \\cup A_2\\cup\\dots\\cup A_n) = \\text{Pr}(A_1) + \\text{Pr}(A_2) + \\dots + \\text{Pr}(A_n) \\]\n\nThis captures the essence of how we think about probabilities mathematically. Most important: We can only easily add probabilities when they do not share atomic events."
  },
  {
    "objectID": "W11.html#some-basic-probability-rules",
    "href": "W11.html#some-basic-probability-rules",
    "title": "W#11: Probability",
    "section": "Some basic probability rules",
    "text": "Some basic probability rules\n\nWe can compute the probabilities of all events by summing the probabilities of the atomic events in it. So, the probabilities of the atomic events are building blocks for the whole probability function.\n\\(\\text{Pr}(\\emptyset) = 0\\)\nFor any events \\(A,B \\subset S\\) it holds\n\n\\(\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cap B)\\)\n\\(\\text{Pr}(A \\cap B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cup B)\\)\n\\(\\text{Pr}(A^c) = 1 - \\text{Pr}(A)\\)\n\n\nRecap from the motivation of logistic regression: When the probability of an event is \\(A\\) is \\(\\text{Pr}(A)=p\\), then its odds (in favor of the event) are \\(\\frac{p}{1-p}\\). The logistic regression model “raw” predictions are log-odds \\(\\log\\frac{p}{1-p}\\)."
  },
  {
    "objectID": "W11.html#sample-space-atomic-events-events",
    "href": "W11.html#sample-space-atomic-events-events",
    "title": "W#11: Probability",
    "section": "Sample space, atomic events, events",
    "text": "Sample space, atomic events, events\nIn the following, we say \\(S\\) is the sample space which is a set of atomic events.\nExample for sample spaces:\n\nFor a coin toss the atomic events are \\(H\\) (for HEADS) and \\(T\\) for TAILS, and the sample space is \\(S = \\{H,T\\}\\).\n\nFor the selection of one person of a group of \\(N\\) individuals labeled \\(1,\\dots,N\\), the sample space is \\(S = \\{1,\\dots,N\\}\\).\nFor two successive coin tosses the atomic events are \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\). The sample space is \\(\\{HH,HT, TH, TT\\}\\). Important: The atomic events for two coin tosses tare not \\(H\\) and \\(T\\).\n\nAn event \\(A\\) is a subset of the sample space \\(A \\subset S\\).\nImportant: Note the difference of atomic events and events."
  },
  {
    "objectID": "W11.html#random-variable",
    "href": "W11.html#random-variable",
    "title": "W#11: Probability",
    "section": "Random variable",
    "text": "Random variable\n\nA random variable is a numerical function where values come with probabilities.\nIn some statistical model, we consider variables in a data frame as random variables, for example the response variable in a generalized linear model.\n\nFormally, a random variable is\n\na function \\(X: S \\to \\mathbb{R}\\)\nwhich assigns a value to each atomic event in the sample space.\n\nTogether with a probability function \\(\\text{Pr}: \\mathcal{F}(S)\\to [0,1]\\) probabilities can be assigned to values of the random variable (see the probability mass function in two slides)."
  },
  {
    "objectID": "W11.html#probability-mass-function",
    "href": "W11.html#probability-mass-function",
    "title": "W#11: Probability",
    "section": "Probability mass function",
    "text": "Probability mass function\nFor a random variable \\(X\\) and a probability function \\(\\text{Pr}\\) the probability mass function \\(f_X: \\mathbb{R} \\to [0,1]\\) is defined as \\(f_X(x) = \\text{Pr}(X=x)\\), where \\(\\text{Pr}(X=x)\\) is an abbreviation for \\(\\text{Pr}(\\{a\\in S\\text{ for which } X(a) = x\\})\\).\nExample:\n\nTwo coin tosses \\(S = \\{HH, HT, TH, TT\\}\\)\n\nWe define \\(X\\) to be the number of heads: \\(X(HH) = 2\\), \\(X(TH) = 1\\), \\(X(HT) = 1\\), and \\(X(TT) = 0\\).\n\nWe assume the probability function \\(\\text{Pr}\\) assigns for each atomic event a probability of 0.25.\nThen the probability mass function is \\[\\begin{align} f_X(0) = & \\text{Pr}(X=0) = \\text{Pr}(\\{TT\\}) & = 0.25 \\\\\nf_X(1) = & \\text{Pr}(X=1) = \\text{Pr}(\\{HT,TH\\}) & = 0.25 + 0.25 = 0.5 \\\\\nf_X(2) = &\\text{Pr}(X=2) = \\text{Pr}(\\{HH\\}) & = 0.25\\end{align}\\]\n\n\nFor all \\(x\\not\\in \\{0,1,2\\}\\) it is obviously \\(f_X(x) = 0\\).\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W11.html#examples-of-random-variables",
    "href": "W11.html#examples-of-random-variables",
    "title": "W#11: Probability",
    "section": "Examples of random variables",
    "text": "Examples of random variables\n\n\nFor two coin tosses a random variable can be the number of HEADS. In this case each atomic event is mapped to a number: Either 0, 1, or 2.\nFor 62 randomly selected organ donations a random variable can be the number of complications. Each atomic event is mapped to an integer from 0 to 62. (Note, an atomic event are 62 randomly selected organ donations. So, the set of events is \\(2^62 \\approx 4.61\\cdot 10^18\\).)\nIn the palmer penguins dataset we can consider a variable, e.g. flipper length, to be a random variable. The atomic event would be the random selection of a penguin and the random variable is its flipper length. So we map each penguin to its flipper length."
  },
  {
    "objectID": "W11.html#probability-mass-function-pmf",
    "href": "W11.html#probability-mass-function-pmf",
    "title": "W#11: Probability",
    "section": "Probability mass function (pmf)",
    "text": "Probability mass function (pmf)\nFor\n\na random variable \\(X\\) and\na probability function \\(\\text{Pr}\\)\n\nthe probability mass function \\(f_X: \\mathbb{R} \\to [0,1]\\) is defined as\n\\[f_X(x) = \\text{Pr}(X=x),\\]\nwhere \\(\\text{Pr}(X=x)\\) is an abbreviation for \\(\\text{Pr}(\\{a\\in S\\text{ for which } X(a) = x\\})\\)."
  },
  {
    "objectID": "W11.html#example-pmf-for-2-coin-tosses",
    "href": "W11.html#example-pmf-for-2-coin-tosses",
    "title": "W#11: Probability",
    "section": "Example pmf for 2 coin tosses",
    "text": "Example pmf for 2 coin tosses\nTwo coin tosses \\(S = \\{HH, HT, TH, TT\\}\\)\n\nWe define \\(X\\) to be the number of heads:\n\\(X(HH) = 2\\), \\(X(TH) = 1\\), \\(X(HT) = 1\\), and \\(X(TT) = 0\\).\n\nWe assume the probability function \\(\\text{Pr}\\) assigns for each atomic event a probability of 0.25.\nThen the probability mass function is \\[\\begin{align} f_X(0) = & \\text{Pr}(X=0) = \\text{Pr}(\\{TT\\}) & = 0.25 \\\\\nf_X(1) = & \\text{Pr}(X=1) = \\text{Pr}(\\{HT,TH\\}) & = 0.25 + 0.25 = 0.5 \\\\\nf_X(2) = &\\text{Pr}(X=2) = \\text{Pr}(\\{HH\\}) & = 0.25\\end{align}\\]\nNote that \\(\\text{Pr}(\\{HT,TH\\}) = \\text{Pr}(\\{HT\\}) + \\text{Pr}(\\{HT\\})\\) by adding the probabilities of the atomic events.\nFor all \\(x\\) which are not 0, 1, or 2 it is obviously \\(f_X(x) = 0\\)."
  },
  {
    "objectID": "W11.html#binomial-distribution",
    "href": "W11.html#binomial-distribution",
    "title": "W#11: Probability",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe number of HEADS in several coin tosses and the number of complications in randomly selected organ donations are examples of random variable which have a binomial distribution.\nDefinition: The binomial distribution with parameters \\(n\\) and \\(p\\) is the number of successes in a sequence of \\(n\\) independent Bernoulli trials which each delivers a success with probability \\(p\\) and a failure with probability \\((1-p)\\).\n\nThe default model for the number of successes drawn from a sample of size \\(n\\) drawn from a population of size \\(N\\) with replacement.\nWhen \\(N\\) is much larger than \\(n\\) it is also a good approximation for drawing without replacement."
  },
  {
    "objectID": "W11.html#binomial-probability-mass-function",
    "href": "W11.html#binomial-probability-mass-function",
    "title": "W#11: Probability",
    "section": "Binomial probability mass function",
    "text": "Binomial probability mass function\n\\[f(k,n,p) = \\Pr(k;n,p) = \\Pr(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\\]\nwhere \\(k\\) is the number of successes, \\(n\\) is the number of Bernoulli trials, and \\(p\\) the success probability.\nProbability to have exactly 3 complications in 62 randomly selected organ donations with complication probability \\(p=0.1\\) is\n\n# x represents k, and size represents n\ndbinom(x = 3, size = 62, prob = 0.1)\n\n[1] 0.07551437\n\n\nThe probability to have 3 complications or less can be computed as\n\ndbinom(3, 62, 0.1) + dbinom(2, 62, 0.1) + dbinom(1, 62, 0.1) + dbinom(0, 62, 0.1)\n\n[1] 0.1209787\n\n\n\nThis was the p-value we computed with simulation for the hypothesis testing example."
  },
  {
    "objectID": "W11.html#distribution-functions-are-vectorized",
    "href": "W11.html#distribution-functions-are-vectorized",
    "title": "W#11: Probability",
    "section": "Distribution functions are vectorized!",
    "text": "Distribution functions are vectorized!\nCompute the p-value:\n\ndbinom(0:3, 62, 0.1) |> sum()\n\n[1] 0.1209787\n\n\nPlotting the probability mass function\n\n\n# What is this?\ntibble(x = 0:62) |> \n mutate(pr = dbinom(x, size = 62, prob = 0.1)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)"
  },
  {
    "objectID": "W11.html#plotting",
    "href": "W11.html#plotting",
    "title": "W#11: Probability",
    "section": "Plotting",
    "text": "Plotting"
  },
  {
    "objectID": "W11.html#general-sysematic-of-functions-for-distributions-in-r",
    "href": "W11.html#general-sysematic-of-functions-for-distributions-in-r",
    "title": "W#11: Probability",
    "section": "General sysematic of functions for distributions in R",
    "text": "General sysematic of functions for distributions in R\nIn R we usually have 4 function for each distribution like: dbinom, pbinom, qbinom, and rbinom\n\nThe mass function (or density function, more on this later)\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = dbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\n\nThe distribution function, or cumulative probability function\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = pbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nTo continue."
  },
  {
    "objectID": "W11.html#calculus",
    "href": "W11.html#calculus",
    "title": "W#11: Probability",
    "section": "Calculus",
    "text": "Calculus\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W11.html#other-plots-of-binomial-mass-function",
    "href": "W11.html#other-plots-of-binomial-mass-function",
    "title": "W#11: Probability",
    "section": "Other plots of binomial mass function",
    "text": "Other plots of binomial mass function\nChanging the sample size:\n\n\ntibble(samplesize = 0:62) |> \n mutate(pr = dbinom(3, size = samplesize, prob = 0.1)) |> \n ggplot(aes(samplesize, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nChanging the success probability:\n\n\ntibble(probs = seq(0,1,0.01)) |> \n mutate(pr = dbinom(3, size = 62, prob = probs)) |> \n ggplot(aes(probs, pr)) + geom_col() + theme_minimal(base_size = 24)"
  },
  {
    "objectID": "W11_projects.html#step-2-download-the-template-file",
    "href": "W11_projects.html#step-2-download-the-template-file",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 2: Download the template file",
    "text": "Step 2: Download the template file\nOnly the first team member needs to do the following: \n\nWho is the first team member? The first one mentioned in the TEAMname.\nExample: In the project project-PoroJan Poro is the first team member\n\nThe first team member copies the template file to project folder locally.\n\nGo to: https://github.com/JU-F22-MDSSB-MET-01/project-TEMPLATE\nClick on the file report.qmd\nClick on “Raw” to see the file content only. Usually, this happens in the browser.\nSelect “Save as” from your browser’s menu and save the file in the project folder on your computer: YOURPATH/project-TEAMname/\ncopy a template quarto file there\none team member commits and pushes the file\nthe other pulls the latest commits"
  },
  {
    "objectID": "W11_projects.html#git-configuration-for-divergent-branches",
    "href": "W11_projects.html#git-configuration-for-divergent-branches",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "??? git configuration for divergent branches",
    "text": "??? git configuration for divergent branches\n\n\nIf you pull for the first time in a local git repository, git may complain like this:\n\nRead that carefully. It advises to configure with git config pull.rebase false as the default version.\n\nHow to do the configuration?\n\nCopy the line git config pull.rebase false and close the window.\nGo to the Terminal pane (not the console, the one besides that). This is a terminal not for R but to speak with the computer in general. Paste the command and press enter. Now you are done and your next git pull should work.\n\n\n\n\n\nWhat is a branch and what a rebase? These are features of git well worth to learn but not now. Learn at http://happygitwithr.com"
  },
  {
    "objectID": "W11_projects.html#step-6-push-and-pull-the-other-way-round",
    "href": "W11_projects.html#step-6-push-and-pull-the-other-way-round",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 6: Push and pull the other way round",
    "text": "Step 6: Push and pull the other way round\n\nThe first member:\n\nThe successful merge creates a new commit, which you can directly push.\nPush.\n\nThe second team member:\n\nPull the changes of your colleague.\n\n\nPractice a bit more pulling and pushing commits and check the merging."
  },
  {
    "objectID": "W11_projects.html#step-7-create-a-merge-conflict",
    "href": "W11_projects.html#step-7-create-a-merge-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 7: Create a merge conflict",
    "text": "Step 7: Create a merge conflict\n\nBoth team members:\n\nChange a word in the title (but different ones)\ngit add and git commit on your local machine.\n\nFirst member: git push\nSecond member:\n\ngit pull. That should result in a conflict. If you receive several hints instead, first read the slide two slides before!\nThe conflict should show directly in the file with markings like this\n\n\n>>>>>>>>\none option of text,\n======== a separator,\nthe other option, and\n<<<<<<<."
  },
  {
    "objectID": "W11_projects.html#step-8-solve-the-conflict",
    "href": "W11_projects.html#step-8-solve-the-conflict",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Step 8: Solve the conflict",
    "text": "Step 8: Solve the conflict\n\nThe second member\n\nYou have to solve this conflict now!\nSolve is by editing the text\nDecide for an option or make a new text\nThereby, remove the >>>>>,=====,<<<<<<\nWhen you are done: git add, git commit, and git push.\n\n\nNow you know how to solve merge conflicts. Practice a bit in your team.\nWorking in VSCode: The workflow is very similar because it essentially relies on git not on the editor of choice."
  },
  {
    "objectID": "W11_projects.html#hints-for-working-collaboratively-with-git",
    "href": "W11_projects.html#hints-for-working-collaboratively-with-git",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Hints for working collaboratively with git",
    "text": "Hints for working collaboratively with git"
  },
  {
    "objectID": "W11_projects.html#hints-collaborative-work-with-git",
    "href": "W11_projects.html#hints-collaborative-work-with-git",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Hints: Collaborative work with git",
    "text": "Hints: Collaborative work with git\n\nWhenever you start a work session: First pull to see if there is anything new. That way you reduce the need for merges.\nInform your colleagues when you pushed new commits.\nCoordinate the work, e.g. discuss who works on what part and maybe when. However, git allows to also work without fully coordination and in parallel.\nWhen you finish your work session, end with a pushing a nice commit. That means. The file should render. You made comments when there are loose ends.\nYou can also use the issues section of the GitHub repository for things to do.\nNote: When you work on different parts of the file, be aware that also a successful merge can create problems. Example: Your colleague changed the data import, while you worked on graphics. Maybe after the merge the imported data is not what you need for your chunk. Then coordinate."
  },
  {
    "objectID": "W11_projects.html#advice-collaborative-work-with-git",
    "href": "W11_projects.html#advice-collaborative-work-with-git",
    "title": "W#11: Data Analysis Projects, Collaborative Git",
    "section": "Advice: Collaborative work with git",
    "text": "Advice: Collaborative work with git\n\nWhenever you start a work session: First pull to see if there is anything new. That way you reduce the need for merges.\nInform your colleagues when you pushed new commits.\nCoordinate the work, e.g. discuss who works on what part and maybe when. However, git allows to also work without fully coordination and in parallel.\nWhen you finish your work session, end with a pushing a nice commit. That means. The file should render. You made comments when there are loose ends.\nYou can also use the issues section of the GitHub repository for things to do.\nNote: When you work on different parts of the file, be aware that also a successful merge can create problems. Example: Your colleague changed the data import, while you worked on graphics. Maybe after the merge the imported data is not what you need for your chunk. Then coordinate.\n\nCommit and push often. This avoids that potential merge conflicts become large."
  },
  {
    "objectID": "index.html#last-data-science-tools-sessions-dec-8",
    "href": "index.html#last-data-science-tools-sessions-dec-8",
    "title": "Data Science Concepts / Tools",
    "section": "Last Data Science Tools Sessions, Dec 8",
    "text": "Last Data Science Tools Sessions, Dec 8\n9:45 - 12:30 in the regular rooms.\nInformal project presentations\n\nEach project team presents their report and the main insights.\nThere is no need to prepare additional slides, we can look at the visualization on the html-page of the report.\nEach presentation should be 5-15 minutes, and allow for short questions or feedback.\nThe presentation is not graded.\nThe final version of the report can be improved afterwards.\n\nDeadline for the reports is the end of the year Dec, 31. We (the instructors) need to submit the grades by January 9, 2023."
  },
  {
    "objectID": "index.html#exam-data-science-concepts-module-dec-15",
    "href": "index.html#exam-data-science-concepts-module-dec-15",
    "title": "Data Science Concepts / Tools",
    "section": "Exam Data Science Concepts module, Dec 15",
    "text": "Exam Data Science Concepts module, Dec 15\nScheduled officially on campusnet by academic offices."
  },
  {
    "objectID": "W12.html#probability-topics-for-data-science",
    "href": "W12.html#probability-topics-for-data-science",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability Topics for Data Science",
    "text": "Probability Topics for Data Science\nToday concepts and topics\n\nThe concept of probability form the mathematical perspective.\nWhat are probabilistic events, probability functions and random variables.\nHow do random variables relate to data?\n(Binomial distribution)"
  },
  {
    "objectID": "W12.html#sample-space-atomic-events-events",
    "href": "W12.html#sample-space-atomic-events-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Sample space, atomic events, events",
    "text": "Sample space, atomic events, events\nIn the following, we say \\(S\\) is the sample space which is a set of atomic events.\nExample for sample spaces:\n\nFor a coin toss the atomic events are \\(H\\) (for HEADS) and \\(T\\) for TAILS, and the sample space is \\(S = \\{H,T\\}\\).\n\nFor the selection of one person of a group of \\(N\\) individuals labeled \\(1,\\dots,N\\), the sample space is \\(S = \\{1,\\dots,N\\}\\).\nFor two successive coin tosses the atomic events are \\(HH\\), \\(HT\\), \\(TH\\), and \\(TT\\). The sample space is \\(\\{HH,HT, TH, TT\\}\\). Important: The atomic events for two coin tosses tare not \\(H\\) and \\(T\\).\n\nAn event \\(A\\) is a subset of the sample space \\(A \\subset S\\).\nImportant: Note the difference of atomic events and events."
  },
  {
    "objectID": "W12.html#example-events-for-one-coin-toss",
    "href": "W12.html#example-events-for-one-coin-toss",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example events for one coin toss",
    "text": "Example events for one coin toss\n\nThe set with one atomic event is a subset \\(\\{H\\} \\subset \\{H,T\\}\\).\nAlso the sample space \\(S = \\{H,T\\} \\subset \\{H,T\\}\\) is an event. It is called the sure event.\nAlso the empty set \\(\\{\\} = \\emptyset \\subset \\{H,T\\}\\) is an event. It is called the impossible event.\nIn interpretation, the event \\(\\{H,T\\}\\) means: The coin comes up HEAD or TAIL.\nThe empty set is interpreted as the event that it comes up neither HEADS nor TAILS."
  },
  {
    "objectID": "W12.html#more-example-events",
    "href": "W12.html#more-example-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "More example events",
    "text": "More example events\n\n2 coin tosses: The event \\(\\{HH, TH\\}\\) means “The first toss comes up HEAD or TAIL and the second is HEADS.”\n\nThe event \\(\\{HT, TH, HH\\}\\) means that “We have HEAD once or twice and it does not matter what coins.”\nThe event \\(\\{TT, HH\\}\\) means “Both coins show the same side.”\n\nQuiz questions for three coin tosses:\n\nWhat is the event “The coins show one HEAD”? \\(\\{HTT, THT, TTH\\}\\)\nWhat is the event “The first and the third coin are not HEAD? \\(\\{THT, TTT\\}\\)\nHow many atomic events exist? \\(2^3=8\\)\n\n\nFor selecting one random person:\nThe event \\(\\{2,5,6\\}\\) means that the selected person is either 2, 5, or 6. (Not all three people which is a different random variable!)"
  },
  {
    "objectID": "W12.html#the-set-of-all-events",
    "href": "W12.html#the-set-of-all-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The set of all events",
    "text": "The set of all events\nThe collection of all events is called a sigma-algebra. (This is a mathematical term which linguistic meaning we do not analyze deeper here.)\nDefinition: A sigma-algebra \\(\\mathcal{F}(S)\\) is a collection of subsets of a sample space \\(S\\) when it has the following properties\n\nThe empty set (the impossible event) is part of it \\(\\emptyset \\in \\mathcal{F}(S)\\)\nWhen \\(A \\in \\mathcal{F}(S)\\) then its complement \\(A^c \\in \\mathcal{F}(S)\\). That means: For any event \\(A\\) also its opposite \\(A^c = S \\setminus A\\) (read \\(S\\) minus the elements of \\(A\\)) is an event.\n\\(\\mathcal{F}(S)\\) is closed under the countable set union of its members. That means if \\(A_1,A_2,A_3, \\dots \\in \\mathcal{F}(S)\\) the \\(\\bigcup_{i}^\\infty A_i = A_1 \\cup A_2 \\cup A_3 \\cup \\dots \\in \\mathcal{F}(S)\\).\n\nThe mathematical technicality is not central here. Important is: The sigma-algebra is the set of all possible events and this is usually larger / more complex then one may naively think."
  },
  {
    "objectID": "W12.html#the-power-set",
    "href": "W12.html#the-power-set",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The power set",
    "text": "The power set\n\n\n\nA sigma-algebra \\(\\mathcal{F}(S)\\) is a subset of the set of all subsets (also called power set) of the sample space sometimes denoted \\(\\mathcal{P}(S)\\) or \\(2^S\\).\nThe notation \\(2^S\\) matches the fact that the power set of a set with \\(n\\) elements has \\(2^n\\) elements.\n\n\n\nExample powerset of a three element set."
  },
  {
    "objectID": "W12.html#example-for-the-set-of-all-events",
    "href": "W12.html#example-for-the-set-of-all-events",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example for the set of all events",
    "text": "Example for the set of all events\n\nFor 3 coin tosses: How many events exist? \\(2^3=8\\) atomic events \\(\\to\\) \\(2^8=256\\) event\nHow is it for four coin tosses? \\(2^{(2^4)} = 65536\\)\nWe select two out of five people at random (without replacement). How many atomic events? How many events?\n\n\nAtomic events: 12, 13, 14, 15, 23, 24, 25, 34, 35, 45 (here the order does not matter)\nThe number can be computed by “n choose k” \\({n \\choose k} =\\frac{n!}{(n-k)!k!}\\). Here: \\({5\\choose 2}\\).\n\nchoose(5,2)\n\n[1] 10\n\n\nThus there are \\(2^{10} = 1024\\) events.\nExample event: “Person 1 is among the selected.” = \\(\\{12, 13, 14, 15\\}\\)\nThese are typical problems of combinatorics, the theory of counting, which is basic for many probability models. We do not go deeper into it here."
  },
  {
    "objectID": "W12.html#probability-function",
    "href": "W12.html#probability-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability function",
    "text": "Probability function\nDefinition: For a collection of events (in a sigma-algebra \\(\\mathcal{F}(S)\\)) a function \\(\\text{Pr}: \\mathcal{F}(S) \\to \\mathbb{R}\\) is a probability function when\n\nThe probability of any event is between 0 and 1: \\(0\\leq \\text{Pr}(A) \\leq 1\\). (So, actually a probability function is a function \\(\\text{Pr}: \\mathcal{F}(S) \\to [0,1]\\).)\nThe probability of the event coinciding with the whole sample space (the sure event) is 1: \\(\\text{Pr}(S) = 1\\).\nFor events \\(A_1, A_2, \\dots, A_n \\in \\mathcal{F}(S)\\) which are pairwise disjoint we can sum up their probabilities:\n\\[\\text{Pr}(A_1 \\cup A_2\\cup\\dots\\cup A_n) = \\text{Pr}(A_1) + \\text{Pr}(A_2) + \\dots + \\text{Pr}(A_n) \\]\n\nThis captures the essence of how we think about probabilities mathematically. Most important: We can only easily add probabilities when they do not share atomic events."
  },
  {
    "objectID": "W12.html#some-basic-probability-rules",
    "href": "W12.html#some-basic-probability-rules",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Some basic probability rules",
    "text": "Some basic probability rules\n\nWe can compute the probabilities of all events by summing the probabilities of the atomic events in it. So, the probabilities of the atomic events are building blocks for the whole probability function.\n\\(\\text{Pr}(\\emptyset) = 0\\)\nFor any events \\(A,B \\subset S\\) it holds\n\n\\(\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cap B)\\)\n\\(\\text{Pr}(A \\cap B) = \\text{Pr}(A) + \\text{Pr}(B) - \\text{Pr}(A \\cup B)\\)\n\\(\\text{Pr}(A^c) = 1 - \\text{Pr}(A)\\)\n\n\nRecap from the motivation of logistic regression: When the probability of an event is \\(A\\) is \\(\\text{Pr}(A)=p\\), then its odds (in favor of the event) are \\(\\frac{p}{1-p}\\). The logistic regression model “raw” predictions are log-odds \\(\\log\\frac{p}{1-p}\\)."
  },
  {
    "objectID": "W12.html#random-variable",
    "href": "W12.html#random-variable",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Random variable",
    "text": "Random variable\n\nA random variable is a numerical function where values come with probabilities.\nIn some statistical model, we consider variables in a data frame as random variables, for example the response variable in a generalized linear model.\n\nFormally, a random variable is\n\na function \\(X: S \\to \\mathbb{R}\\)\nwhich assigns a value to each atomic event in the sample space.\n\nTogether with a probability function \\(\\text{Pr}: \\mathcal{F}(S)\\to [0,1]\\) probabilities can be assigned to values of the random variable (see the probability mass function in two slides)."
  },
  {
    "objectID": "W12.html#examples-of-random-variables",
    "href": "W12.html#examples-of-random-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Examples of random variables",
    "text": "Examples of random variables\n\n\nFor two coin tosses a random variable can be the number of HEADS. In this case, each atomic event is mapped to a number: Either 0, 1, or 2.\nFor 62 randomly selected organ donations a random variable can be the number of complications. Each atomic event is mapped to an integer from 0 to 62. (Note, an atomic event are 62 randomly selected organ donations. So, the set of events is \\(2^{62} \\approx 4.61\\cdot 10^{18}\\).)\nIn the palmer penguins dataset we can consider a variable, e.g. flipper length, to be a random variable. The atomic event would be the random selection of a penguin and the random variable is its flipper length. So we map each penguin to its flipper length.\n\n\n\nA random variable is a way to look at a numerical aspect of a sample space. It often simplfies because many atomic events may be mapped to the same number."
  },
  {
    "objectID": "W12.html#probability-mass-function-pmf",
    "href": "W12.html#probability-mass-function-pmf",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability mass function (pmf)",
    "text": "Probability mass function (pmf)\nFor\n\na random variable \\(X\\) and\na probability function \\(\\text{Pr}\\)\n\nthe probability mass function \\(f_X: \\mathbb{R} \\to [0,1]\\) is defined as\n\\[f_X(x) = \\text{Pr}(X=x),\\]\nwhere \\(\\text{Pr}(X=x)\\) is an abbreviation for \\(\\text{Pr}(\\{a\\in S\\text{ for which } X(a) = x\\})\\)."
  },
  {
    "objectID": "W12.html#example-pmf-for-2-coin-tosses",
    "href": "W12.html#example-pmf-for-2-coin-tosses",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example pmf for 2 coin tosses",
    "text": "Example pmf for 2 coin tosses\nTwo coin tosses \\(S = \\{HH, HT, TH, TT\\}\\)\n\nWe define \\(X\\) to be the number of heads:\n\\(X(HH) = 2\\), \\(X(TH) = 1\\), \\(X(HT) = 1\\), and \\(X(TT) = 0\\).\n\nWe assume the probability function \\(\\text{Pr}\\) assigns for each atomic event a probability of 0.25.\nThen the probability mass function is \\[\\begin{align} f_X(0) = & \\text{Pr}(X=0) = \\text{Pr}(\\{TT\\}) & = 0.25 \\\\\nf_X(1) = & \\text{Pr}(X=1) = \\text{Pr}(\\{HT,TH\\}) & = 0.25 + 0.25 = 0.5 \\\\\nf_X(2) = &\\text{Pr}(X=2) = \\text{Pr}(\\{HH\\}) & = 0.25\\end{align}\\]\nNote that \\(\\text{Pr}(\\{HT,TH\\}) = \\text{Pr}(\\{HT\\}) + \\text{Pr}(\\{HT\\})\\) by adding the probabilities of the atomic events.\nFor all \\(x\\) which are not 0, 1, or 2 it is obviously \\(f_X(x) = 0\\)."
  },
  {
    "objectID": "W12.html#binomial-distribution",
    "href": "W12.html#binomial-distribution",
    "title": "W#12: Probability",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe number of HEADS in several coin tosses and the number of complications in randomly selected organ donations are examples of random variable which have a binomial distribution.\n\nDefinition: The binomial distribution with parameters \\(n\\) and \\(p\\) is the number of successes in a sequence of \\(n\\) independent Bernoulli trials which each delivers a success with probability \\(p\\) and a failure with probability \\((1-p)\\).\n\nThe default model for the number of successes drawn from a sample of size \\(n\\) drawn from a population of size \\(N\\) with replacement.\nWhen \\(N\\) is much larger than \\(n\\) it is also a good approximation for drawing without replacement."
  },
  {
    "objectID": "W12.html#binomial-probability-mass-function",
    "href": "W12.html#binomial-probability-mass-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Binomial probability mass function",
    "text": "Binomial probability mass function\n\\[f(k,n,p) = \\Pr(k;n,p) = \\Pr(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\\]\nwhere \\(k\\) is the number of successes, \\(n\\) is the number of Bernoulli trials, and \\(p\\) the success probability.\nProbability to have exactly 3 complications in 62 randomly selected organ donations with complication probability \\(p=0.1\\) is\n\n# x represents k, and size represents n\ndbinom(x = 3, size = 62, prob = 0.1)\n\n[1] 0.07551437\n\n\nThe probability to have 3 complications or less can be computed as\n\ndbinom(3, 62, 0.1) + dbinom(2, 62, 0.1) + dbinom(1, 62, 0.1) + dbinom(0, 62, 0.1)\n\n[1] 0.1209787\n\n\n\nThis was the p-value we computed with simulation for the hypothesis testing example."
  },
  {
    "objectID": "W12.html#distribution-functions-are-vectorized",
    "href": "W12.html#distribution-functions-are-vectorized",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Distribution functions are vectorized!",
    "text": "Distribution functions are vectorized!\nCompute the p-value:\n\ndbinom(0:3, 62, 0.1) |> sum()\n\n[1] 0.1209787\n\n\nPlotting the probability mass function\n\n\ntibble(x = 0:62) |> \n mutate(pr = dbinom(x, size = 62, prob = 0.1)) |> \n ggplot(aes(x, pr)) + \n geom_col() + \n theme_minimal(base_size = 24)"
  },
  {
    "objectID": "W12.html#other-plots-of-binomial-mass-function",
    "href": "W12.html#other-plots-of-binomial-mass-function",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Other plots of binomial mass function",
    "text": "Other plots of binomial mass function\nChanging the sample size:\n\n\ntibble(samplesize = 0:62) |> \n mutate(pr = dbinom(3, size = samplesize, prob = 0.1)) |> \n ggplot(aes(samplesize, pr)) + \n geom_col() + \n theme_minimal(base_size = 24)\n\n\n\n\n\n\nThe probability of 3 successes is most likely for samples sizes around 30. Sensible?\n\nChanging the success probability:\n\n\ntibble(probs = seq(0,0.3,0.01)) |> \n mutate(pr = dbinom(3, size = 62, prob = probs)) |> \n ggplot(aes(probs, pr)) + \n geom_col() + \n scale_x_continuous(breaks = seq(0,0.3,0.05)) + \n theme_minimal(base_size = 24)\n\n\n\n\n\n\nThe probability of 3 successes is most likely for success probabilities around 0.05."
  },
  {
    "objectID": "W12.html#general-sysematic-of-functions-for-distributions-in-r",
    "href": "W12.html#general-sysematic-of-functions-for-distributions-in-r",
    "title": "W#12: Probability",
    "section": "General sysematic of functions for distributions in R",
    "text": "General sysematic of functions for distributions in R\nIn R we usually have 4 function for each distribution like: dbinom, pbinom, qbinom, and rbinom\n\nThe mass function (or density function, more on this later)\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = dbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\n\nThe distribution function, or cumulative probability function\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = pbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nTo continue."
  },
  {
    "objectID": "W12.html#calculus",
    "href": "W12.html#calculus",
    "title": "W#12: Probability",
    "section": "Calculus",
    "text": "Calculus"
  },
  {
    "objectID": "W12.html#three-step-of-pca",
    "href": "W12.html#three-step-of-pca",
    "title": "W#11: Probability",
    "section": "Three step of PCA",
    "text": "Three step of PCA\n\nLook at the data in PC coordinates.\nLook at the rotation matrix.\nLook at the variance explained by each PC.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W12.html#context-data-of-the-owid-corona-dataset",
    "href": "W12.html#context-data-of-the-owid-corona-dataset",
    "title": "W#11: Probability",
    "section": "Context data of the OWiD Corona dataset",
    "text": "Context data of the OWiD Corona dataset\n\nWe select the variables listed as Others in the OWiD corona data documentation.\n\nowid <- read_csv(\"data/owid-covid-data.csv\")\nowid_inds <- owid |> \n filter(date == \"2022-10-01\", !is.na(continent)) |> \n select(iso_code, continent, location, \n        population:human_development_index) |>\n select(-handwashing_facilities, -male_smokers, \n        - female_smokers, -extreme_poverty) |> \n drop_na()\n# owid_pca <- owid_inds |> select(where(is.numeric)) |> \n#  prcomp()\n\n\nRight column\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W12.html#context-data-of-the-owid-corona",
    "href": "W12.html#context-data-of-the-owid-corona",
    "title": "W#11: Probability",
    "section": "Context data of the OWiD Corona",
    "text": "Context data of the OWiD Corona\n\n\n\nSelect the variables listed as Others in the OWiD corona data documentation\nRemove those which have many NAs\n\n\nowid <- read_csv(\"data/owid-covid-data.csv\")\nowid_inds <- owid |> \n # Filter for one day and remove rows where continent is NA\n # These are rows with data for continents or world regions\n filter(date == \"2022-10-01\", !is.na(continent)) |> \n # These are the \"Other\" variables\n select(iso_code, continent, location, \n        population:human_development_index) |>\n # We remove the ones with many NA's\n select(-handwashing_facilities, -male_smokers, \n        - female_smokers, -extreme_poverty) |> \n drop_na()\nowid_inds |> count(continent)\n\n# A tibble: 6 × 2\n  continent         n\n  <chr>         <int>\n1 Africa           39\n2 Asia             42\n3 Europe           39\n4 North America    20\n5 Oceania           6\n6 South America    12\n\n\n\nWe have 158 countries and 11 numeric variables.\n\nnames(owid_inds)\n\n [1] \"iso_code\"                   \"continent\"                 \n [3] \"location\"                   \"population\"                \n [5] \"population_density\"         \"median_age\"                \n [7] \"aged_65_older\"              \"aged_70_older\"             \n [9] \"gdp_per_capita\"             \"cardiovasc_death_rate\"     \n[11] \"diabetes_prevalence\"        \"hospital_beds_per_thousand\"\n[13] \"life_expectancy\"            \"human_development_index\""
  },
  {
    "objectID": "W12.html#pca-description",
    "href": "W12.html#pca-description",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "PCA Description",
    "text": "PCA Description\nPrinciple component analysis\n\nis a dimensionality-reduction technique, that means it can be used to reduce the number of variables\ncomputes new variables which represent the data in a different way\ntransforms the data linearly to a new coordinate system where most of the variation in the data can be described with fewer variables than the original data\ncan be seen as unsupervised learning technique because there is no response variable. (Response variable are often produced/supervised by humans for training, e.g., a spam dummy.)\n\nToday: Quick walk through how to use and interpret it."
  },
  {
    "objectID": "W12.html#variables",
    "href": "W12.html#variables",
    "title": "W#11: Probability",
    "section": "2 Variables",
    "text": "2 Variables\n\npca1 <- owid_inds |> select(median_age, life_expectancy) |> \n prcomp(~., data = _, scale = FALSE)\nowid_inds |> ggplot(aes(median_age, life_expectancy, color = continent)) +\n geom_point()\n\n\n\npca1 |> augment(owid_inds) |> \n ggplot(aes(.fittedPC1, .fittedPC2, color = continent)) +\n geom_point()\n\n\n\narrow_style <- arrow(\n  angle = 20, ends = \"first\", type = \"closed\", length = grid::unit(8, \"pt\")\n)\npca1 |> tidy(matrix = \"rotation\") |> \n pivot_wider(names_from = \"PC\", names_prefix = \"PC\", values_from = \"value\") |>\n ggplot(aes(PC1*pca1$sdev+rep(mean(owid_inds$median_age),2),\n            PC2*pca1$sdev+rep(mean(owid_inds$life_expectancy,2)))) + \n geom_point(data = owid_inds,\n            mapping = aes(median_age, life_expectancy, color = continent)) + \n geom_segment(xend=mean(owid_inds$median_age),\n              yend=mean(owid_inds$life_expectancy), arrow = arrow_style) +\n coord_fixed() +\n labs(x=\"Median age\",y=\"Life expectancy\")"
  },
  {
    "objectID": "W12.html#other-data-of-owid-corona-data",
    "href": "W12.html#other-data-of-owid-corona-data",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "“Other” data of OWiD Corona data",
    "text": "“Other” data of OWiD Corona data\n\n\n\nSelect the variables listed as Others in the OWiD corona data documentation\nRemove those which have many NAs\n\n\nowid <- read_csv(\"data/owid-covid-data.csv\")\nowid_inds <- owid |> \n # Filter for one day and remove rows where continent is NA\n # These are rows with data for continents or world regions\n filter(date == \"2022-10-01\", !is.na(continent)) |> \n # These are the \"Other\" variables\n select(iso_code, continent, location, \n        population:human_development_index) |>\n # We remove the ones with many NA's\n select(-handwashing_facilities, -male_smokers, \n        - female_smokers, -extreme_poverty) |> \n drop_na()\nowid_inds |> count(continent)\n\n# A tibble: 6 × 2\n  continent         n\n  <chr>         <int>\n1 Africa           39\n2 Asia             42\n3 Europe           39\n4 North America    20\n5 Oceania           6\n6 South America    12\n\n\n\nWe have 158 countries and 11 numeric variables.\n\nnames(owid_inds)\n\n [1] \"iso_code\"                   \"continent\"                 \n [3] \"location\"                   \"population\"                \n [5] \"population_density\"         \"median_age\"                \n [7] \"aged_65_older\"              \"aged_70_older\"             \n [9] \"gdp_per_capita\"             \"cardiovasc_death_rate\"     \n[11] \"diabetes_prevalence\"        \"hospital_beds_per_thousand\"\n[13] \"life_expectancy\"            \"human_development_index\""
  },
  {
    "objectID": "W12.html#two-variables",
    "href": "W12.html#two-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Two Variables",
    "text": "Two Variables\nExample for the new axes.\n\n\n\n\n\n\n\nThe two arrows show the two eigenvectors of the covariance matrix of the two variables scaled by the square root of the corresponding eigenvalues, and shifted so their origins are at the means of both variables."
  },
  {
    "objectID": "W12.html#computation-in-r",
    "href": "W12.html#computation-in-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Computation in R",
    "text": "Computation in R\nThe basic function is base-R’s prcomp (there is an older princomp which is not advisable to use).\nThese 3 commands all deliver identical results for prcomp\n\n# prcomp can take a formula with no response variable as 1st argument\nP <- prcomp(~ median_age + life_expectancy, data= owid_inds)\n# prcomp can take a data frame with all numerical vectors as 1st argument\nP <- owid_inds |> select(median_age, life_expectancy) |> prcomp()\n# This is an example using the formula placeholder \".\" for 'take all\n# and the pipe's placeholder \"_\" which one uses when the output should be \n# piped to an argument other than the 1st\nP <- owid_inds |> select(median_age, life_expectancy) |> prcomp(~ ., data= _)\n\n\n\nThe standard output\n\nP\n\nStandard deviations (1, .., p=2):\n[1] 10.69296  2.95809\n\nRotation (n x k) = (2 x 2):\n                      PC1        PC2\nmedian_age      0.8091382  0.5876184\nlife_expectancy 0.5876184 -0.8091382\n\n\n\nThe summary output\n\nsummary(P)\n\nImportance of components:\n                           PC1     PC2\nStandard deviation     10.6930 2.95809\nProportion of Variance  0.9289 0.07109\nCumulative Proportion   0.9289 1.00000"
  },
  {
    "objectID": "W12.html#the-prcomp-object",
    "href": "W12.html#the-prcomp-object",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "The prcomp object",
    "text": "The prcomp object\nIncludes 4 different related entities.\n\n\nThe standard deviations related to each principal component.\n\nP$sdev\n\n[1] 10.69296  2.95809\n\n\nThe matrix of variable loadings. (It is also the matrix which rotates the original data vectors.)\n\nP$rotation\n\n                      PC1        PC2\nmedian_age      0.8091382  0.5876184\nlife_expectancy 0.5876184 -0.8091382\n\n\n\nThe means for each original variable.\n\nP$center\n\n     median_age life_expectancy \n       31.15823        73.57741 \n\n\nNote, there are also standard deviations of original variables in $scale when this is set to be used.\nThe centered (scaled, if set) and rotated data.\n\nP$x\n\n             PC1         PC2\n1   -15.30147798 -0.30158529\n2     8.46967981 -0.01934856\n3     0.27527455 -3.88170827\n4     2.78495581 -2.23213257\n5     2.41746174 -2.06645786\n6     4.56375002  1.44493027\n7    11.25046735 -4.01861358\n8    15.39339097  1.33826834\n9     0.66547151  1.19688861\n10    2.74344303  1.56895631\n11    3.18635423 -2.27431439\n12   -3.54022934 -1.35069463\n13   10.29045200  0.53669877\n14    8.10950037  4.39071615\n15   13.34251723 -0.26236181\n16   -4.37020966 -4.46229111\n17  -16.93776248  2.29190137\n18   -3.12614815 -0.04891248\n19   -5.87404744 -1.71082388\n20   11.42328837  3.57162574\n21   -6.67861938  0.07776886\n22    3.24786441 -0.48704930\n23    2.34605999 -1.11724672\n24   11.82248935  6.76586095\n25  -18.02037585  1.74049561\n26  -18.10128967  1.68173378\n27   -6.70529480 -0.22585660\n28  -18.39505599  4.29856418\n29   13.48895662 -1.14471974\n30   -4.76750699 -2.72397157\n31  -22.33121139  8.86767576\n32    7.31198595 -2.84986882\n33    8.06063011  1.73515374\n34    3.02452659 -2.39183806\n35  -14.14471452  1.16878824\n36    5.91429898 -3.98849568\n37   13.27749971  3.57109260\n38    9.31944327 -2.38070454\n39   13.23408329  2.43962673\n40   13.31812495  0.62211827\n41   -8.45956818  1.84938433\n42   -2.58376417 -2.49754876\n43   -0.45747329 -4.57451459\n44   -5.67290445 -2.15797206\n45   -3.03035412 -1.88260370\n46  -15.80534846  6.85901626\n47  -13.85952988 -1.09586840\n48   12.37252460  2.60490418\n49  -15.68152626  5.15690932\n50  -13.29042762 -1.02861793\n51   -5.67641179  3.46274744\n52   14.31618860  0.09869783\n53   14.10959178 -0.97825052\n54  -10.69666185  1.01571059\n55  -17.82510905  1.30143881\n56    6.21550849  4.27584778\n57   17.05009523  2.80094774\n58  -13.72522235  1.78240564\n59   16.53294823  1.30072809\n60   -2.11451419 -0.08048350\n61   -6.25743781 -5.43736541\n62  -16.87582996  2.54697854\n63   -6.08601239  0.11265381\n64  -11.17711330  3.71942404\n65   -4.06917156 -5.04699279\n66   11.84595123  4.52123414\n67   10.50055615 -4.00707238\n68   -4.69555434  1.43141323\n69   -2.59500849  0.41096866\n70    2.82790704 -1.78074007\n71  -10.77812658 -4.14764718\n72   11.22789301 -2.62610131\n73    5.06757765 -7.92793256\n74   19.38298293  1.80093025\n75    0.72013226 -0.58016293\n76   20.28385695  1.07098094\n77   -5.87954413 -5.44718170\n78   -0.43840629 -0.34630735\n79  -13.06983814 -0.99200808\n80   -9.49927316 -0.46289021\n81    3.18052090 -0.05396172\n82   -5.18108013 -1.13341907\n83   -8.79273555  0.60636402\n84   11.31620717  6.10157310\n85    3.09816863 -4.36520494\n86  -15.24495644  0.64165663\n87   -2.13848412 -0.72819134\n88   11.36862762  5.34867729\n89   12.00765033 -1.99802605\n90  -13.19370320 -1.50216242\n91  -16.04098955 -0.13418567\n92    0.49949992 -2.82903407\n93  -20.32523538  2.87209737\n94   14.35685668 -0.63801521\n95    5.88052317  2.52479529\n96   -0.63823938 -2.28346164\n97    4.22661012  5.14255609\n98   -4.24849920  1.49654154\n99    8.36665685  1.99447523\n100   0.56232000 -3.42607145\n101 -18.36842342  2.38992835\n102  -5.45400436  4.00738947\n103  -6.62666413 -1.35520031\n104  14.85726268  0.03436403\n105  10.57470625 -3.08810461\n106  -2.59145830 -2.99748955\n107 -19.54962201 -0.40822638\n108   7.73202903  2.86834452\n109  12.09579308 -2.11939679\n110   2.14576170 -3.73447438\n111  -9.90291188  0.60344735\n112   1.71857540 -4.84803258\n113  -3.37392111 -3.28148244\n114   0.19300798 -3.76842892\n115  -6.20040822 -1.60178884\n116  11.63842401  2.08413905\n117  17.14952515  1.98332087\n118   4.50938307 -4.94698996\n119  11.03457271  4.95777153\n120   6.24446705  5.76757879\n121   4.56869580  0.07669216\n122  -0.09619205  1.22461257\n123 -11.95340613 -4.74163201\n124   1.51252948 -0.82038499\n125   3.97524412  3.10618305\n126  14.99736068 -1.51997588\n127  10.45367524  2.69444253\n128  15.34502876  1.57504058\n129  -8.72053195 -5.61948425\n130  -8.67330823  5.37709112\n131  15.45980409 -0.45496597\n132  17.47043209  0.35018930\n133   4.37972753 -1.02453034\n134 -14.12936912 -0.04359145\n135  -2.37577176  0.61961970\n136  13.38272012 -1.67914822\n137  15.65777638 -1.23810515\n138  -7.81416124 -2.61307574\n139 -15.65950282 -1.34019889\n140   9.33445203  2.36362626\n141 -13.04278322 -4.43283183\n142 -16.88124094  3.23514328\n143  -8.73494696 -3.04695782\n144   4.03988214  3.01717785\n145   3.08240087 -1.62063733\n146   2.77409099 -3.06806436\n147 -17.93950488 -0.41300384\n148   7.39535433  7.24604370\n149   4.88055588 -1.88433903\n150  12.35121731 -0.59914731\n151   8.88283060 -0.07771314\n152   6.13991994 -0.89560137\n153  -3.48506054 -0.23541152\n154  -2.63795972 -0.04042384\n155   2.23758320 -0.62751947\n156 -13.16791532 -0.34642241\n157 -16.58206363 -0.06985186\n158 -16.45498504  2.98855475"
  },
  {
    "objectID": "W12.html#pca-as-exploratory-data-analysis",
    "href": "W12.html#pca-as-exploratory-data-analysis",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "PCA as Exploratory Data Analysis",
    "text": "PCA as Exploratory Data Analysis\nSuppose we do a PCA with all 158 countries (rows) and all 11 numeric variables.\n\nHow long will the vector of standard deviations be? 11\n\nWhat dimensions will the rotation matrix have? 11 x 11\n\nWhat dimensions will the rotated data frame have? 158 x 11\n\n\nWhen we do a PCA for exploration there are 3 things to look at:\n\nThe data in PC coordinates - the centered (scaled, if set) and rotated data.\n\nThe rotation matrix - the variable loadings.\nThe variance explained by each PC - based on the standard deviations."
  },
  {
    "objectID": "W12.html#all-variables",
    "href": "W12.html#all-variables",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "All variables",
    "text": "All variables\nNow, with scale = TRUE (recommended). Data will be centered and scaled (a.k.a. standardized) first.\n\nowid_PCA <- owid_inds |> select(-iso_code, -continent, -location) |> \n prcomp(~ ., data = _, scale = TRUE)\nowid_PCA\n\nStandard deviations (1, .., p=11):\n [1] 2.34241265 1.22235717 1.03373351 0.99489682 0.92287148 0.66475775\n [7] 0.58432716 0.45329540 0.25966139 0.21927321 0.06713354\n\nRotation (n x k) = (11 x 11):\n                                    PC1         PC2         PC3          PC4\npopulation                  0.008865647 -0.03734342  0.47595823  0.870416239\npopulation_density         -0.074719077 -0.46044638 -0.10288680  0.046602918\nmedian_age                 -0.409689271  0.06447942  0.12973022 -0.004468196\naged_65_older              -0.390007415  0.24159016 -0.02468961  0.056783114\naged_70_older              -0.385548522  0.26731058 -0.03864289  0.046944080\ngdp_per_capita             -0.307869646 -0.34923225 -0.10326975 -0.049084375\ncardiovasc_death_rate       0.203402666  0.30746105  0.55758803 -0.314193965\ndiabetes_prevalence        -0.015935611 -0.52365806  0.58702717 -0.310745215\nhospital_beds_per_thousand -0.281461985  0.34868645  0.26585730 -0.180587716\nlife_expectancy            -0.385997705 -0.16628186  0.03523644 -0.001063976\nhuman_development_index    -0.401390496 -0.11256612  0.07650908 -0.066730340\n                                   PC5        PC6          PC7         PC8\npopulation                  0.01039838 -0.1088643 -0.009530076 -0.01188854\npopulation_density          0.85694114  0.1686582 -0.030330909  0.05784099\nmedian_age                  0.02964348  0.1083349  0.151566064 -0.04089755\naged_65_older               0.06392873  0.3010299  0.133281270 -0.30838406\naged_70_older               0.04970870  0.2883236  0.134769694 -0.32266678\ngdp_per_capita             -0.04210924 -0.7155301  0.255572328 -0.38998232\ncardiovasc_death_rate       0.30899782 -0.1596115  0.556827122  0.11586467\ndiabetes_prevalence        -0.24595405  0.3128238 -0.194418712 -0.28853159\nhospital_beds_per_thousand  0.24614720 -0.3347129 -0.712831766  0.04133735\nlife_expectancy            -0.15876730  0.1057214  0.114444482  0.64382282\nhuman_development_index    -0.12068739 -0.1252574  0.075440136  0.36145260\n                                   PC9        PC10         PC11\npopulation                 -0.01568443  0.04316042  0.008103028\npopulation_density          0.01930238  0.04356815  0.017850825\nmedian_age                  0.43211922 -0.76462456  0.050093900\naged_65_older              -0.12286262  0.19597621 -0.724308013\naged_70_older              -0.18333355  0.25044108  0.687015497\ngdp_per_capita             -0.19559611 -0.02457951 -0.012540620\ncardiovasc_death_rate      -0.07816501  0.06233211 -0.006416554\ndiabetes_prevalence        -0.04883139  0.05032474  0.006819981\nhospital_beds_per_thousand -0.11397971 -0.01992701 -0.007573789\nlife_expectancy            -0.58309459 -0.13429476 -0.009463395\nhuman_development_index     0.60349415  0.53386017  0.010100958"
  },
  {
    "objectID": "W12.html#data-in-pc-coordinates",
    "href": "W12.html#data-in-pc-coordinates",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Data in PC coordinates",
    "text": "Data in PC coordinates\n\n\n\nStart plotting PC1 against PC2. By default these are the most important ones. Drill deeper later.\nUse the function augment to append the original data. Here used to draw labels and color by continent.\nNote: augment also created the variables names like .fittedPC1\n\n\n\nplotdata <- owid_PCA |> parsnip::augment(owid_inds)\nplotdata |> ggplot(aes(.fittedPC1, .fittedPC2, color = continent)) +\n geom_point() + \n geom_text(data = plotdata |> \n            filter(.fittedPC2< -3 | .fittedPC1< -5 | .fittedPC1>4), \n           mapping = aes(.fittedPC1, .fittedPC2, label = iso_code),\n           color = \"black\") +\n coord_fixed() + theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W12.html#variable-loadings",
    "href": "W12.html#variable-loadings",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Variable loadings",
    "text": "Variable loadings\n\nThe columns of the rotation matrix shows how the original variables load on the principle components.\nWe can try to interpret these loadings and give names to components.\ntidy extracts the rotation matrix in long format with a PC, a column (for the original variable name), and a value variable .\n\n\nowid_PCA |> parsnip::tidy(matrix = \"rotation\") |> \n filter(PC<=6) |> \n ggplot(aes(value, column, fill=value)) + geom_col() +\n facet_wrap(~PC, nrow = 1)"
  },
  {
    "objectID": "W12.html#variance-explained",
    "href": "W12.html#variance-explained",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Variance explained",
    "text": "Variance explained\n\nPrinciple components are by default sorted by importance.\nThe squares of the standard deviation for each component gives its variances and variances have to sum up to the number of variables (in the standardized case).\nThis way the tidy command creates a variable percent which gives the fraction of the total variance explained by each component.\n\n\nowid_PCA |> tidy(matrix = \"eigenvalues\") |> \n ggplot(aes(PC, percent)) + geom_col()"
  },
  {
    "objectID": "W12.html#interpretations",
    "href": "W12.html#interpretations",
    "title": "W#11: Probability",
    "section": "Interpretations",
    "text": "Interpretations\n\nThe first component explains almost 50% of the variance. So most emphasize should be on this.\nTo reach mor than 75% of the total variance one also needs to consider\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W12.html#interpretations-1",
    "href": "W12.html#interpretations-1",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (1)",
    "text": "Interpretations (1)\n\n\nThe first component explains almost 50% of the variance. So most emphasize should be on this.\nTo reach more than 75% of the total variance the first four components are needed.\nAfter the fifth component the added explained variance drops substantially. This is another typical reason to cut off the rest.\nTaking the five components explains 89.9% of the variance of the original 11 variables!"
  },
  {
    "objectID": "W12.html#interpretations-2",
    "href": "W12.html#interpretations-2",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (2)",
    "text": "Interpretations (2)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population.\nPC4 focuses mostly on population.\nPC5 mostly on population density."
  },
  {
    "objectID": "W12.html#interpretations-3",
    "href": "W12.html#interpretations-3",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (3)",
    "text": "Interpretations (3)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths."
  },
  {
    "objectID": "W12.html#interpretations-4",
    "href": "W12.html#interpretations-4",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (4)",
    "text": "Interpretations (4)\n\n\nTo score high on PC1 a country needs to be poor, and having few old people.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population."
  },
  {
    "objectID": "W12.html#interpretations-5",
    "href": "W12.html#interpretations-5",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (5)",
    "text": "Interpretations (5)\n\n\nPC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths.\nPC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population."
  },
  {
    "objectID": "W12.html#interpretations-6",
    "href": "W12.html#interpretations-6",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Interpretations (6)",
    "text": "Interpretations (6)\n\n\nPC4 focuses mostly on population.\nPC5 mostly on population density."
  },
  {
    "objectID": "W12.html#apply-pca",
    "href": "W12.html#apply-pca",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Apply PCA",
    "text": "Apply PCA\n\nBesides standardization, PCA may benefit by preprocessing steps of data transformation with variables with skew distributions (log, square-root, or Box-Cox transformation). This may result in less outliers.\nPCA is a often a useful step of exploratory data analysis when you have a large number of numerical variables to show the empirical dimensionality of the data and its structure\nLimitation: PCA is only sensitive for linear relation ships (no U-shaped) or the like\nThe can be used as predictors in a model instead of the raw variables."
  },
  {
    "objectID": "W12.html#properties-and-realtions-of-pca",
    "href": "W12.html#properties-and-realtions-of-pca",
    "title": "W#11: Probability",
    "section": "Properties and realtions of PCA",
    "text": "Properties and realtions of PCA\n\nThe principal components (the columns of the rotation matrix) are maximally uncorrelated (actually they are even orthogonal).\nThis also holds for the columns of the rotated data.\nThe sum of the variances is the number of variables (when variables are standardized)\nThe PCA is unique. All principle components together are a complete representation of the data. (Unlike other technique of dimensionality reduction which may rely on starting values, random factors, or tuning parameters)\nA technique similar in spirit is factor analysis (e.g. factanal). It is more theory based as it requires to specify to the theoriezed number of factors up front.\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W12.html#properties-and-relations-of-pca",
    "href": "W12.html#properties-and-relations-of-pca",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Properties and relations of PCA",
    "text": "Properties and relations of PCA\n\nThe principal components (the columns of the rotation matrix) are maximally uncorrelated (actually they are even orthogonal).\nThis also holds for the columns of the rotated data.\nThe total variances of all prinicipal components sum up to the number of variables (when variables are standardized)\nThe PCA is unique. All principle components together are a complete representation of the data. (Unlike other technique of dimensionality reduction which may rely on starting values, random factors, or tuning parameters)\nA technique similar in spirit is factor analysis (e.g. factanal). It is more theory based as it requires to specify to the theoriezed number of factors up front.\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W12.html#example-2-die-rolls",
    "href": "W12.html#example-2-die-rolls",
    "title": "W#12: Probability",
    "section": "Example: 2 die rolls 🎲 🎲",
    "text": "Example: 2 die rolls 🎲 🎲\nRandom variable: The sum of both dice.\n\nEvents: All 36 combinations of rolls 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 31, 32, 33, 34, 35, 36, 41, 42, 43, 44, 45, 46, 51, 52, 53, 54, 55, 56, 61, 62, 63, 64, 65, 66\nAttention, these are not 2-digit numbers! 11 does not mean eleven but 1 and 1.\n\n\nPossible values of the random variable:  2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 (These are numbers.)\n\n\nProbability mass function: (Assuming each number has probability of \\(\\frac{1}{6}\\) for each die.)\n\n\n\\(\\text{Pr}(2) = \\text{Pr}(12) = \\frac{1}{36}\\)\n\\(\\text{Pr}(3) = \\text{Pr}(11) = \\frac{2}{36}\\)\n\\(\\text{Pr}(4) = \\text{Pr}(10) = \\frac{3}{36}\\)\n\\(\\text{Pr}(5) = \\text{Pr}(9) = \\frac{4}{36}\\)\n\\(\\text{Pr}(6) = \\text{Pr}(8) = \\frac{5}{36}\\)\n\\(\\text{Pr}(7) = \\frac{6}{36}\\)\n\n\ntibble(Value = 0:15, pmf=c(0,c(0:6,5:0)/36,0,0)) |> \n ggplot(aes(Value,pmf)) + geom_line() + geom_point() +\n theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W12.html#example-roll-two-dice",
    "href": "W12.html#example-roll-two-dice",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Example: Roll two dice 🎲 🎲",
    "text": "Example: Roll two dice 🎲 🎲\nRandom variable: The sum of both dice.\n\nEvents: All 36 combinations of rolls 1+1, 1+2, 1+3, 1+4, 1+5, 1+6, 2+1, 2+2, 2+3, 2+4, 2+5, 2+6, 3+1, 3+2, 3+3, 3+4, 3+5, 3+6, 4+1, 4+2, 4+3, 4+4, 4+5, 4+6, 5+1, 5+2, 5+3, 5+4, 5+5, 5+6, 6+1, 6+2, 6+3, 6+4, 6+5, 6+6\n\n\nPossible values of the random variable:  2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 (These are numbers.)\n\n\nProbability mass function: (Assuming each number has probability of \\(\\frac{1}{6}\\) for each die.)\n\n\n\\(\\text{Pr}(2) = \\text{Pr}(12) = \\frac{1}{36}\\)\n\\(\\text{Pr}(3) = \\text{Pr}(11) = \\frac{2}{36}\\)\n\\(\\text{Pr}(4) = \\text{Pr}(10) = \\frac{3}{36}\\)\n\\(\\text{Pr}(5) = \\text{Pr}(9) = \\frac{4}{36}\\)\n\\(\\text{Pr}(6) = \\text{Pr}(8) = \\frac{5}{36}\\)\n\\(\\text{Pr}(7) = \\frac{6}{36}\\)\n\n\ntibble(Value = 0:15, pmf=c(0,c(0:6,5:0)/36,0,0)) |> \n ggplot(aes(Value,pmf)) + geom_line() + geom_point() +\n theme_minimal(base_size = 20)"
  },
  {
    "objectID": "W12.html#expected-value-of-a-discrete-random-variable",
    "href": "W12.html#expected-value-of-a-discrete-random-variable",
    "title": "W#12: Probability",
    "section": "Expected value of a discrete random variable",
    "text": "Expected value of a discrete random variable\nA discrete random variable takes only a finite (or at least discrete) set of values.\nFor \\(X: S \\to \\mathbb{R}\\), when \\(S\\) is finite, there is naturally only a set of values \\(x_1,\\dots,x_k\\in\\mathbb{R}\\) which \\(X\\) can be. We call their probabilities \\(p_1,\\dots,p_k\\) with \\(p_i = \\text{Pr}(X=x_i) = \\text{Pr}(\\{a \\in S \\text{ for which } X(a) = x_i \\})\\). (The probability of all other values in \\(\\mathbb{R}\\) is zero.).\nThe expected value of \\(X\\) is\n\\[E(X) = \\sum_{i=1}^k p_i x_i = p_1x_1 + \\dots + p_kx_k.\\]\nExamples: \\(X\\) is a die roll 🎲. \\(E(X) = 1\\cdot\\frac{1}{6} + 2\\cdot\\frac{1}{6} + 3\\cdot\\frac{1}{6} + 4\\cdot\\frac{1}{6} + 5\\cdot\\frac{1}{6} + 6\\cdot\\frac{1}{6} = \\frac{21}{6} = 3.5\\)\n\\(X\\) sum of two die rolls 🎲🎲. \\(E(X) = 2\\cdot\\frac{1}{36} + 3\\cdot\\frac{2}{36} + 4\\cdot\\frac{3}{36} + 5\\cdot\\frac{4}{36} + 6\\cdot\\frac{5}{36} + 7\\cdot\\frac{6}{36} + 8\\cdot\\frac{5}{36} + 9\\cdot\\frac{4}{36} + 10\\cdot\\frac{3}{36} + 11\\cdot\\frac{2}{36} + 12\\cdot\\frac{1}{36} = 7\\)."
  },
  {
    "objectID": "W12.html#general-systematic-of-functions-for-distributions-in-r",
    "href": "W12.html#general-systematic-of-functions-for-distributions-in-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "General systematic of functions for distributions in R",
    "text": "General systematic of functions for distributions in R\nIn R we usually have 4 function for each distribution: The d, p, q, and r version. For the binomial distribution:\n\ndbinom the density function (more on the name later)\npbinom distribution function\nqbinom the quantile function, and\nrbinom random number generator."
  },
  {
    "objectID": "W12.html#expected-value-discrete-random-variable",
    "href": "W12.html#expected-value-discrete-random-variable",
    "title": "W#12: Probability",
    "section": "Expected value discrete random variable",
    "text": "Expected value discrete random variable\nA discrete random variable takes only a finite (or at least discrete) set of values.\nFor \\(X: S \\to \\mathbb{R}\\), when \\(S\\) is finite, there is naturally only a set of values \\(x_1,\\dots,x_k\\in\\mathbb{R}\\) which \\(X\\) can be. We call their probabilities \\(p_1,\\dots,p_k\\) with \\(p_i = \\text{Pr}(X=x_i) = \\text{Pr}(\\{a \\in S \\text{ for which } X(a) = x_i \\})\\). (The probability of all other values in \\(\\mathbb{R}\\) is zero.).\nThe expected value of \\(X\\) is \\(E(X) = \\sum_{i=1}^k p_i x_i = p_1x_1 + \\dots + p_kx_k.\\)\nExamples: \\(X\\) is a die roll 🎲. \\(E(X) = 1\\cdot\\frac{1}{6} + 2\\cdot\\frac{1}{6} + 3\\cdot\\frac{1}{6} + 4\\cdot\\frac{1}{6} + 5\\cdot\\frac{1}{6} + 6\\cdot\\frac{1}{6} = \\frac{21}{6} = 3.5\\)\n\\(X\\) sum of two die rolls 🎲🎲.\n$E(X) = 2 + 3 + 4 + 5 + 6 + 7 + 8 + $\n$ + 9 + 10 + 11 + 12 = 7$"
  },
  {
    "objectID": "W12.html#expected-value-binomial-distribution",
    "href": "W12.html#expected-value-binomial-distribution",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Expected value binomial distribution",
    "text": "Expected value binomial distribution\nFor \\(X \\sim \\text{Binom}(n,p)\\) (read “\\(X\\) has a binomial distribution with samplesize \\(n\\) and success probability \\(p\\)”)\nThe expected value of \\(X\\) is by definition\n\\[E(X) = \\underbrace{\\sum_{k = 0}^n k}_{\\text{sum over successes}} \\cdot \\underbrace{\\binom{n}{k}p^k(1-p)^{n-k}}_{\\text{probability of successes}}\\]\nComputation shows that \\(E(X) = p\\cdot n\\).\nExample: For \\(n = 62\\) organ donations with complication probability \\(p=0.1\\), the expected number of complications is \\(E(X) = 6.2\\)."
  },
  {
    "objectID": "W12.html#expected-value-discrete-rv",
    "href": "W12.html#expected-value-discrete-rv",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Expected value discrete RV",
    "text": "Expected value discrete RV\nA discrete random variable takes only a finite (or at least discrete) set of values.\nFor \\(X: S \\to \\mathbb{R}\\), when \\(S\\) is finite, there is naturally only a set of values \\(x_1,\\dots,x_k\\in\\mathbb{R}\\) which \\(X\\) can be. We call their probabilities \\(p_1,\\dots,p_k\\) with \\(p_i = \\text{Pr}(X=x_i) = \\text{Pr}(\\{a \\in S \\text{ for which } X(a) = x_i \\})\\).\n(The probability of all other values in \\(\\mathbb{R}\\) is zero.).\nThe expected value of \\(X\\) is \\(E(X) = \\sum_{i=1}^k p_i x_i = p_1x_1 + \\dots + p_kx_k.\\)\nExamples: \\(X\\) is a die roll 🎲. \\(E(X) = 1\\cdot\\frac{1}{6} + 2\\cdot\\frac{1}{6} + 3\\cdot\\frac{1}{6} + 4\\cdot\\frac{1}{6} + 5\\cdot\\frac{1}{6} + 6\\cdot\\frac{1}{6} = \\frac{21}{6} = 3.5\\)\n\\(X\\) sum of two die rolls 🎲🎲.\n\\(E(X) = 2\\cdot\\frac{1}{36} + 3\\cdot\\frac{2}{36} + 4\\cdot\\frac{3}{36} + 5\\cdot\\frac{4}{36} + 6\\cdot\\frac{5}{36} + 7\\cdot\\frac{6}{36} + 8\\cdot\\frac{5}{36} +\\)\n\\(+ 9\\cdot\\frac{4}{36} + 10\\cdot\\frac{3}{36} + 11\\cdot\\frac{2}{36} + 12\\cdot\\frac{1}{36} = 7\\)"
  },
  {
    "objectID": "W12.html#binomial-distribution-1",
    "href": "W12.html#binomial-distribution-1",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe number of HEADS in several coin tosses and the number of complications in randomly selected organ donations are examples of random variable which have a binomial distribution.\n\nDefinition: The binomial distribution with parameters \\(n\\) and \\(p\\) is the number of successes in a sequence of \\(n\\) independent Bernoulli trials which each delivers a success with probability \\(p\\) and a failure with probability \\((1-p)\\).\n\nThe default model for the number of successes drawn from a sample of size \\(n\\) drawn from a population of size \\(N\\) with replacement.\nWhen \\(N\\) is much larger than \\(n\\) it is also a good approximation for drawing without replacement."
  },
  {
    "objectID": "W12.html#explanation-by-example",
    "href": "W12.html#explanation-by-example",
    "title": "W#12: Probability",
    "section": "Explanation by example",
    "text": "Explanation by example\n\nThe mass function (or density function, more on this later) dbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = dbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability for one number \\(\\text{Pr}(X = x)\\) or \\(f_X(x)\\).\n\nThe distribution function, or cumulative probability function pbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = pbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability that the random variable is less or equal for one number \\(\\text{Pr}(X \\leq x)\\)."
  },
  {
    "objectID": "W12.html#probability-mass-function-d",
    "href": "W12.html#probability-mass-function-d",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Probability mass function d",
    "text": "Probability mass function d\n\nThe mass function (or density function, more on this later) dbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = dbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability for the number \\(x\\): \\(\\text{Pr}(X = x)\\) or \\(f_X(x)\\)."
  },
  {
    "objectID": "W12.html#distribution-function-p",
    "href": "W12.html#distribution-function-p",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Distribution function p",
    "text": "Distribution function p\n\nThe distribution function, or cumulative probability function pbinom\n\n\n\nx <- 0:10\ntibble(x = x) |> \n mutate(pr = pbinom(x, size = 10, prob = 0.5)) |> \n ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nGives the probability that the random variable is less or equal to \\(x\\):\n\\(\\text{Pr}(X \\leq x)\\)."
  },
  {
    "objectID": "W12.html#quantile-function-q",
    "href": "W12.html#quantile-function-q",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Quantile function q",
    "text": "Quantile function q\n\nThe quantile function, qbinom with argument \\(p\\) representing the fraction of lowest values of \\(X\\) among all values for which we want the \\(x\\) value for.\n\n\n\nprobs <- seq(0, 1, by = 0.01)\ntibble(p = probs) |> \n mutate(x = qbinom(p, size = 10, prob = 0.5)) |> \n ggplot(aes(p, x)) + geom_line() + theme_minimal(base_size = 24)\n\n\n\n\n\n\nA point \\((p,x)\\) means: When we want a \\(p\\)-fraction of the probability mass, we need all events with values lower or equal to \\(x\\)."
  },
  {
    "objectID": "W12.html#calculus-relations",
    "href": "W12.html#calculus-relations",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Calculus relations",
    "text": "Calculus relations\n\nQuantile, distribution and mass function all carry the full information about the distribution of a random variable \\(X\\).\nThe mass function is the derivative of the distribution function.\n(The distribution function is the anti-derivative of the mass function.)\n\n\npbinom(0:5, size = 5, prob = 0.5) \n\n[1] 0.03125 0.18750 0.50000 0.81250 0.96875 1.00000\n\n# Next comes its derivative (have to append a 0 before first)\npbinom(0:5, size = 5, prob = 0.5) |> append(0, after = 0) |> diff()\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\ndbinom(0:5, size = 5, prob = 0.5)\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\n# Next comes its anti-derivative\ndbinom(0:5, size = 5, prob = 0.5) |> cumsum()\n\n[1] 0.03125 0.18750 0.50000 0.81250 0.96875 1.00000"
  },
  {
    "objectID": "W12.html#more-calculus-relations",
    "href": "W12.html#more-calculus-relations",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "More calculus relations",
    "text": "More calculus relations\n\nThe quantile function is the inverse of the distribution function.\nWe plot the inverse function by interchanging the x and y aesthetic.\n\n\n\nprobs <- seq(0, 1, by = 0.01)\nx <- 0:10\nq <- tibble(p = probs) |> mutate(x = qbinom(p, size = 10, prob = 0.5)) \np <- tibble(x = x) |> mutate(p = pbinom(x, size = 10, prob = 0.5)) \nq_plot <- q |> ggplot(aes(p, x)) + geom_line()\nqinv_plot <- q |> ggplot(aes(x, p)) + geom_line()\np_plot <- p |> ggplot(aes(x, p)) + geom_col()\npinv_plot <- p |> ggplot(aes(p, x)) + geom_col(orientation = \"y\")\nlibrary(patchwork)\n(q_plot | p_plot) / (pinv_plot | qinv_plot)"
  },
  {
    "objectID": "W12.html#random-number-generator-r",
    "href": "W12.html#random-number-generator-r",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Random number generator r",
    "text": "Random number generator r\n\nRandom binomial numbers are drawn with rbinom\n\n\n# 10 random binomial numbers for 62 trials with success probability 0.1\nrbinom(10, size = 62, prob = 0.1)\n\n [1]  7 10  4  9  6  8  5  6  8  9\n\n\n\nWe can reproduce the null distribution from hypothesis testing with 62 organ donations and 10% complication probability this way.\n\nWe produce 100,000 random consultants\nThen we compute the fraction of which have 3 or less complications\n\n\n\nset.seed(2022)\ns <- rbinom(100000, size = 62, prob = 0.1)\nsum(s<=3)/100000\n\n[1] 0.12038\n\n# Two other samples\nsum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000\n\n[1] 0.11918\n\nsum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000\n\n[1] 0.12034"
  },
  {
    "objectID": "W12.html#empircal-distributions",
    "href": "W12.html#empircal-distributions",
    "title": "W#12: Probability",
    "section": "Empircal distributions",
    "text": "Empircal distributions\n\n\n\n\n\\(X\\): select a random person from Europe (in 2018, willing to answer survey) and ask its attitude towards the European union from 0 to 10\n\n\n\n\nWhat is the distribution of the answer?\n\n\neu <- ess |> select(euftf) |> drop_na() |> \n count(euftf) |> mutate(prob = n/sum(n)) \neu\n\n# A tibble: 11 × 3\n   euftf     n   prob\n   <dbl> <int>  <dbl>\n 1     0  3361 0.0736\n 2     1  1787 0.0391\n 3     2  2830 0.0620\n 4     3  3586 0.0786\n 5     4  3739 0.0819\n 6     5 10286 0.225 \n 7     6  4589 0.101 \n 8     7  5165 0.113 \n 9     8  4692 0.103 \n10     9  1786 0.0391\n11    10  3826 0.0838\n\n\n\nMass function and distribution function\n\neu_mass <- eu |> ggplot(aes(euftf, prob)) + geom_col()\neu_distr <- eu |> mutate(cumprob = cumsum(prob)) |> \n ggplot(aes(euftf, cumprob)) + geom_col()\neu_mass | eu_distr"
  },
  {
    "objectID": "W12.html#what-could-be-next",
    "href": "W12.html#what-could-be-next",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "What could be next?",
    "text": "What could be next?\n\nContinuous distribution function\n\nTheoretical and empirical\n\nThe central limit theorem and why it is important empirically\nIndependence of probabilistic events\nConditional probability and the confusion matrix\nMarkov chains"
  },
  {
    "objectID": "W12.html#empirical-distributions",
    "href": "W12.html#empirical-distributions",
    "title": "W#12: Probability, Principal Component Analysis",
    "section": "Empirical distributions",
    "text": "Empirical distributions\n\n\n\n\n\\(X\\): select a random person from Europe (in 2018, willing to answer survey) and ask its attitude towards the European union from 0 to 10\n\n\n\n\nWhat is the distribution of the answer?\n\n\neu <- ess |> select(euftf) |> drop_na() |> \n count(euftf) |> mutate(prob = n/sum(n)) \neu\n\n# A tibble: 11 × 3\n   euftf     n   prob\n   <dbl> <int>  <dbl>\n 1     0  3361 0.0736\n 2     1  1787 0.0391\n 3     2  2830 0.0620\n 4     3  3586 0.0786\n 5     4  3739 0.0819\n 6     5 10286 0.225 \n 7     6  4589 0.101 \n 8     7  5165 0.113 \n 9     8  4692 0.103 \n10     9  1786 0.0391\n11    10  3826 0.0838\n\n\n\nMass function and distribution function\n\neu_mass <- eu |> ggplot(aes(euftf, prob)) + geom_col()\neu_distr <- eu |> mutate(cumprob = cumsum(prob)) |> \n ggplot(aes(euftf, cumprob)) + geom_col()\neu_mass | eu_distr"
  },
  {
    "objectID": "index.html#week-12-nov-17-probability-principal-component-analysis",
    "href": "index.html#week-12-nov-17-probability-principal-component-analysis",
    "title": "Data Science Concepts / Tools",
    "section": "Week 12, Nov 17: Probability, Principal Component Analysis",
    "text": "Week 12, Nov 17: Probability, Principal Component Analysis\nSlides Week 12\nHomework 05 due in 3 days"
  },
  {
    "objectID": "W13.html#continuous-random-variables",
    "href": "W13.html#continuous-random-variables",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nA random variable is a numerical function where values come with probabilities.\nFor every random variable we have\n\nA sample space of atomic events \\(S\\)\nA probability function \\(\\text{Pr}: \\mathcal{F}(S) \\to [0,1]\\)\n\nUp to now we have dealt with discrete random variables.\nIn some statistical model, we consider variables in a data frame as random variables, for example the response variable in a generalized linear model.\n\nFormally, a random variable is\n\na function \\(X: S \\to \\mathbb{R}\\)\nwhich assigns a value to each atomic event in the sample space.\n\nTogether with a probability function \\(\\text{Pr}: \\mathcal{F}(S)\\to [0,1]\\) probabilities can be assigned to values of the random variable (see the probability mass function in two slides).\n\nProbabilistic simulations. For example bootstrapping. Galton or Viertelfest: Quick Bootstrap examples following datascience box\nConditional probabilities and their relation to the confusion matrix. Quick difference between independence uncorrelated Conditional probability and relation to confusion matrix.\nContinuous random variables and some theoretical distributions. Normal and Lognormal, what do they mean.\nThe central limit theorem. Sum of random variables. Products of random variables.\nWhat is the difference between probability theory and statistics?\n\nMatrices:\n\nMatrix multiplication\nAx = b\nAx = lambda x\nSVD A = QDP\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W13.html#random-variables",
    "href": "W13.html#random-variables",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Random Variables",
    "text": "Random Variables\n\nFor every random variable we have\n\nA sample space of atomic events \\(S\\)\nA probability function \\(\\text{Pr}: \\mathcal{F}(S) \\to [0,1]\\)\n\nThe random variable maps atomic events to numbers \\(X(s) \\to \\mathbb{R}\\).\nThrough the random variable we can assign probabilities to such numbers.\n\nDiscrete random variables can only take a countable (usually finite) numbers of values.\nContinuous random variables can take an uncountable number of values."
  },
  {
    "objectID": "W13.html#theoretical-examples",
    "href": "W13.html#theoretical-examples",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Theoretical examples",
    "text": "Theoretical examples\n\n\nDiscrete random variable\nRandom atomic event: 20 (unfair) coin flips with HEADS probability 40%.\nRandom Variable: Number of HEADS.\nBinomial distribution function\n\nggplot() + \n geom_function(fun = pbinom, args = list(size = 20, prob = 0.4)) + \n xlim(c(0,20)) + theme_minimal(base_size = 24)\n\n\n\n\n\nContinuous random variable\nRandom atomic event: Point on a ruler of 1 meter length. Each point is equally likely. Random Variable: The marking on the ruler in meters (number from 0 to 1).\nUniform distribution function\n\nggplot() + \n geom_function(fun = punif) + \n xlim(c(-0.5,1.5)) + theme_minimal(base_size = 24)\n\n\n\n\n\n\nInterpret a point of these graphs."
  },
  {
    "objectID": "W13.html#next",
    "href": "W13.html#next",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Next",
    "text": "Next\n\nIn some statistical model, we consider variables in a data frame as random variables, for example the response variable in a generalized linear model.\n\nFormally, a random variable is\n\na function \\(X: S \\to \\mathbb{R}\\)\nwhich assigns a value to each atomic event in the sample space.\n\nTogether with a probability function \\(\\text{Pr}: \\mathcal{F}(S)\\to [0,1]\\) probabilities can be assigned to values of the random variable (see the probability mass function in two slides).\n\nProbabilistic simulations. For example bootstrapping. Galton or Viertelfest: Quick Bootstrap examples following datascience box\nConditional probabilities and their relation to the confusion matrix. Quick difference between independence uncorrelated Conditional probability and relation to confusion matrix.\nContinuous random variables and some theoretical distributions. Normal and Lognormal, what do they mean.\nThe central limit theorem. Sum of random variables. Products of random variables.\nWhat is the difference between probability theory and statistics?\n\nMatrices:\n\nMatrix multiplication\nAx = b\nAx = lambda x\nSVD A = QDP\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W13.html#distribution-function",
    "href": "W13.html#distribution-function",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Distribution function",
    "text": "Distribution function\nThe distribution function of a random variable \\(X\\) is\n\\[F_X(x) = \\text{Pr}(X \\leq x)\\]\nwhich reads \\(F_X(x)\\) is the probability of the event that the random variable \\(X\\) has a value less or equal to \\(x\\).\nRandom variables are characterized by their distribution function!\nLet’s explore it for discrete and continuous random variables with theoretical and empirical examples."
  },
  {
    "objectID": "W13.html#empirical-examples",
    "href": "W13.html#empirical-examples",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Empirical examples",
    "text": "Empirical examples\n\n\nDiscrete random variable\nRandom atomic event: Ask a European about attitude towards the EU.\nRandom Variable: The answer on the scale 0 to 10.\n\ness |> count(euftf) |> drop_na() |> mutate(freq = n/sum(n)) |>\n add_row(euftf = -4, freq = 0, .before = TRUE) |> add_row(euftf = 14, freq = 0) |> \n ggplot(aes(euftf,cumsum(freq))) + geom_step() + \n scale_x_continuous(breaks = 0:10) + theme_minimal(base_size = 24)\n\n\n\n\n\nContinuous random variable\nRandom atomic event: A visitors estimates the weight of the meat of an ox.\nRandom Variable: The estimated value converted to pounds.\n\ngalton |> mutate(freq = 1/n()) |>  \n add_row(Estimate = 800, freq = 0, .before = TRUE) |> add_row(Estimate = 1600, freq = 0) |>\n ggplot(aes(Estimate, cumsum(freq))) + geom_step() +\n theme_minimal(base_size = 24)\n\n\n\n\n\n\nInterpret a point of these graphs.\nNote: Technically, a finite empirical sample still has a discrete distribution, but …"
  },
  {
    "objectID": "W13.html#three-common-problem-types",
    "href": "W13.html#three-common-problem-types",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Three common problem types",
    "text": "Three common problem types\n\nRegression: We want to explain/predict a numerical variable.\nExample: In a linear regression, we model a numerical response variable as a linear combination of predictors and estimate the coefficients with data.\nClassification: We want to explain/predict a nominal (often binary) variable.\nExample: In a logistic regression, we assume the binary outcomes are realized randomly based on a probability which is a the logistic transformation of a linear combination of predictors and estimate the coefficients with data.\nClustering: We want label cases in data.\n\nClassification and clustering are about producing new nominal data. In classification the categories are already know from the training data. Clustering algorithms produces the categories without training data."
  },
  {
    "objectID": "W13.html#k-means",
    "href": "W13.html#k-means",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "k-Means",
    "text": "k-Means"
  },
  {
    "objectID": "W13.html#hierarchical-clustering",
    "href": "W13.html#hierarchical-clustering",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering"
  },
  {
    "objectID": "W13.html#statistical-learning-perspective",
    "href": "W13.html#statistical-learning-perspective",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Statistical learning perspective",
    "text": "Statistical learning perspective\n\nRegression and classification problems are solved by supervised algorithms.\n\nThat means they receive training data which includes the outcome variable (which is often made by humans) and should perform on new test data.\nPrediction questions can be:\n\nWhat is a house worth?\nWhich emails are spam?\n\n\nA clustering problems is a problem of unsupervised learning.\n\nThe algorithm shall generate labels for the cases as a new variable.\n\nPrediction questions can be:\n\nWhat different types of customers exist and how can each one be labelled?\nWhich players in a sport league are similar and how can we label them?"
  },
  {
    "objectID": "W13.html#explanatory-perspective",
    "href": "W13.html#explanatory-perspective",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Explanatory perspective",
    "text": "Explanatory perspective\n\nIn regression and classification problems we develop a model explicitly.\n\nWe can take a confirmatory perspective: We specify an equation in which we select and transform variables.\nWe can derive the equation from theory without data an then assess the quality of its explanatory and predictive capacity with data.\nPart of this can by hypothesis tests like: We hypothesize that a certain predictor variable has a (positive or negative) effect on the outcome variable.\n\nIn clustering problems we take a exploratory perspective.\n\nWe try to uncover structure in an existing data set."
  },
  {
    "objectID": "W13.html#section",
    "href": "W13.html#section",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "",
    "text": "In that sense, clustering problems are similar to principle component analysis (PCA). Both extract information from existing rectangular data without specifying predictors and response."
  },
  {
    "objectID": "W13.html#cluster-analysis-and-pca",
    "href": "W13.html#cluster-analysis-and-pca",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Cluster analysis and PCA",
    "text": "Cluster analysis and PCA\nSolving a clustering problem is also called cluster analysis.\n\nCluster analysis and principle component analysis (PCA) are both extract information from existing rectangular data without specifying predictors and response.\n\nPCA extracts relations based on linear correlations between numerical variables (columns). These principle components can be used to create new numerical variables (and thereby reduce the number of variables).\nCluster analysis tries to group cases (rows) into clusters such that cases in a cluster a similar.\n\nWhen clusters are found they are labelled and a new nominal variable in the data set is created. Each cases receives a nominal label to which cluster it belongs."
  },
  {
    "objectID": "W13.html#two-clustering-methods",
    "href": "W13.html#two-clustering-methods",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Two clustering methods",
    "text": "Two clustering methods\nToday, we do a quick tour through two methods focusing on\n\nthe general idea, and\nhow to apply, interpret, and look at the results.\n\nThe two paradigms are\n\ncentroid based clustering\nconnectivity based hierarchical clustering"
  },
  {
    "objectID": "W13.html#centroid-based-k-means-clustering",
    "href": "W13.html#centroid-based-k-means-clustering",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Centroid based: k-means clustering",
    "text": "Centroid based: k-means clustering\nk-means clustering aims to partition \\(n\\) observations into \\(k\\) clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid) serving as a prototype of the cluster.\n\nImportant\n\nWe must specify how many (\\(k\\)) clusters we want to have.\nWe need a measure of distance between cases. How far is one case from each other?\nA cluster is characterized by its centroid."
  },
  {
    "objectID": "W13.html#measuring-distance-between-cases",
    "href": "W13.html#measuring-distance-between-cases",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Measuring distance between cases",
    "text": "Measuring distance between cases\nAssume we have \\(m\\) numerical variables.\n\\(\\to\\) Every case is a point in \\(m\\)-dimensional space.\nIn the following we will use the Euclidean distance in \\(m\\) dimensions:\nFor two points \\((x_1, x_2, \\dots, x_m)\\) and \\((y_1, y_2, \\dots, y_m)\\) it is\n\\[\\sqrt{(x_1-y_1)^2 +(x_2-y_2)^2 + \\dots + (x_m-y_m)^2}\\]\n\nIn \\(m=2\\) or \\(3\\) dimensions it is what we would measure with a ruler. ]\nNote, the points represents rows in a dataset.\n\nThere are many more useful distance measures! It is a field to constantly learn about. Two visual examples: Manhattan distance, French railway metric"
  },
  {
    "objectID": "W13.html#algorithm-start",
    "href": "W13.html#algorithm-start",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Algorithm: Start",
    "text": "Algorithm: Start\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#k-means-algorithm-start-for-k-3-clusters",
    "href": "W13.html#k-means-algorithm-start-for-k-3-clusters",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "k-means algorithm: Start for \\(k = 3\\) clusters",
    "text": "k-means algorithm: Start for \\(k = 3\\) clusters\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#algorithm-iteration-step-1",
    "href": "W13.html#algorithm-iteration-step-1",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Algorithm: Iteration step 1",
    "text": "Algorithm: Iteration step 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#k-means-algorithm-start",
    "href": "W13.html#k-means-algorithm-start",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "k-means algorithm: Start",
    "text": "k-means algorithm: Start\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#k-means-algorithm-iteration-step-1",
    "href": "W13.html#k-means-algorithm-iteration-step-1",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "k-means algorithm: Iteration step 1",
    "text": "k-means algorithm: Iteration step 1\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#k-means-algorithm-iteration-step-2",
    "href": "W13.html#k-means-algorithm-iteration-step-2",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "k-means algorithm: Iteration step 2",
    "text": "k-means algorithm: Iteration step 2\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#why-we-need-iteration",
    "href": "W13.html#why-we-need-iteration",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Why we need iteration:",
    "text": "Why we need iteration:\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#why-we-need-iteration-1",
    "href": "W13.html#why-we-need-iteration-1",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Why we need iteration:",
    "text": "Why we need iteration:\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#iterate-steps-1-and-2",
    "href": "W13.html#iterate-steps-1-and-2",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Iterate steps 1 and 2",
    "text": "Iterate steps 1 and 2\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#k-means-algorithm-stop",
    "href": "W13.html#k-means-algorithm-stop",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "k-means algorithm: Stop",
    "text": "k-means algorithm: Stop\n\n\n\nFrom https://allisonhorst.com/k-means-clustering"
  },
  {
    "objectID": "W13.html#calculate-k-means-clustering-in-r",
    "href": "W13.html#calculate-k-means-clustering-in-r",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Calculate k-means clustering in R",
    "text": "Calculate k-means clustering in R\nThe OWiD corona country indicator dataset from Week 12:\n\nowid_inds\n\n# A tibble: 158 × 14\n   iso_code continent    locat…¹ popul…² popul…³ media…⁴ aged_…⁵ aged_…⁶ gdp_p…⁷\n   <chr>    <chr>        <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 AFG      Asia         Afghan…  4.01e7   54.4     18.6    2.58    1.34   1804.\n 2 ALB      Europe       Albania  2.85e6  105.      38     13.2     8.64  11803.\n 3 DZA      Africa       Algeria  4.42e7   17.3     29.1    6.21    3.86  13914.\n 4 ATG      North Ameri… Antigu…  9.32e4  232.      32.1    6.93    4.63  21491.\n 5 ARG      South Ameri… Argent…  4.53e7   16.2     31.9   11.2     7.44  18934.\n 6 ARM      Asia         Armenia  2.79e6  103.      35.7   11.2     7.57   8788.\n 7 AUS      Oceania      Austra…  2.59e7    3.20    37.9   15.5    10.1   44649.\n 8 AUT      Europe       Austria  8.92e6  107.      44.4   19.2    13.7   45437.\n 9 AZE      Asia         Azerba…  1.03e7  119.      32.4    6.02    3.87  15847.\n10 BHS      North Ameri… Bahamas  4.08e5   39.5     34.3    9.00    5.2   27718.\n# … with 148 more rows, 5 more variables: cardiovasc_death_rate <dbl>,\n#   diabetes_prevalence <dbl>, hospital_beds_per_thousand <dbl>,\n#   life_expectancy <dbl>, human_development_index <dbl>, and abbreviated\n#   variable names ¹​location, ²​population, ³​population_density, ⁴​median_age,\n#   ⁵​aged_65_older, ⁶​aged_70_older, ⁷​gdp_per_capita\n\nglimpse(owid_inds)\n\nRows: 158\nColumns: 14\n$ iso_code                   <chr> \"AFG\", \"ALB\", \"DZA\", \"ATG\", \"ARG\", \"ARM\", \"…\n$ continent                  <chr> \"Asia\", \"Europe\", \"Africa\", \"North America\"…\n$ location                   <chr> \"Afghanistan\", \"Albania\", \"Algeria\", \"Antig…\n$ population                 <dbl> 40099462, 2854710, 44177969, 93220, 4527678…\n$ population_density         <dbl> 54.422, 104.871, 17.348, 231.845, 16.177, 1…\n$ median_age                 <dbl> 18.6, 38.0, 29.1, 32.1, 31.9, 35.7, 37.9, 4…\n$ aged_65_older              <dbl> 2.581, 13.188, 6.211, 6.933, 11.198, 11.232…\n$ aged_70_older              <dbl> 1.337, 8.643, 3.857, 4.631, 7.441, 7.571, 1…\n$ gdp_per_capita             <dbl> 1803.987, 11803.431, 13913.839, 21490.943, …\n$ cardiovasc_death_rate      <dbl> 597.029, 304.195, 278.364, 191.511, 191.032…\n$ diabetes_prevalence        <dbl> 9.59, 10.08, 6.73, 13.17, 5.50, 7.11, 5.07,…\n$ hospital_beds_per_thousand <dbl> 0.500, 2.890, 1.900, 3.800, 5.000, 4.200, 3…\n$ life_expectancy            <dbl> 64.83, 78.57, 76.88, 77.02, 76.67, 75.09, 8…\n$ human_development_index    <dbl> 0.511, 0.795, 0.748, 0.778, 0.845, 0.776, 0…"
  },
  {
    "objectID": "W13.html#preprocessing-steps",
    "href": "W13.html#preprocessing-steps",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Preprocessing Steps",
    "text": "Preprocessing Steps\n\nAutomatic Box-Cox-transformation\nStandardization (shift-scale transformzation by mean and standard deviation)\n\n\nlibrary(tidymodels)\nrec_BoxCox <- owid_inds |> recipe() |> \n step_rm(all_nominal()) |> # Transform only the nominal\n step_BoxCox(all_numeric()) |> # Box-Cox transformation, parameter estimated\n step_scale(all_numeric()) |> step_center(all_numeric()) # standardization\n# Call prep to calculate tranformed variables\nowid_BoxCox <- rec_BoxCox |> prep(owid_inds)\nowid_BoxCox\n\nRecipe\n\nInputs:\n\n  14 variables (no declared roles)\n\nTraining data contained 158 data points and no missing data.\n\nOperations:\n\nVariables removed iso_code, continent, location [trained]\nBox-Cox transformation on population, population_density, median_age... [trained]\nScaling for population, population_density, median_age, age... [trained]\nCentering for population, population_density, median_age, age... [trained]"
  },
  {
    "objectID": "W13.html#whats-in-the-preped-object",
    "href": "W13.html#whats-in-the-preped-object",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Whats in the preped object?",
    "text": "Whats in the preped object?\nA lot!\n\nglimpse(owid_BoxCox)\n\nList of 9\n $ var_info      : tibble [14 × 4] (S3: tbl_df/tbl/data.frame)\n  ..$ variable: chr [1:14] \"iso_code\" \"continent\" \"location\" \"population\" ...\n  ..$ type    : chr [1:14] \"nominal\" \"nominal\" \"nominal\" \"numeric\" ...\n  ..$ role    : chr [1:14] NA NA NA NA ...\n  ..$ source  : chr [1:14] \"original\" \"original\" \"original\" \"original\" ...\n $ term_info     : tibble [11 × 4] (S3: tbl_df/tbl/data.frame)\n  ..$ variable: chr [1:11] \"population\" \"population_density\" \"median_age\" \"aged_65_older\" ...\n  ..$ type    : chr [1:11] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  ..$ role    : chr [1:11] NA NA NA NA ...\n  ..$ source  : chr [1:11] \"original\" \"original\" \"original\" \"original\" ...\n $ steps         :List of 4\n  ..$ :List of 6\n  .. ..$ terms   :List of 1\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  .. ..$ role    : logi NA\n  .. ..$ trained : logi TRUE\n  .. ..$ removals: Named chr [1:3] \"iso_code\" \"continent\" \"location\"\n  .. .. ..- attr(*, \"names\")= chr [1:3] \"iso_code\" \"continent\" \"location\"\n  .. ..$ skip    : logi FALSE\n  .. ..$ id      : chr \"rm_Bt9xW\"\n  .. ..- attr(*, \"class\")= chr [1:2] \"step_rm\" \"step\"\n  ..$ :List of 8\n  .. ..$ terms     :List of 1\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  .. ..$ role      : logi NA\n  .. ..$ trained   : logi TRUE\n  .. ..$ lambdas   : Named num [1:11] 0.0395 0.03 0.6877 0.0318 0.0407 ...\n  .. .. ..- attr(*, \"names\")= chr [1:11] \"population\" \"population_density\" \"median_age\" \"aged_65_older\" ...\n  .. ..$ limits    : num [1:2] -5 5\n  .. ..$ num_unique: num 5\n  .. ..$ skip      : logi FALSE\n  .. ..$ id        : chr \"BoxCox_UdJK4\"\n  .. ..- attr(*, \"class\")= chr [1:2] \"step_BoxCox\" \"step\"\n  ..$ :List of 9\n  .. ..$ terms       :List of 1\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  .. ..$ role        : logi NA\n  .. ..$ trained     : logi TRUE\n  .. ..$ sds         : Named num [1:11] 3.607 1.527 3.045 0.777 0.833 ...\n  .. .. ..- attr(*, \"names\")= chr [1:11] \"population\" \"population_density\" \"median_age\" \"aged_65_older\" ...\n  .. ..$ factor      : num 1\n  .. ..$ na_rm       : logi TRUE\n  .. ..$ skip        : logi FALSE\n  .. ..$ id          : chr \"scale_6C4oB\"\n  .. ..$ case_weights: NULL\n  .. ..- attr(*, \"class\")= chr [1:2] \"step_scale\" \"step\"\n  ..$ :List of 8\n  .. ..$ terms       :List of 1\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  .. ..$ role        : logi NA\n  .. ..$ trained     : logi TRUE\n  .. ..$ means       : Named num [1:11] 6.21 3.05 4.56 2.61 1.84 ...\n  .. .. ..- attr(*, \"names\")= chr [1:11] \"population\" \"population_density\" \"median_age\" \"aged_65_older\" ...\n  .. ..$ na_rm       : logi TRUE\n  .. ..$ skip        : logi FALSE\n  .. ..$ id          : chr \"center_PWIXy\"\n  .. ..$ case_weights: NULL\n  .. ..- attr(*, \"class\")= chr [1:2] \"step_center\" \"step\"\n $ template      : tibble [158 × 11] (S3: tbl_df/tbl/data.frame)\n  ..$ population                : num [1:158] 0.789 -0.601 0.842 -2.197 0.856 ...\n  ..$ population_density        : num [1:158] -0.27 0.219 -1.1 0.823 -1.15 ...\n  ..$ median_age                : num [1:158] -1.473 0.789 -0.188 0.15 0.128 ...\n  ..$ aged_65_older             : num [1:158] -1.3726 0.8492 -0.1908 -0.0405 0.6212 ...\n  ..$ aged_70_older             : num [1:158] -1.487 0.868 -0.172 0.061 0.673 ...\n  ..$ gdp_per_capita            : num [1:158] -1.449 -0.0657 0.0793 0.4839 0.3627 ...\n  ..$ cardiovasc_death_rate     : num [1:158] 2.123 0.571 0.377 -0.414 -0.419 ...\n  ..$ diabetes_prevalence       : num [1:158] 0.532 0.638 -0.174 1.231 -0.545 ...\n  ..$ hospital_beds_per_thousand: num [1:158] -1.554 0.293 -0.214 0.648 1.027 ...\n  ..$ life_expectancy           : num [1:158] -1.279 0.725 0.426 0.45 0.39 ...\n  ..$ human_development_index   : num [1:158] -1.4833 0.3204 -0.0306 0.191 0.7165 ...\n $ retained      : logi TRUE\n $ requirements  :List of 1\n  ..$ bake: Named logi(0) \n  .. ..- attr(*, \"names\")= chr(0) \n $ tr_info       :'data.frame': 1 obs. of  2 variables:\n  ..$ nrows    : int 158\n  ..$ ncomplete: int 158\n $ orig_lvls     :List of 14\n  ..$ iso_code                  :List of 3\n  .. ..$ values : chr [1:158] \"AFG\" \"ALB\" \"ARE\" \"ARG\" ...\n  .. ..$ ordered: logi FALSE\n  .. ..$ factor : logi FALSE\n  ..$ continent                 :List of 3\n  .. ..$ values : chr [1:6] \"Africa\" \"Asia\" \"Europe\" \"North America\" ...\n  .. ..$ ordered: logi FALSE\n  .. ..$ factor : logi FALSE\n  ..$ location                  :List of 3\n  .. ..$ values : chr [1:158] \"Afghanistan\" \"Albania\" \"Algeria\" \"Antigua and Barbuda\" ...\n  .. ..$ ordered: logi FALSE\n  .. ..$ factor : logi FALSE\n  ..$ population                :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ population_density        :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ median_age                :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ aged_65_older             :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ aged_70_older             :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ gdp_per_capita            :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ cardiovasc_death_rate     :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ diabetes_prevalence       :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ hospital_beds_per_thousand:List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ life_expectancy           :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n  ..$ human_development_index   :List of 2\n  .. ..$ values : logi NA\n  .. ..$ ordered: logi NA\n $ last_term_info: gropd_df [14 × 6] (S3: grouped_df/tbl_df/tbl/data.frame)\n  ..$ variable: chr [1:14] \"aged_65_older\" \"aged_70_older\" \"cardiovasc_death_rate\" \"continent\" ...\n  ..$ type    : chr [1:14] \"numeric\" \"numeric\" \"numeric\" \"nominal\" ...\n  ..$ role    :List of 14\n  ..$ source  : chr [1:14] \"original\" \"original\" \"original\" \"original\" ...\n  ..$ number  : num [1:14] 4 4 4 0 4 4 4 4 0 4 ...\n  ..$ skip    : logi [1:14] FALSE FALSE FALSE FALSE FALSE FALSE ...\n  ..- attr(*, \"groups\")= tibble [14 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..- attr(*, \".drop\")= logi TRUE\n - attr(*, \"class\")= chr \"recipe\""
  },
  {
    "objectID": "W13.html#preprocessing-in-a-list-in-steps",
    "href": "W13.html#preprocessing-in-a-list-in-steps",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Preprocessing in a list in $steps",
    "text": "Preprocessing in a list in $steps\n\nglimpse(owid_BoxCox$steps)\n\nList of 4\n $ :List of 6\n  ..$ terms   :List of 1\n  .. ..$ : language ~all_nominal()\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  ..$ role    : logi NA\n  ..$ trained : logi TRUE\n  ..$ removals: Named chr [1:3] \"iso_code\" \"continent\" \"location\"\n  .. ..- attr(*, \"names\")= chr [1:3] \"iso_code\" \"continent\" \"location\"\n  ..$ skip    : logi FALSE\n  ..$ id      : chr \"rm_Bt9xW\"\n  ..- attr(*, \"class\")= chr [1:2] \"step_rm\" \"step\"\n $ :List of 8\n  ..$ terms     :List of 1\n  .. ..$ : language ~all_numeric()\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  ..$ role      : logi NA\n  ..$ trained   : logi TRUE\n  ..$ lambdas   : Named num [1:11] 0.0395 0.03 0.6877 0.0318 0.0407 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"population\" \"population_density\" \"median_age\" \"aged_65_older\" ...\n  ..$ limits    : num [1:2] -5 5\n  ..$ num_unique: num 5\n  ..$ skip      : logi FALSE\n  ..$ id        : chr \"BoxCox_UdJK4\"\n  ..- attr(*, \"class\")= chr [1:2] \"step_BoxCox\" \"step\"\n $ :List of 9\n  ..$ terms       :List of 1\n  .. ..$ : language ~all_numeric()\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  ..$ role        : logi NA\n  ..$ trained     : logi TRUE\n  ..$ sds         : Named num [1:11] 3.607 1.527 3.045 0.777 0.833 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"population\" \"population_density\" \"median_age\" \"aged_65_older\" ...\n  ..$ factor      : num 1\n  ..$ na_rm       : logi TRUE\n  ..$ skip        : logi FALSE\n  ..$ id          : chr \"scale_6C4oB\"\n  ..$ case_weights: NULL\n  ..- attr(*, \"class\")= chr [1:2] \"step_scale\" \"step\"\n $ :List of 8\n  ..$ terms       :List of 1\n  .. ..$ : language ~all_numeric()\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n  ..$ role        : logi NA\n  ..$ trained     : logi TRUE\n  ..$ means       : Named num [1:11] 6.21 3.05 4.56 2.61 1.84 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"population\" \"population_density\" \"median_age\" \"aged_65_older\" ...\n  ..$ na_rm       : logi TRUE\n  ..$ skip        : logi FALSE\n  ..$ id          : chr \"center_PWIXy\"\n  ..$ case_weights: NULL\n  ..- attr(*, \"class\")= chr [1:2] \"step_center\" \"step\""
  },
  {
    "objectID": "W13.html#parameters-of-box-cox-transformations",
    "href": "W13.html#parameters-of-box-cox-transformations",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Parameters of Box-Cox transformations",
    "text": "Parameters of Box-Cox transformations\n\n\n\n\n                population         population_density \n                0.03951472                 0.02998966 \n                median_age              aged_65_older \n                0.68771588                 0.03184961 \n             aged_70_older             gdp_per_capita \n                0.04074480                 0.18070045 \n     cardiovasc_death_rate        diabetes_prevalence \n                0.13720781                 0.29224237 \nhospital_beds_per_thousand            life_expectancy \n                0.21346927                 3.43554759 \n   human_development_index \n                1.96999781"
  },
  {
    "objectID": "W13.html#parameters-of-box-cox",
    "href": "W13.html#parameters-of-box-cox",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Parameters of Box-Cox",
    "text": "Parameters of Box-Cox\n\n\n\n\n                population         population_density \n                0.03951472                 0.02998966 \n                median_age              aged_65_older \n                0.68771588                 0.03184961 \n             aged_70_older             gdp_per_capita \n                0.04074480                 0.18070045 \n     cardiovasc_death_rate        diabetes_prevalence \n                0.13720781                 0.29224237 \nhospital_beds_per_thousand            life_expectancy \n                0.21346927                 3.43554759 \n   human_development_index \n                1.96999781 \n\n\nLeft-skew \\(\\lambda > 1\\), right-skew \\(\\lambda \\to 0\\)"
  },
  {
    "objectID": "W13.html#transformed-data-in-template",
    "href": "W13.html#transformed-data-in-template",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Transformed data in $template",
    "text": "Transformed data in $template\nTransformed data looks more “normal”.\n\nowid_BoxCoxData <- owid_BoxCox$template\n\nowid_BoxCoxData |> select(population_density, life_expectancy) |> \n pivot_longer(c(population_density, life_expectancy)) |> \n ggplot(aes(value)) + geom_histogram(bins = 50) + facet_wrap(~name, scales = \"free\")"
  },
  {
    "objectID": "W13.html#for-k3-clusters",
    "href": "W13.html#for-k3-clusters",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "For \\(k=3\\) clusters",
    "text": "For \\(k=3\\) clusters\n\nkmean from base-R\nOutput: Cluster means are the centroids, Cluster vector are the labels for the cases, Within cluster sum of squares by cluster are performance measures\nWarning: In principle, k-means can results in different clusters for different initial starting positions of clusters.\n\n\nowid_BoxCoxData |> kmeans(centers = 3)\n\nK-means clustering with 3 clusters of sizes 49, 63, 46\n\nCluster means:\n  population population_density  median_age aged_65_older aged_70_older\n1  0.1327702         0.07992665  1.13903095     1.1928251     1.1874508\n2 -0.3498207        -0.12751566 -0.04154558    -0.2086192    -0.2145598\n3  0.3376732         0.08950175 -1.15641620    -0.9849005    -0.9710397\n  gdp_per_capita cardiovasc_death_rate diabetes_prevalence\n1      0.9087787            -0.6700604          -0.2457741\n2      0.1057045             0.1737642           0.7104584\n3     -1.1128161             0.4757786          -0.7112163\n  hospital_beds_per_thousand life_expectancy human_development_index\n1               8.936399e-01      1.03474178              1.08733894\n2               8.400952e-06      0.04568398              0.04325333\n3              -9.519323e-01     -1.16479213             -1.21749060\n\nClustering vector:\n  [1] 3 2 2 2 1 2 1 1 2 2 2 3 1 1 1 2 3 2 2 1 2 2 2 1 3 3 3 3 1 3 3 1 1 2 3 2 1\n [38] 1 1 1 3 2 2 2 2 3 3 1 3 3 2 1 1 2 3 2 1 3 1 2 2 3 2 3 3 1 1 3 3 2 3 1 1 1\n [75] 2 1 2 2 3 2 2 2 3 1 2 3 2 1 1 3 3 2 3 1 2 2 2 2 2 2 3 3 3 1 1 2 3 2 1 2 3\n[112] 2 2 2 3 1 1 2 1 1 2 2 3 2 2 1 1 1 2 3 1 1 2 3 2 1 1 3 3 1 3 3 2 2 2 2 3 1\n[149] 2 1 1 1 2 2 2 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 247.6110 384.3323 209.3520\n (between_SS / total_SS =  51.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "W13.html#h",
    "href": "W13.html#h",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "H",
    "text": "H"
  },
  {
    "objectID": "W13.html#how-many-clusters",
    "href": "W13.html#how-many-clusters",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "How many clusters?",
    "text": "How many clusters?\n\nComputer solutions for different numbers of cluster \\(k\\)\n\n\nkclusts <- tibble(k = 1:9) |>\n  mutate(\n    kclust = map(k, ~kmeans(owid_BoxCoxData, .x)), # kmeans for k=1:9\n    tidied = map(kclust, tidy), # tidy extracts the centroids here \n    glanced = map(kclust, glance), # \n    augmented = map(kclust, augment, owid_BoxCoxData)\n  ) # This is large data frame with nested objects as entries. We unnest next\n\nclusters <- kclusts |> unnest(cols = c(tidied))\nclusters\n\n# A tibble: 45 × 18\n       k kclust   population populatio…¹ median_…² aged_65…³ aged_70…⁴ gdp_per…⁵\n   <int> <list>        <dbl>       <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1     1 <kmeans>  -1.29e-16   -5.34e-17  4.41e-16  5.06e-17  3.79e-17 -1.41e-17\n 2     2 <kmeans>  -9.04e- 2    1.57e- 1  8.02e- 1  7.78e- 1  7.65e- 1  6.77e- 1\n 3     2 <kmeans>   9.51e- 2   -1.65e- 1 -8.43e- 1 -8.18e- 1 -8.04e- 1 -7.12e- 1\n 4     3 <kmeans>  -2.87e- 1   -1.07e- 1 -1.44e- 1 -2.92e- 1 -2.92e- 1  5.57e- 2\n 5     3 <kmeans>   1.03e- 1    7.05e- 2  1.13e+ 0  1.19e+ 0  1.18e+ 0  8.95e- 1\n 6     3 <kmeans>   3.93e- 1    1.04e- 1 -1.23e+ 0 -1.03e+ 0 -1.02e+ 0 -1.28e+ 0\n 7     4 <kmeans>   3.93e- 1    1.04e- 1 -1.23e+ 0 -1.03e+ 0 -1.02e+ 0 -1.28e+ 0\n 8     4 <kmeans>  -1.02e- 1   -1.68e- 1  9.93e- 1  1.03e+ 0  1.03e+ 0  3.19e- 1\n 9     4 <kmeans>   1.65e- 1    2.14e- 1  1.15e+ 0  1.22e+ 0  1.21e+ 0  1.16e+ 0\n10     4 <kmeans>  -2.73e- 1   -9.98e- 2 -2.17e- 1 -3.80e- 1 -3.83e- 1  7.21e- 2\n# … with 35 more rows, 10 more variables: cardiovasc_death_rate <dbl>,\n#   diabetes_prevalence <dbl>, hospital_beds_per_thousand <dbl>,\n#   life_expectancy <dbl>, human_development_index <dbl>, size <int>,\n#   withinss <dbl>, cluster <fct>, glanced <list>, augmented <list>, and\n#   abbreviated variable names ¹​population_density, ²​median_age,\n#   ³​aged_65_older, ⁴​aged_70_older, ⁵​gdp_per_capita\n\nclusterings <- kclusts |> unnest(cols = c(glanced))\nclusterings\n\n# A tibble: 9 × 8\n      k kclust   tidied            totss tot.withinss betweenss  iter augmented\n  <int> <list>   <list>            <dbl>        <dbl>     <dbl> <int> <list>   \n1     1 <kmeans> <tibble [1 × 14]>  1727        1727.  5.46e-12     1 <tibble> \n2     2 <kmeans> <tibble [2 × 14]>  1727        1056.  6.71e+ 2     1 <tibble> \n3     3 <kmeans> <tibble [3 × 14]>  1727         843.  8.84e+ 2     2 <tibble> \n4     4 <kmeans> <tibble [4 × 14]>  1727         752.  9.75e+ 2     3 <tibble> \n5     5 <kmeans> <tibble [5 × 14]>  1727         678.  1.05e+ 3     3 <tibble> \n6     6 <kmeans> <tibble [6 × 14]>  1727         632.  1.10e+ 3     3 <tibble> \n7     7 <kmeans> <tibble [7 × 14]>  1727         568.  1.16e+ 3     3 <tibble> \n8     8 <kmeans> <tibble [8 × 14]>  1727         530.  1.20e+ 3     3 <tibble> \n9     9 <kmeans> <tibble [9 × 14]>  1727         507.  1.22e+ 3     4 <tibble> \n\nassignments <- kclusts |> unnest(cols = c(augmented))\nassignments\n\n# A tibble: 1,422 × 16\n       k kclust   tidied   glanced  population populat…¹ media…² aged_…³ aged_…⁴\n   <int> <list>   <list>   <list>        <dbl>     <dbl>   <dbl>   <dbl>   <dbl>\n 1     1 <kmeans> <tibble> <tibble>     0.789     -0.270  -1.47  -1.37   -1.49  \n 2     1 <kmeans> <tibble> <tibble>    -0.601      0.219   0.789  0.849   0.868 \n 3     1 <kmeans> <tibble> <tibble>     0.842     -1.10   -0.188 -0.191  -0.172 \n 4     1 <kmeans> <tibble> <tibble>    -2.20       0.823   0.150 -0.0405  0.0610\n 5     1 <kmeans> <tibble> <tibble>     0.856     -1.15    0.128  0.621   0.673 \n 6     1 <kmeans> <tibble> <tibble>    -0.612      0.205   0.544  0.625   0.695 \n 7     1 <kmeans> <tibble> <tibble>     0.549     -2.28    0.778  1.08    1.08  \n 8     1 <kmeans> <tibble> <tibble>    -0.0193     0.232   1.45   1.38    1.48  \n 9     1 <kmeans> <tibble> <tibble>     0.0565     0.316   0.184 -0.234  -0.167 \n10     1 <kmeans> <tibble> <tibble>    -1.53      -0.506   0.392  0.318   0.209 \n# … with 1,412 more rows, 7 more variables: gdp_per_capita <dbl>,\n#   cardiovasc_death_rate <dbl>, diabetes_prevalence <dbl>,\n#   hospital_beds_per_thousand <dbl>, life_expectancy <dbl>,\n#   human_development_index <dbl>, .cluster <fct>, and abbreviated variable\n#   names ¹​population_density, ²​median_age, ³​aged_65_older, ⁴​aged_70_older"
  },
  {
    "objectID": "W13.html#total-within-sum-of-squares-to-decide",
    "href": "W13.html#total-within-sum-of-squares-to-decide",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Total within sum of squares to decide",
    "text": "Total within sum of squares to decide\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() + geom_point()\n\n\n\nFrom 5 to 6 clusters there is almost no improvement. So, we select \\(k=5\\)."
  },
  {
    "objectID": "W13.html#visualization-cluster-centroids",
    "href": "W13.html#visualization-cluster-centroids",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Visualization: Cluster centroids",
    "text": "Visualization: Cluster centroids\n\ngclusters <- clusters |> filter(k==5) |> \n select(population:human_development_index, cluster) |> \n pivot_longer(population:human_development_index) |> \n ggplot(aes(value,name)) + geom_col() + facet_grid(~cluster)\ngclusters"
  },
  {
    "objectID": "W13.html#visualization-scatter-plot",
    "href": "W13.html#visualization-scatter-plot",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Visualization: Scatter plot",
    "text": "Visualization: Scatter plot\n\nlibrary(patchwork) \ng1 <- assignments |> filter(k==5) |> \n ggplot(aes(population_density, life_expectancy, color = .cluster)) +  geom_point()\ng2 <- assignments |> filter(k==5) |> mutate(continent = owid_inds$continent) |> \n ggplot(aes(population_density, life_expectancy, color = continent)) +  geom_point()\ng1 | g2\n\n\n\nAttention: These are just two out of several variables!"
  },
  {
    "objectID": "W13.html#visualization-clusters-and-continents",
    "href": "W13.html#visualization-clusters-and-continents",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Visualization: Clusters and continents",
    "text": "Visualization: Clusters and continents\n\nclustcont <- assignments |> filter(k==5) |> \n mutate(continent = owid_inds$continent, location = owid_inds$location) |> \n select(.cluster, continent, location) |> arrange(.cluster, continent)\ng1 <- clustcont |> ggplot(aes(.cluster, fill = continent)) + geom_bar()\ng1 | gclusters"
  },
  {
    "objectID": "W13.html#d",
    "href": "W13.html#d",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "D",
    "text": "D\n\n\n\n  [1] 1 2 1 3 2 2 2 4 1 3 3 1 4 2 4 3 1 3 1 2 1 1 3 2 1 1 1 1 2 1 1 2 2 2 3 2 2\n [38] 4 2 4 1 2 2 1 2 1 1 2 1 1 3 2 4 1 1 2 4 1 4 3 1 1 1 1 1 2 2 1 1 1 1 4 4 4\n [75] 2 4 1 2 1 3 3 1 1 2 2 1 1 2 4 1 1 2 1 4 3 2 2 1 2 2 1 1 1 4 2 1 1 2 2 3 1\n[112] 2 1 2 1 2 4 3 2 2 3 3 1 3 3 4 2 4 3 1 4 4 2 1 1 2 4 1 1 2 1 1 3 3 2 2 1 2\n[149] 3 4 2 2 1 1 2 1 1 1"
  },
  {
    "objectID": "W13.html#country-list",
    "href": "W13.html#country-list",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Country list",
    "text": "Country list\n\nclustcont\n\n# A tibble: 158 × 3\n   .cluster continent location         \n   <fct>    <chr>     <chr>            \n 1 1        Africa    Botswana         \n 2 1        Africa    Equatorial Guinea\n 3 1        Africa    Gabon            \n 4 1        Africa    Libya            \n 5 1        Asia      Bahrain          \n 6 1        Asia      Bhutan           \n 7 1        Asia      Brunei           \n 8 1        Asia      Kuwait           \n 9 1        Asia      Kyrgyzstan       \n10 1        Asia      Mongolia         \n# … with 148 more rows"
  },
  {
    "objectID": "W13.html#connectivity-based-hierarchical-clustering",
    "href": "W13.html#connectivity-based-hierarchical-clustering",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Connectivity based: Hierarchical clustering",
    "text": "Connectivity based: Hierarchical clustering\nHierarchical clustering seeks to build a hierarchy of clusters.\nHere, we use a bottom up approach:\n\nWe start with every case building its own cluster\nThen we iteratively join clusters which are close to each other to form a cluster\n\nTo that end we first build the distance matrix. It is a symmetric \\(n \\times n\\) matrix where each entry is the Euclidean distance between two cases."
  },
  {
    "objectID": "W13.html#hierarchical-clustering-step-1",
    "href": "W13.html#hierarchical-clustering-step-1",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Hierarchical clustering: Step 1",
    "text": "Hierarchical clustering: Step 1\n\n\n\nFrom https://allisonhorst.com/agglomerative-hierarchical-clustering"
  },
  {
    "objectID": "W13.html#hierarchical-clustering-step-2",
    "href": "W13.html#hierarchical-clustering-step-2",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Hierarchical clustering: Step 2",
    "text": "Hierarchical clustering: Step 2\n\n\n\nFrom https://allisonhorst.com/agglomerative-hierarchical-clustering"
  },
  {
    "objectID": "W13.html#hierarchical-clustering-step-3",
    "href": "W13.html#hierarchical-clustering-step-3",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Hierarchical clustering: Step 3",
    "text": "Hierarchical clustering: Step 3\n\n\n\nFrom https://allisonhorst.com/agglomerative-hierarchical-clustering"
  },
  {
    "objectID": "W13.html#hierarchical-clustering-step-4",
    "href": "W13.html#hierarchical-clustering-step-4",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Hierarchical clustering: Step 4",
    "text": "Hierarchical clustering: Step 4\n\n\n\nFrom https://allisonhorst.com/agglomerative-hierarchical-clustering"
  },
  {
    "objectID": "W13.html#hierarchical-clustering-step-5",
    "href": "W13.html#hierarchical-clustering-step-5",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Hierarchical clustering: Step 5",
    "text": "Hierarchical clustering: Step 5\n\n\n\nFrom https://allisonhorst.com/agglomerative-hierarchical-clustering"
  },
  {
    "objectID": "W13.html#hierarchical-clustering-step-6",
    "href": "W13.html#hierarchical-clustering-step-6",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Hierarchical clustering: Step 6",
    "text": "Hierarchical clustering: Step 6\n\n\n\nFrom https://allisonhorst.com/agglomerative-hierarchical-clustering"
  },
  {
    "objectID": "W13.html#hierarchical-clustering-step-7",
    "href": "W13.html#hierarchical-clustering-step-7",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Hierarchical clustering: Step 7",
    "text": "Hierarchical clustering: Step 7\n\n\n\nFrom https://allisonhorst.com/agglomerative-hierarchical-clustering"
  },
  {
    "objectID": "W13.html#calculation-hierarchical-clustering-in-r",
    "href": "W13.html#calculation-hierarchical-clustering-in-r",
    "title": "W#13: Probability, Matrices, Clustering",
    "section": "Calculation hierarchical clustering in R",
    "text": "Calculation hierarchical clustering in R\n\n\n\n  [1] 1 2 1 3 2 2 2 4 1 3 3 1 4 2 4 3 1 3 1 2 1 1 3 2 1 1 1 1 2 1 1 2 2 2 3 2 2\n [38] 4 2 4 1 2 2 1 2 1 1 2 1 1 3 2 4 1 1 2 4 1 4 3 1 1 1 1 1 2 2 1 1 1 1 4 4 4\n [75] 2 4 1 2 1 3 3 1 1 2 2 1 1 2 4 1 1 2 1 4 3 2 2 1 2 2 1 1 1 4 2 1 1 2 2 3 1\n[112] 2 1 2 1 2 4 3 2 2 3 3 1 3 3 4 2 4 3 1 4 4 2 1 1 2 4 1 1 2 1 1 3 3 2 2 1 2\n[149] 3 4 2 2 1 1 2 1 1 1"
  },
  {
    "objectID": "W13.html#calculate-hierarchical-clustering-in-r",
    "href": "W13.html#calculate-hierarchical-clustering-in-r",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Calculate hierarchical clustering in R",
    "text": "Calculate hierarchical clustering in R\n\nFirst compute the distance matrix\n\n\nowid_BoxCoxData |> dist(method = \"euclidean\") \n\n             1          2          3          4          5          6\n2    5.7254748                                                       \n3    4.2621370  2.8506982                                            \n4    6.1822007  2.5703527  4.0845931                                 \n5    6.0968971  2.8094315  2.1035927  4.1973758                      \n6    5.3423488  1.1771810  2.6952787  2.8070052  2.6104613           \n7    8.1805454  4.2685069  3.9745042  5.3642390  2.5971817  4.5512899\n8    8.2525580  3.0911442  4.4296667  4.1251275  2.8967389  3.3998199\n9    4.4070181  2.6335467  2.6544230  3.6618169  3.3186537  1.9487153\n10   5.7500869  2.0499757  3.1654422  1.7686082  3.2849860  2.3460811\n11   6.4272994  4.6911028  5.1889798  3.2909347  5.7634803  4.9578762\n12   3.7898585  4.3127686  3.8344498  5.0647707  4.9887800  4.1712232\n13   7.4243641  2.5158907  4.9025226  2.0720994  4.3763980  2.9851270\n14   6.6417633  2.7610002  3.6621385  4.3507529  2.7198608  2.1311527\n15   8.3669756  3.5724125  4.7497731  4.3864182  3.3901019  3.7780030\n16   4.6682805  3.8939441  3.5247678  3.0992701  4.5829256  4.0624636\n17   4.1934925  6.2665413  4.7598282  6.5802573  5.8457302  5.5829288\n18   4.1428908  3.1290200  2.5720013  2.8925334  3.6571943  2.9477953\n19   4.0453198  3.3771525  1.6617381  4.1568752  2.8370407  3.1654637\n20   6.0739359  0.8090918  3.1661610  3.0858447  2.8802043  1.3730407\n21   4.4296162  4.2308885  2.4501039  4.4229997  3.4518291  3.7883160\n22   5.1609724  3.0991975  1.5929393  4.4971104  1.8690393  3.1606270\n23   5.9992117  3.2936747  3.6431311  2.0769530  4.0208698  3.5251775\n24   6.8599177  2.3638930  3.8540042  4.2507140  2.9527478  2.0271936\n25   3.0630437  6.3277056  4.6146635  6.5618792  6.0618264  5.7379091\n26   2.6620727  5.8599988  4.8657005  5.8109033  6.2611885  5.3329471\n27   3.0054327  4.1678437  2.7352418  4.7296411  4.1938598  3.6646362\n28   2.4804166  5.0575497  3.4539451  5.1945935  4.8083007  4.5309161\n29   7.9986007  4.0284119  3.8852672  5.2406119  2.7352987  4.4983037\n30   5.0223757  3.9541984  3.5108262  3.7631030  4.0521051  3.3222982\n31   2.6880795  6.0543279  4.6555534  6.4248476  6.0260907  5.4364628\n32   6.3523204  2.5533712  2.4350967  3.6258362  1.8584081  3.0185965\n33   5.9018464  3.6531340  3.2243872  5.3110205  3.1125547  3.7116493\n34   5.3117752  2.9713898  1.8744099  3.8391017  2.1689214  3.1094106\n35   3.5779828  4.8427777  4.6897108  4.0728347  5.7950714  4.4228341\n36   5.7914225  2.2232218  2.6517159  2.8428837  2.8785622  2.7789804\n37   7.1608737  1.9808818  3.6548249  3.6748351  2.4872568  2.0404955\n38   7.1944880  2.1624815  3.8097404  2.3511744  3.0834417  2.7581333\n39   7.4376328  2.2855472  3.8672772  3.7519680  2.5774322  2.4998298\n40   7.9505550  3.0453039  4.2646810  3.9297173  3.1639937  3.5733148\n41   3.2777014  4.1233701  3.2614163  4.0801438  4.5227532  3.5841103\n42   4.2378120  2.2623090  2.1535805  2.9257861  3.0505711  2.1471839\n43   4.9980747  2.8946949  1.8874457  3.5714761  2.3915004  2.8337218\n44   3.1774335  3.8301862  3.0608921  4.6929436  4.4756835  3.8265617\n45   4.6225712  2.6746585  2.9256437  2.9455955  3.5711913  2.6796590\n46   3.9750848  4.7297308  3.6490208  4.0432472  4.6556607  4.2506552\n47   2.3386419  4.9571791  3.7128594  5.1049586  5.2584870  4.4738937\n48   7.4577055  2.5913706  3.8602357  3.9825430  2.6936868  2.5650511\n49   3.4632638  4.7524247  3.7613862  4.5252212  4.8444789  3.9715128\n50   2.9219789  5.5394056  3.9521390  5.9274094  5.4474963  5.3054034\n51   3.9157820  2.9327907  3.1898906  2.9645884  4.1638243  2.7163181\n52   7.9278182  3.0589923  3.9189799  4.3626770  2.5893406  3.4886490\n53   8.4723868  3.9803730  4.5825306  5.0461554  3.0255716  4.2391589\n54   4.7511891  4.0040289  2.9061144  3.9304601  3.2894364  3.3680148\n55   3.5938912  6.0787227  4.9335285  6.0173416  6.1470021  5.3210247\n56   5.4369402  1.6369263  3.0381280  3.7154766  3.1098816  1.3437445\n57   8.5150332  3.6547550  4.8832719  4.9473680  3.4463360  4.0253427\n58   2.4307434  4.7562440  3.2240217  5.1159770  4.6827610  4.2087642\n59   7.7430283  2.7001963  3.9454530  4.2858929  2.6463500  2.9879872\n60   5.5982655  2.5176742  3.9439072  1.1773986  4.1401086  2.3597204\n61   3.8670494  3.7568841  2.8350436  3.8275220  4.1236279  3.8281080\n62   2.8766682  6.0257521  4.4101257  6.4154196  5.9012626  5.4401085\n63   3.9089552  3.8327963  2.9938342  4.0573982  4.2197897  3.6242463\n64   2.4794393  4.6030678  4.0588496  5.0745390  5.4500925  4.1183183\n65   3.2217454  3.4511746  2.2649958  3.9066538  3.8813681  3.2998167\n66   7.0237578  1.9811433  3.6654179  3.6497346  2.5219608  2.0436485\n67   8.2451662  4.0738964  4.4301815  4.5007252  3.4907321  4.3851376\n68   3.9723255  4.8884202  3.8470736  5.9561353  5.0043523  4.8652676\n69   3.5187292  3.7925639  2.2571248  4.9793067  3.6198857  3.5373536\n70   4.3077407  2.8537706  1.3009266  3.9662232  2.6794853  2.9667392\n71   3.2703698  4.1793095  2.4606345  4.2409821  3.9047918  3.9361095\n72   7.9695028  3.6245993  4.1034148  4.3422499  2.9885994  3.8379438\n73   7.4381778  3.3617464  4.0996609  3.5751761  3.3522131  3.7445555\n74   8.4202081  3.7439578  4.6950633  5.1336709  3.4909589  4.1690863\n75   4.9792439  1.8143225  2.9984858  2.2526673  3.4459501  2.0705253\n76   9.7797332  5.0483442  6.0778775  6.0246930  4.4655416  5.3061391\n77   3.6441404  3.3501536  2.4201858  3.2565263  3.7593819  3.3280929\n78   5.3089308  3.3145494  2.4100630  4.4776511  2.4500234  2.8353129\n79   3.3418693  5.3285705  3.4858277  5.6316749  4.7079968  4.6991163\n80   4.1036292  4.5862099  5.0219136  3.8361631  6.1713809  4.4939045\n81   5.9540753  4.5121189  4.2595191  3.3832045  4.9383263  4.7567451\n82   3.5175463  3.3291590  2.4501267  3.9724210  3.4811317  2.5711883\n83   2.9392685  4.1382794  2.5310768  4.6498618  3.9806850  3.4610874\n84   7.1047534  2.4281983  3.8236295  4.0556800  2.8055859  2.2097068\n85   5.3044693  1.9272393  3.2979459  2.3268534  3.6971843  2.2769268\n86   3.2708087  5.7634988  4.3266291  5.9807695  5.6203157  5.0923594\n87   4.4556850  3.5723476  2.1769773  4.0685797  3.1487647  3.3178986\n88   7.3248427  2.7643659  3.9748544  4.3024022  2.8155165  2.4363632\n89   8.4042850  3.6386240  4.8831024  3.6954814  3.8667652  3.8943811\n90   2.0802324  5.7741761  4.0656338  6.3503537  5.8131134  5.3323575\n91   3.1085366  5.4752797  4.1705711  5.5769809  5.3887856  4.8518198\n92   4.7592105  2.7275460  2.4860778  3.2620473  3.3314108  3.1264706\n93   3.4533045  6.9369358  5.0265435  7.2787100  6.5513291  6.4581254\n94   8.2531830  3.1516447  5.3833777  3.2357438  4.6680559  3.6476578\n95   6.3812981  2.6391020  4.5468592  2.1924927  4.6150674  3.2194241\n96   4.9193022  3.2356732  2.2688303  4.0097586  2.9360767  3.5374656\n97   5.3259311  2.1176295  3.2122424  3.4714292  3.1171013  1.0324167\n98   5.0287781  4.5567529  3.2601110  5.0959190  3.7530449  3.8507451\n99   6.1564843  1.2800071  3.4018351  2.7605761  3.1812468  1.5427414\n100  3.4713192  2.7386307  1.7175266  4.1706849  3.3209969  2.5770877\n101  2.5183422  5.9047794  4.1677402  6.3712545  5.6079561  5.2707384\n102  3.5968341  4.0611698  2.5868303  4.8169517  3.7637152  3.6462907\n103  2.9883988  4.1447736  3.2280325  4.8167051  4.7165406  3.9916613\n104  8.3186035  3.6180196  4.7862969  4.4578401  3.6875009  4.0231705\n105  7.4395527  2.8546997  3.4814783  3.8064363  2.4589979  3.4271024\n106  4.2703248  3.3228827  2.6139553  3.4000117  3.7086471  3.4618606\n107  3.4156236  6.7983404  4.8694548  7.1042218  6.3123874  6.2578279\n108  5.7541332  0.7895593  2.9912706  2.5465496  2.7961560  0.9414600\n109  8.2108521  3.6782699  4.1182543  4.5159582  2.7572248  4.0423228\n110  4.8041853  3.8901423  2.7431742  3.6803878  3.9923214  4.0338111\n111  2.3499230  4.7513002  3.5080971  5.6091233  5.0636939  4.4667705\n112  5.7977366  2.4268282  2.3972482  2.5962819  2.3154611  2.6845655\n113  4.1679410  2.8943952  1.5223240  3.4589554  2.7440446  2.8186841\n114  5.7235194  3.6615111  2.4249248  4.2112561  2.4518184  3.7061692\n115  3.1186391  3.8784113  2.8053214  4.7205220  4.1819717  3.5816932\n116  7.0873019  2.4682473  3.4668541  4.1094677  2.1232365  2.5018217\n117  7.8291705  2.6087021  4.2112626  3.8603457  3.1585393  3.3426483\n118  6.4845202  5.5889305  5.1519573  4.4753525  6.0744369  5.8892431\n119  6.6241375  1.9802899  3.5368225  3.8954612  2.7254254  2.0640652\n120  6.4215989  3.5948017  3.2432266  5.4285851  2.3953771  3.2415818\n121  5.7890199  2.1662577  3.8854499  1.6140029  4.2476535  2.6131067\n122  5.3855606  2.4465393  3.9361048  1.3096085  4.3360390  2.4193660\n123  4.6432669  4.9632988  4.4563265  4.4584102  5.2404509  4.1905212\n124  5.2287406  4.0027623  2.9589958  4.1551598  3.7742054  4.2174705\n125  6.1332556  2.2338112  3.9592358  1.1225373  3.9581808  2.2916910\n126  9.0407655  5.0087900  6.4561075  4.7324770  5.9969654  5.5650078\n127  6.6849345  1.6462050  3.3303648  3.1775849  2.4234698  1.7159773\n128  7.8608489  2.4533708  4.2092749  3.4474218  3.0155781  2.9535746\n129  3.2039960  4.5532130  4.1121854  4.3675459  5.6065958  4.4591026\n130  4.0273910  3.8297023  2.1585437  4.4428726  2.9144230  3.2974179\n131  8.7784058  4.3362271  5.2757675  4.8568450  3.9022702  4.5265487\n132  8.1450717  3.4554483  4.2917079  4.7217094  3.1061834  4.0332393\n133  5.5082323  1.9195845  2.8546097  2.8299784  2.8210456  2.1272725\n134  1.7969149  5.0769822  3.6527272  5.4948278  5.3704942  4.8847500\n135  5.1573751  3.2685133  2.9586668  3.3727164  3.4541247  3.1927320\n136  7.9707366  3.3914901  3.9516837  4.6628976  2.7501984  3.8307598\n137  8.6351927  3.6896070  4.8513282  4.3966425  3.5493780  4.1240225\n138  3.2159088  3.8395614  2.8544713  4.2089434  3.9700124  3.0977662\n139  2.5819320  5.3144831  3.4929805  5.6192474  4.9799636  4.8876777\n140  6.2790262  2.8906283  2.9518183  4.0217220  2.5046761  3.1484818\n141  3.9012206  4.2534001  3.5808681  3.7987171  4.4364474  3.5170930\n142  2.3018922  5.3757155  4.1722088  5.3661145  5.6415463  4.8602234\n143  4.9103065  3.3768343  4.1324178  2.1179181  4.7564578  3.3039586\n144  5.7720910  1.7001170  3.3030350  1.7442640  3.2948250  1.9437316\n145  4.4161004  1.5504607  1.5550586  3.0412269  2.4544941  1.4666180\n146  5.5675291  2.6227436  2.3636812  3.5236050  2.3986595  2.9778334\n147  3.4737974  6.3233142  4.7017392  6.4647045  6.0132121  5.7638746\n148  6.1292380  2.9056101  3.7685296  4.8471972  3.2462803  2.3630499\n149  5.6109104  5.4517996  4.7263072  4.8817790  5.9139719  5.6499368\n150  7.8745793  3.6127201  4.2540181  4.8733606  3.1568804  3.9227761\n151  7.2498309  3.6406199  3.5385518  5.0479472  2.6305354  4.1209917\n152  6.4564223  2.0809984  2.7160403  3.2767194  1.8400953  2.3486747\n153  3.4078166  3.6818334  2.9190653  4.7427764  4.0599623  3.0429694\n154  4.1052302  3.1779100  1.5159281  4.0214488  2.8331649  3.0818184\n155  4.6002112  2.8682660  2.4346019  4.1061268  2.9779947  2.5585480\n156  1.3291634  5.4532509  3.8592150  6.0134867  5.5877184  4.9136948\n157  3.1807346  5.3567218  3.4029390  5.4756644  4.6350654  4.6899630\n158  3.4873458  5.6741955  4.0093294  6.0471935  5.1072904  4.8531625\n             7          8          9         10         11         12\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8    2.9805677                                                       \n9    5.5161633  4.6068783                                            \n10   4.4419628  3.8196129  3.1765680                                 \n11   6.8961212  5.7152358  4.8769227  4.1767504                      \n12   7.0386270  6.2316262  3.8527251  5.1482899  4.7191893           \n13   5.3381613  3.1924157  4.3086483  2.9165489  4.1846069  5.4501005\n14   4.4536655  3.2096185  2.6226794  3.6761959  6.3541126  5.6294494\n15   3.6215659  1.3743034  4.9384944  4.4172340  5.5279821  5.9233254\n16   5.7354263  5.9712477  4.4031335  2.7234682  4.4093415  5.1065659\n17   7.6484718  7.8251952  5.3532629  6.4134656  7.0823280  4.5671771\n18   5.2175682  5.2818131  3.3156739  2.3017793  4.6216537  4.4204871\n19   4.4884156  5.1538957  3.5572089  3.2157560  5.5637938  4.1532842\n20   4.2597604  3.0390288  2.8533764  2.3483738  5.3193238  4.7809332\n21   4.8500929  5.6822829  3.7104599  3.4806303  5.7877818  5.2868063\n22   3.6038796  3.9633751  3.4962208  3.6193452  5.3293040  3.8684792\n23   4.9416050  4.3778342  3.6583169  1.8746536  2.9928357  5.3132355\n24   4.4149774  2.9078456  2.9700604  3.5397712  6.3506471  5.5892940\n25   7.9649642  8.2184784  5.2846831  6.3429013  6.8626825  4.2335835\n26   8.4239346  8.0694324  4.8743806  5.9560229  5.9106513  3.2365534\n27   6.1731007  6.1562627  3.4672757  4.4426859  5.3877130  2.8665285\n28   6.9408085  7.0389155  4.0732930  4.8081657  5.6767405  3.5252809\n29   1.0330189  3.0117861  5.5098193  4.2169549  6.6566490  6.7536105\n30   5.6772605  5.3260899  3.7729476  3.9840459  5.1004050  4.3343254\n31   8.0261054  8.3759282  5.0284794  5.8177444  7.6121712  5.2905269\n32   2.3945744  2.7985885  4.0487500  2.8283520  4.9781093  4.9121239\n33   4.8726918  4.1646246  3.6522795  4.7486909  5.6664140  3.7796919\n34   3.5621004  3.8159522  3.7594104  3.2748643  4.6502314  3.7860179\n35   7.8708996  7.1583119  4.2180970  4.5336449  4.5746574  3.7504061\n36   3.7519492  3.4452612  3.8620316  2.5780853  4.0659233  4.0932755\n37   3.4508518  1.7429974  3.4678641  3.1136992  5.7863447  5.5445010\n38   3.4992937  2.1232283  4.1203974  2.3228650  4.3139525  5.4551248\n39   3.4619884  1.1549478  3.6390475  3.3345989  5.4216920  5.5065008\n40   3.0044080  1.6178621  4.8771803  3.6114182  5.2563445  5.8265498\n41   6.4191723  6.3629839  3.6335573  3.8458773  5.4209966  4.0221635\n42   5.0187076  4.3000725  2.3152730  2.6936861  3.7928575  2.6665201\n43   3.9431100  4.0322796  3.5236528  3.2218649  4.5706194  3.5519887\n44   6.5045189  6.1414386  2.9137587  4.0805926  4.9470695  3.1550362\n45   5.3463764  4.5906188  3.4314809  3.0609014  3.9922026  2.7368074\n46   6.4722101  6.3954438  3.8889984  3.7245742  4.7593054  4.6079852\n47   7.2105217  7.3187534  4.2407845  4.8514518  6.0352515  3.8522723\n48   3.1452191  2.1872702  3.8951095  3.2709636  6.2394073  6.2020821\n49   6.9072540  6.6633943  3.4437688  4.3441566  5.4700463  4.3179387\n50   7.2936908  7.4901032  5.1106613  5.6766274  5.9438958  2.9419795\n51   6.0343292  5.5278076  2.5832651  2.2508211  4.5935164  4.3313968\n52   1.8403710  1.7887077  4.7770330  3.5527332  6.2001672  6.4439974\n53   2.8791935  1.6621468  5.3464024  4.8004624  6.1194670  6.1274144\n54   5.1570356  5.3631425  3.1707530  3.1462165  5.6367950  5.3933825\n55   8.1796736  7.9837926  4.8404552  6.1041952  6.5133216  4.4055783\n56   4.8177121  3.8831686  2.2695074  2.8582554  5.7423673  4.5618434\n57   3.7685027  1.5265227  4.9545554  4.6445216  5.9620131  6.1228091\n58   6.8272354  6.7417813  3.6408552  4.8361138  5.3926561  2.7673661\n59   2.8742017  1.3202802  4.3475789  3.8653306  5.9876232  5.7625895\n60   5.7272087  4.4214060  3.1237098  2.0107967  3.6344526  4.5779841\n61   5.7399642  5.6478440  4.0313647  3.7530029  4.0283426  2.6671561\n62   7.7861771  8.0475527  5.0128285  6.0785440  6.9523746  4.2619062\n63   5.7133525  6.1559794  3.5926854  2.9889487  5.8429379  5.3212661\n64   7.6529863  7.0690912  3.7022796  5.0103600  5.6010735  2.4698121\n65   5.6544692  5.6658476  3.3732857  3.6685046  4.6429850  2.7553077\n66   3.8102579  1.8388950  3.1742022  3.0805857  5.5237334  5.3150846\n67   2.1774857  3.4057989  5.5646810  3.7058859  6.5586730  7.4494628\n68   6.9300032  6.5430843  4.4483101  5.5992956  5.5229336  1.7744507\n69   5.6905321  5.4729590  2.9042183  4.4334754  5.0684926  2.1489497\n70   4.4031722  4.4374173  2.7235203  3.1860991  4.3770862  3.2649655\n71   5.8444741  5.8789254  3.4056448  3.9201124  4.2508539  2.9874683\n72   2.5746110  2.0929825  4.8334752  3.9838838  5.5789621  6.1832270\n73   3.7838622  2.6232015  4.7579150  3.8501135  4.2532416  5.0526466\n74   3.3691805  1.9352836  5.3471793  4.8748201  6.1608392  5.8944473\n75   5.1260880  4.0997876  3.0931582  2.3057327  3.8304379  3.2717027\n76   4.3374059  2.4775139  6.3562074  6.0101823  6.9729995  7.1606609\n77   5.5478064  5.3766281  3.2434814  3.1161363  3.5935322  2.9815129\n78   4.2875189  4.4471454  2.3438243  3.3115652  6.0953025  5.4535955\n79   6.7380556  6.8845564  4.2424063  5.4385363  5.8764774  3.4725292\n80   7.9685820  7.3120147  4.3428326  4.0044366  4.9282655  4.9452203\n81   5.9352915  5.4255413  4.5767293  3.5754604  1.9388622  4.8831028\n82   5.7330442  5.4971979  1.9661994  3.4674742  5.4262427  4.0346022\n83   6.0255129  6.1543864  2.9395548  4.1069225  5.7145225  3.8391864\n84   3.7982367  2.6810494  3.3970723  3.1861098  6.3682557  6.0346943\n85   5.3875881  3.9607424  2.7982605  2.7528453  3.3710507  3.2561816\n86   7.5387585  7.7295730  4.8665676  5.8027817  6.7814338  4.3921253\n87   4.7311136  5.2712901  2.9829047  2.8788783  5.5894890  5.2593753\n88   3.8054150  2.5197556  3.4723659  3.5913508  6.4350277  6.0877443\n89   3.7785333  2.0995783  4.9594800  3.8254688  5.1216843  6.4750101\n90   7.6753346  8.0095594  4.8068392  5.9412046  6.7513250  3.8507210\n91   7.5190667  7.3584679  4.5304893  5.6153123  5.9397774  3.2955064\n92   4.8854205  4.4549234  3.0075360  2.6317893  3.5918372  3.6276096\n93   8.1819828  8.8533636  6.0987442  6.8033963  7.7239645  5.2136537\n94   5.2093467  2.7263222  4.8640845  3.8956980  4.5880153  5.8495030\n95   5.9173794  4.2455774  3.9127326  2.5973789  3.3526722  4.6554973\n96   4.4312608  4.4550145  3.7959951  3.3720468  4.3063277  3.4052697\n97   5.2233453  3.9863594  1.9013511  3.1191540  5.4874951  4.3202944\n98   5.3973438  5.9072241  3.3167390  4.1229456  6.7960068  6.1690656\n99   4.4202045  3.3590322  2.7960997  1.8873856  5.3027553  5.3376963\n100  5.2915346  5.0663597  2.2846739  3.5965807  5.0822209  2.7068785\n101  7.6586773  7.9296448  4.7901808  5.9912462  6.9948607  4.1009669\n102  5.7425948  5.7326697  3.6532264  4.3707694  5.4310018  2.7791747\n103  6.5847456  6.3993116  4.0026478  4.6001260  5.1867465  2.0489949\n104  3.7878905  1.8404522  5.1308557  4.4727349  5.2492118  5.7115615\n105  1.8553270  2.2245009  4.6501003  3.0183804  5.4967816  6.0395789\n106  5.1637051  5.2589842  4.0298439  3.1035977  4.3403603  3.5386912\n107  8.0420626  8.6696932  5.9351817  6.7355935  7.6345643  5.0532660\n108  4.4240172  3.1583188  2.3935407  1.8888034  4.8695276  4.5860885\n109  1.4501622  2.0585778  5.1611021  3.7906758  6.1207546  6.7724846\n110  5.0220692  5.4198356  3.6236012  2.9577554  4.0189671  5.0342658\n111  7.2277597  6.9175338  3.7233872  5.2548032  5.5997762  1.8466686\n112  3.3904034  3.2613223  3.6221046  2.2151324  4.0486430  4.4574868\n113  4.3441790  4.7842097  3.2772046  2.6262779  4.8873069  3.9947918\n114  3.4499686  4.1609929  4.4643916  3.6904683  5.1170789  4.4556827\n115  6.3278080  5.8433033  2.8817805  4.4639102  4.6526029  1.5229858\n116  3.3833760  1.5772854  3.4200021  3.6224606  5.5180756  5.0971597\n117  3.2417032  1.6255971  4.7631269  3.5111859  5.4202952  5.6864367\n118  6.7834720  6.5118424  5.4221786  4.6783242  2.5846376  5.8207759\n119  4.2511568  2.6592262  2.8404920  3.1499095  5.6783576  5.1349173\n120  4.0527491  3.9074609  3.2515735  4.2598115  7.0086677  5.7757362\n121  5.4170634  4.2785077  3.7553446  2.0921590  3.7277859  4.4053618\n122  5.8773957  4.6706359  3.2424935  1.9918271  3.7284704  4.4548183\n123  7.0979006  6.6487374  4.1029396  4.8661632  5.5511776  4.7232494\n124  4.9154216  5.1822073  3.7252226  3.2075399  4.2798125  5.1088582\n125  5.2428617  3.8452540  3.2098944  1.5261905  3.7767320  5.0502247\n126  6.3366654  4.2764545  6.2943329  5.4708869  4.1201770  6.0704373\n127  3.7581460  1.9587016  2.8003449  2.6355327  5.0480031  5.0765528\n128  3.1329550  1.1385955  4.4218288  3.1538509  5.3391458  5.9707944\n129  7.3161610  7.2925962  4.1530614  4.0517210  5.5762287  4.9054642\n130  5.1002114  5.1225589  3.1058725  3.7767751  5.1409341  3.4545207\n131  4.2720214  2.2118461  5.4011474  5.0658712  5.4904480  6.2022409\n132  2.7423107  1.8263090  5.2417025  4.3288965  5.8018311  5.8591421\n133  4.6279297  3.2992541  2.8741517  2.8378762  3.7858466  3.2538464\n134  7.3446252  7.5266982  4.1872426  4.8466229  5.9869626  3.9278186\n135  4.6782707  5.0796252  3.6437811  2.0729545  5.5875614  5.7328231\n136  1.8274020  2.0839805  5.0322897  3.9487656  6.1946489  6.3156597\n137  3.1939809  1.3318129  5.3093245  4.3326513  5.5113812  6.3218929\n138  6.2818305  5.9396799  2.2529218  3.9710808  5.2499629  3.7293317\n139  6.9697941  7.2143049  4.5137620  5.3169097  5.8715695  3.2109515\n140  3.6569626  3.0403838  4.1015185  3.6553813  4.7993655  3.9686651\n141  6.5864099  6.1416222  2.9569508  3.8920504  5.0368792  4.4157927\n142  7.7203648  7.5971358  4.4466143  5.2770944  5.6897184  3.3608631\n143  6.3777660  5.6416002  3.8337294  2.5709456  4.0687540  4.6592465\n144  4.7951312  3.3309013  2.8074051  1.6563543  3.5110055  4.1818642\n145  4.4456946  3.9073462  1.8839823  2.4046197  4.4972592  3.3267553\n146  3.9292187  3.4341977  3.3431159  3.0077071  4.0484172  3.7333718\n147  7.9475285  8.0208070  5.3287466  6.4322311  6.3786843  3.7232477\n148  5.2617651  4.0101083  2.5974910  4.1489850  6.5714563  5.0119179\n149  6.9154769  6.7738568  4.8104256  4.7039758  3.3610109  5.4886697\n150  3.3983294  2.1574632  4.9234882  4.6119626  5.6460766  5.3389445\n151  2.9491064  2.9686021  4.6950591  4.1458521  5.7393221  5.4491581\n152  2.6165841  2.5871709  3.7731448  2.3604773  5.4563621  5.2900910\n153  6.3577719  5.8477329  1.4935472  4.2072296  5.6278275  3.7310301\n154  4.5083726  4.7928177  3.2688050  3.2239637  4.8665907  3.3848097\n155  5.1378481  4.3155221  2.6928544  3.9568685  4.6865911  2.2254500\n156  7.7020681  7.7967345  4.1303734  5.6350704  6.5053915  3.5893930\n157  6.6458124  7.0223415  4.1990152  5.0981658  6.1211455  4.3088732\n158  7.1732669  7.3457280  4.3708234  5.7409729  6.7989493  4.4272871\n            13         14         15         16         17         18\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14   4.0983926                                                       \n15   3.4130740  3.9066930                                            \n16   4.7662463  5.6965057  6.3617415                                 \n17   7.6275983  6.7564131  7.4720463  5.8739141                      \n18   4.4170864  4.5681245  5.6147302  1.6785368  4.7112722           \n19   5.1979198  4.4921816  5.4035670  2.9146440  4.1604819  1.9138109\n20   2.7825854  2.4317026  3.7082598  4.3505926  6.6273381  3.5032179\n21   5.8506596  4.7117886  6.0075828  3.1867790  4.2235319  2.1254118\n22   4.8179824  3.8954210  4.2447363  4.1975085  5.4592599  3.4361887\n23   3.6027579  4.6715576  4.7228576  3.0210028  6.5722640  2.9355868\n24   3.7264563  1.0667386  3.6132081  5.7627528  6.9665604  4.6525123\n25   7.7734631  7.0334890  8.0030611  5.3652156  1.4417706  4.4078921\n26   6.9108856  6.8687859  7.8562225  4.9432466  3.0343533  4.2367908\n27   5.7563084  5.1349230  6.0238691  4.0231211  2.4183636  2.8212024\n28   6.4195944  5.8391701  7.0601993  3.8546032  3.1442521  3.0382605\n29   5.1430042  4.6169531  3.6960016  5.5022665  7.7493460  5.0804681\n30   4.8203735  4.7408579  5.0697559  4.1221118  3.2513291  2.8623335\n31   7.7005262  6.5526284  8.5959430  4.7646897  3.7656662  3.9088831\n32   3.8210043  3.9128064  3.1903655  3.9282921  6.2694748  3.4258834\n33   5.0162336  3.8735631  4.3723681  5.7679875  6.6321096  5.0341689\n34   4.3126645  4.2837123  3.8767892  3.6498376  5.1252222  2.9651617\n35   5.3442782  6.1066423  7.1347143  3.5794091  4.6224318  3.2218526\n36   3.2711108  4.3712508  3.4600511  3.3733753  5.6966755  2.9172884\n37   3.0343414  1.9964473  2.4393993  5.3596207  6.8830757  4.3790844\n38   2.0785093  3.7744415  2.4484651  4.2777059  7.0211784  3.8572254\n39   2.8945337  2.3300490  1.9037227  5.5957098  7.2980259  4.7847560\n40   3.2605267  4.1269240  1.5706164  5.5365619  7.3539001  4.9542359\n41   5.4286656  5.1650782  6.4618652  3.0612605  3.3063219  1.8131389\n42   3.6909835  3.8512132  4.2705051  3.4254878  4.6439603  2.5411350\n43   4.2351728  4.2512820  3.9218382  3.5215415  4.3943272  2.6391750\n44   5.4798084  4.9163473  6.4641373  4.0450197  5.8855575  3.7279605\n45   3.5433438  4.5620666  4.4329906  3.3608098  4.6102261  2.5846294\n46   5.6299989  5.4959481  6.5156160  3.1756635  4.1288241  2.3223250\n47   6.4222165  6.0291230  7.3447963  3.7109429  2.9007038  2.8191070\n48   3.6867716  2.4353166  2.8356041  5.4823448  6.8982134  4.4862609\n49   5.9290603  5.1871265  6.6552449  4.0729154  2.9571342  2.8508510\n50   6.9043016  6.8261159  7.3260122  4.5160967  3.3591743  3.9616279\n51   4.2622875  4.2036731  5.9782059  2.5734987  5.5774890  2.0287549\n52   3.9865783  3.5829211  2.5458257  5.4820927  7.4862029  4.8291764\n53   4.1995613  4.1935590  1.4606181  6.4671729  7.6602512  5.7757478\n54   5.2391758  3.9567671  5.8448507  3.3413000  5.0661287  2.3578210\n55   7.2320984  6.5943830  7.7231204  5.4477832  1.7780017  4.3779182\n56   3.7897613  2.3144503  4.3534218  4.6536070  5.9514759  3.5722427\n57   3.7594839  3.6816004  1.9473926  6.6900257  8.3920789  6.0742777\n58   6.2251764  5.5710650  6.6097898  4.2653095  2.5162741  3.2644872\n59   3.5069735  3.0601741  1.6940883  5.8364089  7.1427744  4.9758179\n60   2.4589976  4.0367873  4.5713554  3.2993625  5.8931740  2.7312816\n61   4.8499106  5.6050218  5.5211545  2.9658170  4.3211655  2.6247177\n62   7.6185478  6.7414787  7.8880366  5.2523907  1.5393480  4.2065583\n63   5.5221878  4.8333758  6.7199635  2.3163282  5.3641898  1.7689751\n64   5.9548871  5.6922367  6.9497393  4.6553763  3.4269942  3.6924787\n65   5.0121592  5.0544181  5.6352913  2.9948748  3.6499954  2.1584031\n66   2.9128921  1.7667359  2.6128814  5.3844958  7.1010489  4.4796422\n67   4.8086023  4.7065830  4.0319972  5.0897147  7.6364445  4.6868319\n68   6.3017462  6.0190458  6.3753359  5.5243365  5.1625451  4.9332200\n69   5.5822761  4.5861911  5.4112882  4.6824361  4.2223558  3.7477796\n70   4.6752833  4.0546027  4.6779836  3.6042089  5.2579138  2.9650734\n71   5.4654345  5.2689044  5.8747923  3.2986547  4.0721515  2.7301028\n72   4.1668370  4.1020957  1.9230556  5.7539940  6.9571048  5.0235695\n73   3.2225535  4.6611462  2.0289291  5.1342962  6.8030821  4.7424906\n74   4.1361125  4.3287031  1.5897895  6.6349328  7.6990754  5.9005025\n75   2.7245437  4.0492409  4.1564379  3.1884574  5.4100576  2.5415523\n76   4.6852472  4.8794728  2.3635784  7.8206455  9.1291051  7.1949357\n77   4.5054938  4.9985254  5.4425182  2.3636055  4.6820497  2.1173646\n78   5.1763412  2.5118891  5.1793763  4.4996134  6.1133173  3.5777707\n79   6.6994715  5.8428622  6.6700644  4.7765865  2.0356938  3.7525682\n80   5.1981438  6.1358999  7.6373803  3.0757747  6.3705890  3.3355345\n81   4.6058920  5.9763150  5.4971646  3.5760920  6.6850755  3.7908837\n82   5.0103128  3.5623534  5.8453023  3.5673166  4.5329370  2.4757551\n83   5.8824554  4.6323706  6.2498713  3.8458317  2.8911110  2.4901718\n84   3.7622731  1.7318869  3.4338058  5.5002285  6.8935424  4.4190708\n85   2.4976936  3.9043897  4.0210093  3.8337713  6.1088916  3.3670111\n86   7.1771295  6.4113901  7.5891813  4.8949072  1.5917438  3.8225182\n87   5.3145302  3.9559465  5.9472690  2.9610905  5.6925606  2.2871980\n88   3.9373390  1.6524274  3.1352312  5.8830844  6.8705653  4.7524372\n89   3.2814957  4.2439618  1.8941763  5.8795613  7.5367326  5.2231210\n90   7.5426905  6.7088014  7.9261763  4.9443659  2.5214971  4.1044975\n91   6.6259584  6.2322058  7.1264095  4.7773068  2.2157337  3.8362198\n92   3.9932497  4.3555440  4.8002003  3.2111692  6.2298664  3.0898353\n93   8.5765662  7.7344179  8.7297443  5.6359842  2.3053379  4.8127032\n94   1.7911486  4.4208590  2.4865048  5.9286402  7.9975214  5.4498481\n95   2.0144985  4.5777090  4.5563891  4.0374759  7.4777146  3.9895100\n96   4.5229256  4.7475586  4.6117984  3.5564732  5.6941245  3.2618232\n97   3.5768272  1.9771759  4.3102438  4.6613059  5.4540739  3.4050127\n98   6.2413950  3.8526901  6.5220692  4.4632669  5.4989583  3.4376759\n99   2.8928645  2.4917204  4.0948286  4.1296593  6.7650019  3.3645076\n100  4.8592565  3.9630009  5.2008473  3.9240452  4.4825307  2.9361068\n101  7.5173898  6.4656346  7.8563234  5.0913727  2.0397910  4.0751914\n102  5.6020371  4.9758927  5.6079298  4.2105689  2.9730852  2.9982470\n103  5.6421977  5.7296630  6.2272758  4.0362142  3.4900422  3.2735351\n104  3.5017447  4.4183211  0.9253023  6.3195985  7.5683449  5.6856424\n105  3.7377481  3.9745042  2.8600232  4.6246935  7.2317283  4.2235550\n106  4.4607229  5.2348222  5.3595770  2.1724838  4.7506100  1.8535399\n107  8.3607074  7.4999060  8.5463940  5.4935336  2.1070203  4.6556750\n108  2.5570006  2.3093701  3.7691761  3.9508600  6.3169636  3.0603873\n109  4.3500027  4.1235860  2.6452755  5.5458309  7.5945548  4.9929364\n110  5.1875129  5.1119638  5.8501654  2.6065271  6.0409349  2.6774607\n111  6.3693652  5.7481425  6.8426926  4.9322980  4.0710251  4.1505118\n112  3.3256893  4.0256136  3.4191228  3.1044502  5.6107284  2.5830344\n113  4.5932334  4.3229363  5.0515052  2.3563004  4.5042966  1.4394288\n114  4.7966276  4.8161793  4.1416937  3.7869336  4.9959513  3.1583277\n115  5.4275653  4.8996431  5.6906239  4.5273945  4.0521587  3.7039137\n116  3.4231672  2.2134466  2.0592111  5.6413418  6.8926988  4.7390623\n117  2.8509644  3.8741597  2.1589589  5.3897473  7.7221842  4.8763631\n118  5.8084568  7.0264652  6.5736040  4.4360218  7.4568845  4.8502681\n119  3.3101332  1.6937954  3.4673550  5.3282696  7.2063092  4.4651710\n120  5.3971489  2.0740895  4.7018372  5.8875505  6.8869540  4.8516726\n121  2.3754390  4.5477272  4.3644871  3.2509734  6.0614667  2.8281312\n122  2.5865060  4.2644821  4.8451487  3.0741909  5.8378413  2.5158293\n123  5.7204777  5.5015376  6.4367700  4.5905269  3.2243114  3.5430287\n124  5.2304344  4.9249277  5.7527094  3.3846892  6.8235416  3.4923323\n125  2.1633587  3.7359605  4.1472714  3.5655510  6.3703621  2.9413430\n126  3.7985041  6.4181028  3.5782079  6.9531565  8.7620616  6.7422134\n127  2.7327079  1.8637761  2.6156551  4.9569009  6.7654441  4.0592846\n128  2.5957324  3.3414247  1.8047858  5.3198118  7.5108978  4.6939515\n129  5.8444773  5.9745864  7.7150703  2.5615624  5.8264605  2.7729440\n130  5.2906400  4.2524637  5.1676892  3.9818314  3.7614201  2.8182319\n131  3.7769678  4.4301765  1.9536676  6.7244848  8.2483933  6.1775755\n132  3.9083968  4.3592450  1.9570235  5.9514088  7.7166357  5.4103690\n133  2.7056843  3.4763299  3.3554735  4.0347178  5.9127322  3.3980178\n134  6.7192720  6.1683263  7.8228029  3.6960649  5.0167933  3.4161066\n135  4.6647747  4.1839789  5.8031293  2.3109833  6.0840683  1.8486594\n136  4.3303274  4.0698832  2.4339043  5.6883555  7.3244826  5.0392856\n137  3.5086019  4.3391053  0.9815083  6.2368959  7.9261949  5.6751773\n138  5.2896212  4.1133612  6.1607261  3.7928375  4.3290419  2.8371694\n139  6.7526682  6.2639439  7.1041937  4.2586862  2.7181188  3.4972641\n140  3.8036485  4.1081861  2.9152370  4.7085208  5.8494876  3.9426985\n141  5.1661947  4.6079162  6.2963369  3.6304729  4.4095050  2.7468360\n142  6.6126282  6.4066413  7.4822203  4.2085459  2.7153542  3.4316970\n143  3.6252100  5.0916337  5.8653356  2.3041359  5.8081639  2.3242864\n144  2.1508839  3.4355163  3.5569074  3.6717850  6.1226322  2.9803498\n145  3.6398593  3.0556273  4.2108147  3.3952725  5.1146724  2.3954956\n146  3.7335708  3.9393342  3.6660735  3.9347891  6.1812953  3.5295210\n147  7.5606362  7.0942712  7.6601576  5.5272045  1.6868943  4.6745937\n148  4.4627673  1.5170722  4.5947814  5.8969097  6.7436923  4.8001620\n149  6.2082951  6.6611319  6.9841028  4.3964839  7.2301215  4.7213953\n150  4.1469192  4.2492742  1.4687270  6.2962153  7.0500742  5.5486681\n151  4.7444908  4.2637445  3.4296563  5.5769147  7.5457628  5.1427825\n152  3.4323485  3.2062971  3.1481531  3.9974470  6.1603372  3.1806372\n153  5.5144442  3.6462088  6.1628388  4.6810551  5.1116529  3.7187102\n154  4.9272558  4.4475926  4.8601233  3.3869804  4.0990589  2.3557714\n155  4.2678223  3.7295083  4.1584290  4.6147886  4.6197655  3.5923811\n156  7.1840893  6.1636747  7.8232726  4.7530821  3.0641723  3.8578642\n157  6.7760298  5.7472449  7.0304481  4.2091968  2.6503853  3.2324107\n158  7.1644036  5.8133038  7.2269118  5.2335560  1.6722885  4.0075658\n            19         20         21         22         23         24\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20   3.6770155                                                       \n21   1.9138153  4.4899509                                            \n22   2.3886075  3.2891837  3.5539077                                 \n23   3.9631189  3.7664828  3.7560580  4.1444772                      \n24   4.5780743  1.8582925  5.0282351  3.9451577  4.7063227           \n25   3.9902866  6.7095280  4.1232343  5.3642545  6.4521328  7.2655533\n26   4.3658075  6.3029886  4.8602842  5.4712228  6.1105363  7.0590009\n27   2.3958643  4.5725221  3.0036234  3.5147501  4.7438809  5.2623107\n28   2.7817933  5.4162587  3.2141006  4.0330050  5.0622597  6.1124593\n29   4.3501726  3.9903355  4.9579830  3.2966011  4.7982597  4.4132293\n30   3.1517431  4.4042393  3.1273016  4.2883526  4.1989551  4.9064591\n31   3.7651717  6.2261807  3.8301150  5.5511570  6.4827395  6.8140686\n32   2.8868035  2.8261589  3.8047222  1.9626179  3.4122313  3.7557914\n33   4.3564166  3.7094394  5.4036163  2.2625826  5.1780080  3.9071206\n34   2.2114058  3.3566158  3.3527974  1.2716734  3.6177336  4.2721867\n35   4.2135341  5.3181302  4.5789585  5.3069935  4.7261991  6.2871879\n36   2.8296443  2.7987951  3.8674884  2.6082264  3.0229157  4.1122847\n37   4.2936110  1.7169427  4.8365973  3.5826558  4.1587509  1.4370134\n38   4.1750086  2.5298757  4.7432174  3.7402157  2.8752915  3.4532643\n39   4.6839253  2.1876749  5.2672613  3.5596955  4.0807270  1.9506242\n40   4.7257927  3.2004419  5.4339363  3.7949398  4.0630332  3.6546036\n41   2.4492621  4.4651592  2.6180472  4.1775499  4.3687897  5.3213888\n42   2.5377165  2.8700390  3.4518205  2.6234072  3.0395842  3.8178449\n43   1.9514668  3.3935005  3.0264868  1.9950777  3.5628427  4.2685008\n44   3.6527146  4.1569929  4.4783244  3.5101743  4.4101062  5.0366632\n45   2.7010615  3.2317792  3.9208394  3.0108614  3.5940572  4.4036233\n46   3.0720770  5.0898842  2.6025857  4.3519470  3.5792029  5.7582151\n47   2.8987584  5.3340875  3.2945174  4.6463519  5.2825139  6.1945290\n48   4.3499109  2.3941800  4.6184879  4.0341373  4.2658804  2.0406451\n49   3.2991480  5.1066156  2.8222437  4.7656596  4.4877128  5.5104625\n50   3.3588957  5.9590658  4.3886863  4.1798571  5.8761573  6.9053051\n51   3.0247751  3.2256928  3.3227825  4.0074398  3.1347789  4.2697434\n52   4.4338342  2.9861552  4.9565263  3.6564374  4.3256122  3.2053011\n53   5.2738895  4.0300867  6.0075756  3.7435472  5.1736219  3.9882471\n54   2.7141010  4.1829644  1.9858065  3.7476068  3.6102825  4.4879598\n55   4.4566116  6.4848120  4.3165559  5.8425139  6.1910001  6.8885952\n56   3.5002532  1.4655971  4.1720571  3.5529384  4.1698068  1.8932007\n57   5.7761517  3.5722751  6.5359111  4.0856638  5.1296323  3.3653189\n58   2.8965447  5.1724018  3.3709121  3.9138644  4.9866013  5.7875188\n59   4.6323536  2.6286751  5.3131727  3.5992124  4.6324758  2.6003820\n60   3.8870371  3.0159943  4.1868803  4.4680309  2.6065923  3.9812762\n61   2.4891657  4.3495044  3.6736075  3.0723162  3.8368299  5.5821902\n62   3.7508274  6.3703880  3.8992580  5.2728788  6.3205739  6.9171770\n63   2.4144687  4.0222770  2.2600107  4.0976557  3.8304521  4.9951792\n64   3.7650978  4.9683753  4.5091002  4.7119257  5.4628465  5.7170748\n65   1.9750080  3.9858697  3.0034388  3.0600335  3.9868284  5.0922534\n66   4.4403201  1.7003276  5.0279101  3.4497634  4.0537007  1.3019787\n67   4.5527876  4.1378067  4.5502814  4.6083218  4.2544671  4.5681262\n68   4.2139676  5.2232780  5.4809857  3.4999025  5.7977833  5.9660322\n69   2.9679035  4.1455946  3.8794095  2.4949004  4.6212617  4.6781308\n70   2.4385304  3.2535357  3.2901663  1.5684009  3.3487175  4.1269772\n71   2.5167899  4.6943873  3.0478760  2.9998194  3.7158395  5.5156237\n72   4.6956812  3.8340110  4.9718921  3.9669141  4.1004952  3.9038308\n73   4.5973918  3.8651690  5.3502106  3.7153584  3.8164250  4.5007331\n74   5.4048662  3.7581217  6.2842609  3.9184689  5.3484021  3.8554201\n75   2.9660595  2.4032807  4.0823986  3.1342181  3.1556039  3.7923093\n76   6.8695880  4.9862608  7.6006391  5.1698746  6.3864516  4.6782803\n77   2.4016374  3.9618503  3.2126621  2.9607716  3.1233004  5.1320018\n78   3.3044849  3.2874684  3.1079231  3.2022923  3.9890289  3.1207869\n79   3.2093351  5.7384014  3.4262172  4.0853492  5.4756910  6.1872548\n80   4.6546055  4.9683957  4.9848418  5.7955471  4.5010567  6.1996799\n81   4.6261235  5.0415435  4.6286220  4.3757007  2.1693998  6.0565727\n82   2.6997615  3.5848852  2.7932160  3.5201194  4.1361795  4.0365800\n83   2.2667847  4.4492360  2.1483952  3.6988304  4.4722789  4.9221351\n84   4.3393627  2.0159691  4.6194674  4.0414252  4.3814159  1.2094600\n85   3.8700947  2.5784459  4.7661815  3.4531825  3.1457886  3.7538043\n86   3.5804616  6.1133269  3.6630210  5.1886474  6.1315138  6.6621748\n87   2.4686925  3.7093862  2.0910253  3.1798424  3.4214794  4.3375354\n88   4.6019661  2.4537279  4.8107702  4.1600615  4.5756305  1.3114333\n89   5.4159289  3.8648119  5.6041459  4.8397949  3.8189758  3.9729829\n90   3.5701460  6.1474946  3.9689633  4.9743551  6.2037930  6.8601137\n91   3.6650250  5.8957094  4.1076688  4.7429043  5.8029826  6.4966811\n92   3.1885326  3.2036557  4.0163685  2.5423674  2.7150314  4.3486342\n93   4.1701272  7.2683132  4.2592461  5.8002127  6.9923231  7.9011671\n94   5.9019900  3.4218851  6.5196655  5.2311093  4.2118455  3.9333364\n95   4.8381608  2.9922737  5.6056571  4.4574264  3.1034717  4.2638302\n96   2.6558743  3.6423875  3.9240171  1.5632198  3.6115048  4.7122206\n97   3.6300716  2.0843731  4.0719778  3.6880305  4.2229358  2.0679558\n98   3.4914382  4.5745286  2.4140028  4.3618345  4.6577218  4.5145091\n99   3.8091021  1.1390878  4.2528555  3.8836818  3.3709723  2.1130856\n100  2.4540318  3.1275630  3.4533334  2.6058597  4.1558252  3.9666599\n101  3.5328872  6.2153770  3.8152121  4.9438464  6.3397706  6.7306773\n102  2.2396740  4.3706543  3.1965506  2.8286458  4.7549910  5.0265260\n103  2.8357758  4.6036744  4.0710769  3.6441112  5.0038862  5.6469297\n104  5.4406631  3.8201478  6.1742813  4.1936640  4.6732584  4.0144548\n105  3.9061176  2.9996880  4.5161032  3.2270694  3.6571149  3.7092927\n106  1.9610001  3.8150716  3.2144809  2.8214299  3.5555474  5.1882205\n107  4.0437874  7.1392393  4.1720328  5.6171377  6.9617521  7.7454943\n108  3.4662685  0.7600620  4.1147760  3.2662788  3.2677585  1.9915400\n109  4.6120568  3.7258700  4.9353832  3.8500180  4.2108401  3.9202170\n110  3.2410988  4.3325463  2.8994193  3.5950156  2.3087077  5.3221921\n111  3.6675066  5.1109948  4.6271198  3.8854200  5.5066689  5.8156964\n112  2.5777612  2.9525995  3.2840277  2.4610815  2.5309568  3.9910605\n113  0.8114238  3.3033496  2.0340326  2.3180865  3.2892721  4.3904917\n114  2.2658552  4.0296632  3.2499785  1.9959004  3.9810834  4.8531623\n115  3.2716351  4.3480101  4.1452186  3.2341029  4.5833104  5.0002884\n116  4.4055402  2.4026267  5.0580944  3.0223758  4.2954253  2.0296760\n117  4.7461023  2.5928054  5.6736397  3.5661482  4.2906200  3.3149015\n118  5.6734131  6.1561421  5.4102114  5.5426084  3.0146599  7.1395094\n119  4.4069736  1.5796686  5.0668302  3.3702403  4.2178009  1.2875548\n120  4.1939252  3.2431916  4.5370673  3.1707745  5.1714901  2.4300449\n121  3.7542664  2.7114998  4.4663648  4.2332657  2.8465669  4.1680216\n122  3.7826300  2.9247590  4.1871555  4.4770630  2.7406413  4.1050671\n123  4.1059358  5.4299329  3.7884311  5.4494811  4.9565049  5.8388208\n124  3.6757910  4.3106246  3.7280199  3.2126639  2.8014621  5.1521316\n125  4.0552051  2.5980838  4.2417110  4.4005466  2.2243690  3.5385845\n126  7.0485671  5.4167110  7.7426251  5.9894608  5.0389466  6.0262687\n127  4.1340748  1.5839770  4.6116837  3.3337798  3.4912392  1.5383041\n128  4.7279344  2.4711515  5.3207108  3.9340234  3.9107330  2.8672179\n129  3.7466634  4.9116770  4.0355885  5.1118112  4.5865870  6.1328016\n130  2.0686877  4.1007078  2.6282398  2.3337849  4.0829953  4.5051987\n131  6.0906466  4.4642939  6.6830958  4.5086056  5.1589427  4.3961662\n132  4.9408816  3.5122463  5.8532819  3.4461696  4.8162917  3.9440402\n133  3.4776982  2.4096219  4.4926004  2.5326539  3.3509641  3.3688925\n134  3.3554067  5.3736725  3.9925988  4.3299124  5.2275324  6.3410156\n135  2.5582618  3.3720391  2.4658070  3.7219034  3.1877152  4.3230080\n136  4.4743309  3.4355055  5.0771339  3.6184796  4.5201284  3.6985055\n137  5.4979254  3.8377087  6.0882739  4.3296242  4.5606489  3.9996285\n138  3.1204095  4.1801686  3.2017843  3.8368089  4.3716249  4.6262342\n139  2.9135679  5.7300769  3.5850939  4.0106093  5.4813566  6.4849022\n140  3.4038952  3.1295818  4.5268855  1.9561900  4.1033533  3.8652829\n141  3.5251061  4.6561619  3.2506012  4.5566369  4.0631585  5.1128486\n142  3.5112997  5.7979974  3.9076346  4.8779257  5.4753641  6.5927013\n143  3.6457651  3.8774194  4.0898551  4.7467485  3.2110939  5.1152178\n144  3.6282659  2.1473734  4.1972370  3.4400508  2.2882519  3.1751409\n145  2.3340017  2.0071821  3.2377897  2.2062304  3.2026952  3.0371399\n146  3.1861098  3.0314649  4.2000102  1.6886272  3.2314691  3.9256678\n147  4.2319530  6.7803640  4.5389034  5.2588240  6.3914973  7.3379800\n148  4.5689892  2.5130380  5.1786338  3.8031018  5.2773332  1.6264563\n149  5.3878687  5.9507226  5.0858096  5.2683002  3.4186840  6.8510237\n150  4.9509615  3.7854602  5.8033768  3.5562147  4.9211349  3.8892713\n151  4.3772547  3.6907620  5.3032379  2.4916495  4.5356210  4.0679242\n152  2.8737599  2.1662723  3.6127238  2.6895144  3.4775786  2.9531006\n153  3.7101197  3.8875255  3.9185935  3.8708196  4.6777850  4.0788220\n154  1.3891962  3.5522557  2.5138928  1.9666588  3.5925910  4.4229278\n155  3.1464245  3.2439138  4.2021421  2.3060735  4.3776194  3.7819946\n156  3.5948426  5.7838451  3.9177354  4.7997284  5.9452452  6.4124546\n157  2.8610960  5.7112053  2.6696152  4.2104435  5.2305272  6.1775865\n158  3.5558887  5.9746132  3.3924487  4.8516024  6.0063271  6.2049781\n            25         26         27         28         29         30\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26   2.2028507                                                       \n27   2.2109519  2.5398991                                            \n28   2.3406901  2.1932085  2.0172679                                 \n29   7.9626049  8.3192763  6.1045019  6.8318026                      \n30   3.7658414  3.9411491  2.4460311  3.6017016  5.8605604           \n31   2.9584319  3.3066903  3.3133615  2.5090216  7.9663773  4.6690732\n32   6.3760781  6.4433226  4.3934811  5.1476678  2.0434664  4.2350614\n33   6.5275531  6.1933750  4.7016133  5.1939904  4.5569442  5.5653039\n34   5.1236048  5.1890558  3.2326138  3.9448179  3.3051602  3.5562303\n35   4.0477238  2.4014991  3.3199662  2.8196495  7.7453995  3.7025102\n36   5.7761566  5.5925480  3.7481353  4.7216431  3.4341653  3.5089565\n37   7.2585338  7.1236777  5.1683811  6.1650520  3.4495293  4.4863519\n38   7.2638008  6.9480485  5.2088605  6.0482267  3.3576630  4.2696016\n39   7.6303005  7.3791858  5.5227721  6.4132025  3.4163712  4.9242997\n40   7.7619687  7.6952996  5.7311148  6.7338069  2.7772841  4.9878385\n41   2.9453919  2.8236587  1.8410993  2.1224694  6.3670944  2.3851751\n42   4.5830443  4.1049513  2.5510759  3.3466525  4.8305399  2.9515105\n43   4.5286722  4.6353093  2.6142993  3.5910851  3.8551759  2.6441003\n44   5.2147566  4.4558560  3.7542814  3.5676890  6.1715917  5.2268460\n45   4.5655863  3.8998543  2.6724367  3.3867842  5.0896950  2.8489323\n46   3.7377253  3.6560796  3.0011724  2.3089493  6.4168204  3.2142058\n47   2.1155761  2.0773821  1.7996887  1.7406869  7.1286758  3.2170796\n48   7.3747407  7.4914677  5.3871184  6.4437215  3.3240802  4.4770365\n49   2.8410886  2.9069973  2.2725301  2.2711931  6.9845710  2.4751065\n50   2.4812461  2.3750270  2.2935732  2.0381815  7.0432122  4.2863800\n51   5.1854558  4.4947228  3.5967535  3.5065703  5.8189211  3.9914750\n52   7.8640334  8.0390342  5.8516555  6.8182366  1.7997154  5.1857758\n53   8.1046933  8.1056492  6.1698933  7.0846001  2.9112664  5.5234317\n54   4.9098689  5.0054797  3.7075799  3.3436350  5.3146091  3.5119773\n55   1.7674365  2.1572168  2.5216651  2.8885847  8.2866586  3.0973349\n56   6.1004276  5.8475551  4.0910477  5.0161195  4.6634882  4.1552684\n57   8.7030995  8.4021875  6.6388419  7.4191056  3.5948254  6.2191713\n58   1.9551091  1.9618385  1.1857209  1.2999101  6.7412094  3.1411070\n59   7.5968132  7.5976633  5.5171219  6.6715308  2.8775014  4.8244132\n60   5.9321278  5.1047237  4.1693794  4.5997458  5.6446627  3.2530539\n61   3.9182952  3.4488327  2.3917703  2.7736345  5.4534713  3.3459015\n62   0.8270326  2.4680054  2.0556566  2.4594316  7.7661751  3.6885709\n63   4.8924013  4.9102041  3.6345830  3.5055047  5.5857333  4.1397613\n64   2.9313481  1.8376812  2.0733973  2.4963390  7.4677504  3.6718232\n65   3.2835307  3.1092333  1.4346385  2.4732668  5.4837603  2.7139670\n66   7.3666657  7.0605140  5.2717200  6.0639279  3.7192716  4.8179832\n67   7.9945296  8.3829047  6.2257662  7.0564538  2.5040446  5.1983338\n68   4.6798473  4.1353654  3.5141523  3.7412880  6.5191284  5.3767982\n69   3.9681904  3.9143198  2.3271551  3.0646542  5.4684315  4.0134003\n70   4.9900409  4.9107504  3.0735328  3.6722475  4.1098307  4.0539431\n71   3.5405016  3.3213380  2.2959758  2.0290181  5.7067721  3.4782167\n72   7.4944479  7.7538456  5.6048741  6.7238761  2.8216360  4.6232322\n73   7.1435628  6.8771898  5.2396035  6.1393848  3.7482049  4.4228349\n74   8.1503980  8.1231799  6.1329225  7.2749148  3.1677892  5.6501511\n75   5.3549831  4.5990508  3.3417587  4.0100803  4.8208484  3.3062347\n76   9.5674407  9.3269863  7.6171807  8.4519346  4.3695348  6.9165355\n77   4.2234970  3.6205772  2.5882551  2.6917841  5.3417400  3.3261515\n78   6.0570821  6.1939509  4.4134726  4.6686191  4.4287959  4.6068402\n79   1.7673040  2.4805819  1.6071321  1.8784306  6.7949700  3.0010164\n80   5.7061485  4.3643314  4.6388799  4.2343413  7.7334072  5.0263640\n81   6.3562630  5.8386683  4.9055688  4.9912959  5.6695103  4.8683808\n82   4.2678876  3.9608750  2.7465822  2.9403442  5.7894898  3.2568474\n83   2.6637650  3.1736191  1.4196326  2.0944899  6.0619487  2.5930206\n84   7.2828658  7.2869224  5.3067881  6.2198694  3.8670673  4.6584100\n85   6.0048014  5.0742193  3.9816898  4.6614280  5.1455906  3.9375370\n86   1.2562013  2.3132386  1.9892866  2.3594737  7.5977368  3.0579343\n87   5.3210651  5.4456459  3.8476134  3.7674066  4.6972926  4.2740024\n88   7.3493924  7.4024836  5.4018560  6.3843680  3.9877676  4.6471566\n89   8.0457625  7.9866707  6.1254770  7.1622762  3.9380531  4.7982799\n90   1.5889022  2.5318953  2.0462802  2.4845816  7.5787378  4.1086920\n91   1.6964635  1.3322248  1.8499676  1.7807637  7.5116572  3.1302983\n92   5.8625745  5.3073788  3.9956742  4.1730543  4.4887127  4.6549313\n93   1.5261966  3.3933580  3.0629330  3.0486224  8.1328682  4.6310565\n94   8.3493391  7.7123065  6.3004239  7.2966695  5.0799905  5.2608042\n95   7.3093001  6.2322606  5.3721877  5.6637718  5.5070853  5.1860745\n96   5.4057079  5.1260172  3.6307148  3.8311462  3.9797126  4.4689531\n97   5.6590071  5.2214806  3.7409628  4.5527927  5.2389414  3.4402772\n98   5.3931803  5.8357736  4.2873447  4.3336708  5.6676943  4.3227895\n99   6.8582606  6.4950850  4.8111176  5.5590729  4.2801298  4.3761627\n100  4.2501774  4.0912711  2.2556683  3.3564633  5.1000986  3.5446192\n101  1.1159414  2.2280600  2.0633668  1.8632143  7.6434532  3.8476110\n102  2.8381224  3.1283714  1.2264408  2.1596471  5.5713799  2.9281026\n103  3.0532514  2.6303869  1.6460973  2.5371781  6.3038934  3.5093085\n104  8.0370096  7.8618562  6.0359515  7.1114984  3.6635327  5.2990466\n105  7.4708346  7.5616488  5.4768623  6.3012788  1.6265575  4.8986427\n106  4.3856214  4.0375964  2.7347594  3.0304163  4.8480233  3.3133952\n107  1.2289074  3.0346765  2.8622516  2.7465202  8.0526464  4.3356543\n108  6.3738228  5.8927719  4.2738297  4.9925001  4.2385001  3.9818435\n109  7.9864033  8.2563130  6.0749668  6.9516119  1.6636492  5.2709388\n110  5.6186527  5.6123488  4.1216113  4.3167146  4.8658921  4.4492119\n111  3.3948160  2.8247846  2.4455715  2.5894438  6.9585110  4.5630971\n112  5.6986120  5.5923808  3.7705064  4.4452296  3.2676473  3.2518589\n113  4.3052513  4.4154902  2.5185560  2.9961926  4.1843836  2.9907031\n114  5.1052654  5.4093748  3.4704075  4.0881887  3.3258890  3.4775261\n115  3.7293122  3.2241058  2.1834207  2.8354434  6.1334587  3.8089399\n116  7.2167407  7.0417724  5.1487204  6.0319022  3.3630774  4.7747391\n117  7.9638572  7.6920887  5.8396645  6.7388829  2.8138807  5.3489701\n118  7.0636858  6.6889840  5.8133917  6.0116951  6.5757923  5.8311817\n119  7.3242152  6.9320178  5.2451765  5.8998596  4.0648643  5.1597995\n120  6.9914766  7.1326829  5.2407081  5.6739260  4.0735612  5.5519398\n121  6.0892088  5.4009160  4.1731742  4.9668132  5.1302019  3.5661696\n122  5.8060738  4.9534221  4.0254036  4.5236479  5.7131719  3.3118079\n123  3.5362147  3.3914029  2.9404314  3.5760432  7.3292000  1.7524713\n124  6.3588617  6.1686175  4.7972755  4.6430106  4.6207074  5.3896038\n125  6.4645274  5.8526862  4.5829256  5.1632832  5.1160550  3.6440333\n126  9.0293207  8.3344491  7.1746414  8.0546519  6.0620487  6.4892867\n127  7.0192172  6.7346843  4.9131278  5.7643263  3.7026856  4.3851859\n128  7.8751040  7.6783060  5.7723534  6.7352220  3.0258279  4.8870588\n129  5.0094380  4.2182502  4.0242189  3.6298488  7.1219373  4.8281731\n130  3.6770802  3.8754839  2.2116911  2.2478448  5.0152271  3.1428340\n131  8.6028185  8.2277062  6.6731306  7.4141811  4.3243328  5.8991195\n132  8.0352574  7.9874996  5.9968886  6.9703028  2.3700582  5.5902790\n133  5.9051519  5.2083147  3.8322635  4.4809904  4.3730363  3.8648572\n134  3.9652186  3.4742947  3.3230850  2.3703434  7.0561044  5.1347174\n135  5.8252299  5.7817402  4.2841284  4.2418377  4.5666837  4.2093259\n136  7.7493582  8.0162518  5.7839801  6.8440882  1.7677402  5.1744982\n137  8.3808000  8.2783858  6.3692887  7.3780588  3.1808587  5.4578884\n138  3.9623229  3.4374776  2.6586093  2.6581145  6.3480172  3.3028134\n139  1.8366167  2.1028433  1.7337214  1.1664090  6.8580123  3.6494129\n140  6.0273372  5.9092083  4.0574368  4.9315918  3.2883366  4.1171874\n141  4.1381696  3.5490517  3.1258780  2.8833369  6.7292537  2.9494363\n142  1.8115572  1.0728421  1.9119986  1.4239030  7.6080590  3.4797354\n143  5.5407665  4.5316847  4.0898042  4.0103673  6.2300092  3.6615141\n144  6.1744656  5.5621227  4.1567476  4.7651706  4.5451401  3.7170061\n145  5.0240716  4.7090204  2.8834920  3.7492976  4.2635625  3.2923383\n146  6.0587214  5.6921030  4.1012306  4.5105553  3.5762907  4.4603650\n147  1.1984813  2.0327510  2.3327909  2.4936025  7.9471455  3.7299895\n148  6.8671054  6.4890463  4.9991900  5.5836607  5.2054940  5.2021069\n149  6.7003637  6.3454089  5.4818745  5.5776020  6.6860448  5.9788796\n150  7.5304467  7.5406028  5.5769621  6.6855304  3.2651605  5.1387252\n151  7.6135000  7.6066172  5.6971841  6.2205071  2.3938746  5.9590570\n152  6.4003458  6.4740655  4.4105965  5.2202327  2.4934359  3.8473831\n153  4.7879158  4.3670807  3.3279175  3.6401830  6.3373798  4.3256644\n154  3.9871610  4.2850558  2.1491065  2.9312778  4.2669999  3.1238669\n155  4.6531409  4.2271446  2.6776792  3.6587107  4.9927248  3.3823739\n156  2.0677668  2.2002479  2.0902340  2.0741644  7.6188335  4.1303536\n157  2.1480553  2.8662652  2.1132816  1.4748572  6.7363673  3.1685918\n158  1.7317559  2.8428750  2.1148734  2.3323330  7.3176897  3.0799477\n            31         32         33         34         35         36\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32   6.5420746                                                       \n33   6.7742960  3.5155891                                            \n34   5.6296324  1.5247835  3.1008613                                 \n35   3.9709848  5.7952212  6.0374981  4.8747160                      \n36   6.2001882  1.5506664  3.9574511  1.6913446  4.8704613           \n37   7.1363755  2.7859605  3.9392326  3.5724104  6.2897666  3.1572708\n38   7.3943204  2.1883938  4.5648461  3.1273798  5.7670923  2.0140082\n39   7.6633970  2.7682299  3.5651464  3.5793695  6.4921980  3.2230418\n40   8.1657064  2.3065450  4.4386149  3.3424523  6.9097226  2.5974912\n41   2.6820263  4.6634459  5.5468592  3.7862275  2.5351203  4.0054803\n42   4.9825953  2.9216967  3.4475029  2.2905183  3.5394279  2.1551199\n43   5.2232329  2.0201103  3.6620083  1.0410164  4.3907140  1.6165615\n44   4.7773126  4.5978849  3.6326367  3.9672634  4.0661121  4.3105680\n45   5.0397050  3.1151992  3.9743164  2.3355528  3.2652466  2.0694867\n46   3.7095759  4.8163683  5.6799563  3.9955413  3.0210232  4.3919607\n47   2.0380276  5.4210648  5.9206095  4.3871709  2.7346612  4.7324538\n48   7.2322326  3.0054888  4.7786704  3.9348894  6.7069327  3.4478767\n49   3.0542254  5.3461022  5.8815080  4.4853953  2.8600536  4.7866898\n50   3.6648984  5.3431732  5.2528303  3.9709734  3.6896589  4.7213996\n51   4.1852404  4.2105074  4.9200733  3.9507409  3.1297416  3.7322496\n52   7.8794955  2.1541271  4.5895263  3.4996414  7.2899172  3.0586972\n53   8.6365266  2.8359024  3.9478276  3.5472434  7.5384670  3.6009798\n54   4.0802081  4.0829153  5.1034050  3.7394946  4.1590459  4.2744520\n55   3.3752551  6.5959378  6.8024843  5.4941266  3.4629339  5.8583901\n56   5.5760111  3.5127224  3.9977634  3.7646201  5.1689723  3.3897937\n57   8.9222674  3.4143846  3.5565418  4.2080358  7.6066897  4.0770542\n58   3.0845631  5.0047913  4.8891114  3.7789531  3.0211921  4.4169089\n59   7.8308941  2.5338751  3.9962699  3.4525651  6.9496724  3.0404798\n60   5.7517026  3.9346720  5.2130404  3.9139988  3.4247453  3.1088796\n61   4.6741027  3.5267376  4.3521143  2.4115175  3.2533979  2.6042609\n62   2.6492982  6.2264841  6.4727834  5.0886102  4.1316176  5.6240374\n63   3.4700694  4.3065805  5.6283530  4.0733943  4.0688385  4.1440808\n64   3.3439305  5.6390188  5.2533715  4.5984016  2.6506173  4.7790784\n65   3.8720718  3.6295229  4.4040305  2.5938744  3.1484285  2.7940320\n66   7.1714275  2.9475089  3.4157902  3.6155499  6.1475955  3.3630583\n67   7.8801781  2.9405008  6.1103533  4.1743329  7.4955092  3.6480578\n68   5.5099603  4.9503683  3.3280997  3.8141299  4.8418082  4.5386447\n69   4.7250004  3.8965683  2.9187049  2.8391336  4.4080137  3.6164820\n70   5.2538562  2.5144581  2.6865599  1.8508975  4.6283947  2.5073581\n71   4.1931203  3.9370578  4.1466171  2.7888531  3.2919232  3.5079058\n72   8.0978287  2.6007069  4.8457882  3.5008657  7.1538081  3.0479989\n73   7.9168103  2.5370608  4.3057158  2.9634767  6.1013158  2.3142612\n74   8.7310419  3.0127923  3.9716412  3.7374781  7.6457578  3.4774924\n75   5.4113986  2.9050532  3.9294891  2.6062313  3.5389299  1.8930756\n76  10.0142960  4.4495354  4.6802534  5.1127441  8.6105766  5.1075323\n77   4.5284933  3.4060118  4.1431573  2.4893222  2.8872819  2.6832460\n78   5.3094868  3.6410328  3.9793072  3.7686703  5.5765017  4.2496932\n79   3.5217440  5.1760729  5.1683885  3.8618157  3.6880522  4.7515507\n80   4.6142894  5.9749377  6.5169149  5.5392715  2.4735042  5.1436273\n81   6.9069214  4.1532931  5.1527880  3.7880081  4.6893496  3.6436564\n82   3.5601904  4.2414936  4.3665090  3.6537694  3.3692893  4.0540547\n83   2.7787471  4.5208726  4.9639433  3.6305097  3.5105476  4.1481030\n84   6.8618162  3.4146151  4.5543763  4.1712694  6.4743612  3.8304517\n85   6.1788152  3.3021299  3.5789168  3.1151943  3.9368911  2.4615261\n86   2.5342375  6.0078413  6.4442682  4.8859698  3.6274734  5.4074458\n87   4.3551326  3.5944347  4.5804695  3.4352079  4.6902209  3.9202506\n88   7.1304014  3.5796456  4.5392822  4.2783332  6.6803080  4.0152166\n89   8.5532354  3.4191541  5.4279427  4.2710983  7.0344142  3.4466415\n90   2.7110643  6.0160896  6.1079309  4.8930482  4.1069733  5.4017151\n91   3.1995753  5.7335218  5.6630587  4.4370862  2.8485729  5.0655618\n92   5.8167915  2.8388605  3.1694578  2.5899197  4.4200768  2.6227818\n93   2.9614228  6.7550379  7.2325956  5.6190702  5.0436973  6.2663606\n94   8.7043891  4.0645247  5.2378941  4.7101898  6.4975681  3.5694484\n95   7.0713585  3.9581334  4.5549819  4.1458809  4.5970986  3.3153176\n96   5.6870465  2.3106623  2.8048312  1.5283155  4.7087986  2.3264849\n97   5.2598782  3.8637223  3.9792553  3.7615073  4.4226076  3.6729534\n98   4.3330138  4.8195479  5.5365449  4.6365548  5.3356238  5.1887732\n99   6.2050346  3.2120966  4.5174148  3.8594231  5.3123487  3.1434628\n100  4.4165464  3.4636976  3.3608887  2.7840077  4.1196952  3.0108215\n101  2.1076352  6.0931488  6.0622480  4.8983266  3.8608582  5.6326764\n102  3.7695209  3.9352405  4.0436446  2.6542148  3.7938316  3.5039268\n103  3.9202636  4.4931708  4.6015642  3.3602299  3.3313681  3.6029489\n104  8.7379698  3.1016719  4.3034901  3.7778923  7.1919850  3.2229214\n105  7.5087786  1.3933884  4.4466382  2.8766167  6.7164610  2.3359110\n106  4.5422352  2.9877702  4.4284823  2.1002256  3.4168422  2.2427378\n107  2.7517876  6.6181137  7.0069023  5.4181082  4.7008657  6.1604880\n108  5.8912176  2.8748752  3.7634429  3.2452164  4.7650243  2.7394131\n109  8.1552042  2.2491393  4.9360625  3.5651760  7.5030104  3.2448817\n110  5.5521053  3.5580784  4.9263017  3.3742728  4.7563977  3.4331450\n111  4.0170985  5.2595196  4.1744177  4.1110380  3.7887214  4.6982116\n112  5.9883312  1.3968413  3.9875257  1.5708978  4.7298336  1.1574665\n113  4.1677152  2.5101549  4.2078176  1.9144231  3.9312658  2.2610834\n114  5.7158365  1.8608556  3.9587000  1.0261493  5.1726546  2.1252361\n115  4.5818498  4.3677967  3.5434318  3.3066615  3.6631943  3.7651459\n116  7.3593183  2.6382702  2.8922034  3.2033620  6.3704318  3.2222899\n117  8.0667359  2.2707326  3.9263512  3.3181637  6.7870888  2.6703897\n118  7.7504198  5.2637234  6.2659149  5.0178068  5.6995308  4.7347920\n119  6.8969700  3.2524782  3.0860271  3.7899903  6.0381356  3.6713034\n120  6.3860254  3.7853950  3.2385642  4.0677780  6.7350967  4.7015591\n121  6.0955344  3.4292276  5.1275549  3.5400507  4.0562178  2.2526576\n122  5.5540795  3.9753960  5.2809460  3.9128545  3.2875765  3.0261366\n123  4.2979291  5.6401172  6.4915988  4.8801572  3.1619065  4.8734119\n124  6.0838354  3.5112620  4.1351596  3.4118755  5.2816719  3.8079704\n125  6.2526498  3.6215300  5.1508171  3.8938262  4.2991283  2.9689499\n126  9.9638221  5.0659165  5.7380334  5.3812507  7.3656502  4.4886826\n127  6.9091328  2.7231933  3.5245713  3.3678852  5.7874406  2.9784032\n128  7.9554356  2.4917841  4.4581361  3.5952801  6.6655054  2.8048464\n129  3.6658415  5.5000394  6.1505330  5.0004912  3.0939462  4.9021718\n130  4.0658680  3.4876071  3.6229225  2.3924883  3.9591031  3.4805367\n131  9.1541457  3.8113587  4.1420093  4.2684280  7.3657394  4.2462138\n132  8.4460448  2.2578037  3.8300665  3.2235056  7.3657091  2.9560441\n133  6.1584067  2.5576127  2.6963344  2.2688182  4.3157102  2.1301600\n134  2.9811204  5.4837131  5.2276773  4.5732030  3.5669968  5.1190290\n135  4.6414326  3.4703241  5.2927750  3.6345408  4.6196832  3.6202747\n136  8.0263904  2.1302265  4.5965173  3.3772318  7.4391322  2.9618258\n137  8.9040884  2.9519177  4.6535570  3.9116919  7.4668364  3.3408965\n138  3.5595913  4.6884159  4.5039270  3.9050293  2.9453364  4.3762886\n139  3.0038971  5.1754525  5.1970994  3.8424683  3.3611438  4.6718901\n140  6.6212756  1.8269007  2.7779683  1.5084186  5.5320231  1.9804206\n141  3.8298113  5.0330840  5.4096771  4.3301143  2.5758562  4.5909344\n142  2.6494471  5.8017271  5.9027441  4.5968269  2.3654146  5.0393979\n143  4.9783051  4.4482758  5.7647352  4.1993938  2.5873062  3.5951836\n144  6.1718322  2.9012266  4.0028555  3.0471618  4.3078455  2.3074613\n145  4.9413835  2.5260942  3.0933918  2.2140035  4.0776532  2.1947431\n146  6.3086873  1.9173136  2.3496967  1.6982119  5.0162142  2.1435100\n147  3.8483119  6.2718026  6.2467556  4.9294669  3.9364648  5.6067825\n148  6.2481883  4.3256613  3.2871066  4.4275512  5.9010600  4.6548926\n149  7.1055736  5.3878115  5.8579332  5.0568940  5.5148432  4.9997477\n150  8.2829407  2.7687610  3.7419355  3.3101538  7.1580378  3.1123190\n151  7.7730597  2.2382814  2.8109211  2.9261056  7.1071869  3.3233962\n152  6.2461831  1.4151140  4.1075667  2.3660282  5.6890624  2.0147792\n153  4.3108921  4.8775548  3.9830714  4.3164227  4.1231918  4.7008830\n154  4.4315845  2.6427212  3.7125750  1.7127876  4.2919724  2.3732445\n155  5.3262431  3.2596839  2.4405680  2.3113263  4.1844858  2.8855976\n156  2.2902423  5.9885184  5.6642893  4.8707481  3.5227520  5.4485296\n157  2.7142367  5.2101481  5.5498402  4.0650099  3.5500277  4.9476701\n158  2.8346204  5.8384797  5.9627650  4.7369575  4.0190527  5.4812252\n            37         38         39         40         41         42\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38   2.2577152                                                       \n39   1.0408311  2.1463500                                            \n40   2.3307167  1.6916045  2.0357829                                 \n41   5.2692062  5.1337307  5.7564928  6.0554147                      \n42   3.5354177  3.2004460  3.5993543  3.8957214  3.0369449           \n43   3.5763938  3.1076594  3.7232680  3.4854550  3.2070596  1.8744308\n44   5.3272397  5.3147707  5.2930984  6.0282930  4.0975976  2.8226520\n45   3.9293997  3.3167503  4.0687559  4.0270411  2.9038671  1.4102226\n46   5.6596899  5.2124076  5.9006439  6.1109253  2.2756718  3.2881350\n47   6.2228741  6.1092194  6.6657012  6.9309175  1.3680351  3.5599752\n48   1.1377346  2.5128342  1.8872409  2.5187830  5.4118582  3.9870236\n49   5.6132619  5.6201505  6.0023680  6.4544858  1.7819130  3.3252523\n50   6.7548113  6.4688605  6.9469468  6.9234015  3.1624205  3.7640896\n51   4.4448922  4.2315296  4.7328309  5.3843234  2.8156772  2.6000488\n52   2.0121667  2.2609086  2.1254288  1.6670126  6.0285237  4.2979106\n53   2.8802777  3.0196551  2.3773347  1.9956996  6.6793156  4.6226659\n54   4.5514644  4.6053354  4.8718052  5.5462001  2.9444911  3.4675307\n55   6.9482833  6.9923762  7.3447315  7.6702111  2.7799063  4.4731165\n56   2.3176994  3.4395979  2.8896059  3.9360851  4.1494797  2.7623293\n57   2.6381852  3.1385713  1.7467924  2.4094564  7.0732813  4.6690906\n58   5.8218367  5.8051300  6.0604644  6.3628270  2.2468580  2.8562981\n59   1.3312867  2.3043922  1.3863521  1.6251301  5.8751036  3.9579547\n60   3.6578625  2.8697157  3.8394363  4.2483939  3.5226351  2.5155350\n61   5.1242651  4.3077539  5.2007309  4.9724889  2.8722311  2.0321882\n62   6.9679324  7.0801532  7.4091425  7.5697445  2.7528604  4.4101952\n63   5.0625769  4.9250411  5.5424221  5.9664453  2.6260562  3.5803406\n64   5.9220218  6.0314935  6.2408655  6.7047688  2.5503837  3.1022732\n65   4.8088861  4.4279757  5.0888593  5.1439132  2.0226612  1.9253047\n66   0.9470765  2.4720302  0.8134358  2.6330750  5.3959144  3.4160382\n67   3.5003225  2.9771299  3.8066952  3.1277567  6.0302631  5.0468314\n68   5.9769494  6.0094329  5.8666681  6.1178294  4.7592353  3.3548945\n69   4.7156263  4.9623287  4.7390030  5.1663083  3.6678933  2.1805240\n70   3.8883496  3.7483780  3.8412674  4.1540395  3.7582476  1.8808405\n71   5.3259388  4.8456738  5.3436296  5.5166357  2.8464830  2.2116611\n72   2.6662048  2.3595804  2.5334531  1.5792374  6.0200607  4.1221842\n73   3.3266477  2.0118278  2.8669565  1.9663096  5.6812999  3.3771940\n74   2.7475882  3.0553043  2.3629636  1.7503300  6.7525331  4.5580750\n75   3.3425756  2.6935025  3.4840918  3.6339484  3.2812995  1.4659470\n76   3.8376943  4.1816306  3.1669321  3.3482792  8.0657463  5.9403918\n77   4.8078356  4.0331557  4.8407713  4.9563617  2.7068346  1.7376898\n78   3.5267201  4.3317240  3.7567682  4.9190727  4.3705520  3.5233549\n79   6.1095009  6.1223807  6.3418266  6.5887156  2.6859007  3.5420098\n80   6.3279852  5.7414128  6.5747540  7.1114201  3.5462778  4.1161440\n81   5.4940586  4.1336674  5.2054254  5.0013362  4.8682189  3.5613625\n82   4.3541366  4.7332664  4.7050630  5.6652140  2.4132131  2.7299620\n83   5.0322359  5.2719918  5.4688034  5.9413870  1.6573950  2.8309194\n84   1.1984577  3.0190524  2.0039673  3.1890253  5.2567859  3.9140078\n85   3.4137952  2.8005689  3.2724966  3.8035064  4.1074329  1.7580408\n86   6.6646156  6.7435749  7.1372291  7.3685037  2.1893265  4.3208371\n87   4.4205467  4.4908103  4.7196454  5.3809248  3.3552591  3.3794147\n88   1.2303572  3.1557286  1.8879385  3.1560138  5.4904419  4.0387554\n89   2.7658480  2.0127356  2.5545133  1.8956207  6.2633301  4.3451561\n90   6.9302233  6.9928275  7.3238604  7.5159930  2.9030556  4.1957424\n91   6.4768149  6.4106174  6.7555663  7.0446159  2.4112232  3.7334970\n92   4.0877814  3.4326028  3.8637302  4.1778505  4.2236078  2.0378787\n93   7.8662533  7.8515086  8.3100624  8.2740106  3.5804203  5.3364415\n94   3.0015278  2.1898699  2.6478837  2.5982345  6.3761812  4.1879121\n95   3.9286043  2.9659102  3.6908425  4.0813898  5.0764176  3.1635217\n96   4.2492557  3.6670151  4.0707299  3.9914633  4.1644958  2.2644318\n97   2.5446509  3.6389916  3.0402714  4.3402684  3.6313460  2.7394040\n98   4.8691508  5.4862402  5.3251144  6.2653001  3.7695916  4.4335195\n99   2.0538615  2.5599253  2.5658298  3.5238153  4.4584945  3.1049615\n100  4.0090746  4.2839082  4.2649624  4.8010243  3.0720235  1.7534551\n101  6.8670034  7.0597234  7.2717240  7.6017016  2.6196040  4.3292971\n102  4.8802050  4.9917392  5.1634711  5.2955297  2.4234718  2.5085056\n103  5.4916526  5.2832503  5.7405706  5.7667751  2.6831885  2.5091117\n104  2.8309881  2.4664121  2.2530899  1.3106069  6.5821119  4.1959320\n105  2.5271583  1.8191880  2.5059876  1.7469155  5.5856859  3.8334457\n106  4.6774890  3.8547617  4.8762095  4.6656254  2.5997121  2.3145097\n107  7.7107449  7.7138812  8.1414961  8.2057214  3.3023736  5.2243613\n108  1.9093331  2.4278451  2.2937862  3.3621061  4.0365333  2.4996669\n109  2.7219144  2.4407350  2.6615411  1.8586332  6.2524355  4.5640887\n110  4.9907595  4.2088042  5.0232590  5.1376718  3.9705217  3.2471080\n111  6.0095746  6.1391548  6.1212993  6.5678848  3.5590726  3.0250041\n112  3.0838755  1.9321060  3.1112925  2.7443414  3.8206633  2.1926488\n113  3.9999080  3.6038473  4.3241013  4.3503958  2.4262395  2.1004382\n114  4.0394866  3.4506356  4.1206572  3.5985012  3.9089235  2.9645154\n115  5.0452283  5.0757998  5.0610426  5.4950426  3.4262701  1.9858032\n116  1.4137640  2.6244418  0.8508684  2.3594270  5.5707182  3.3970740\n117  2.1416955  1.8648469  1.8539098  1.4319931  6.0337354  3.9484439\n118  6.5999167  5.2126874  6.3081154  6.0463369  5.8678426  4.6584109\n119  1.7068372  3.0846650  1.6484539  3.3689391  5.3600401  3.4053692\n120  3.0784072  4.6192509  3.2226901  4.6225689  5.5237312  4.2336370\n121  3.5526876  2.4557262  3.7978891  3.6561592  3.7782629  2.4907078\n122  3.7978731  3.0299673  4.0639601  4.3865693  3.3021694  2.5208649\n123  5.7123794  5.5411331  6.1144014  6.4687413  2.5436803  3.7940286\n124  4.9596831  4.3693229  4.7635486  5.0945917  4.8010808  3.5055594\n125  3.1087055  2.3316259  3.3151342  3.6825779  4.0088880  2.7920423\n126  5.0892517  3.8632576  4.4317431  3.7152028  7.6167909  5.1155503\n127  1.0051702  2.1068473  0.9634067  2.5117613  5.0233458  2.9887570\n128  1.5122625  1.3316573  1.3751954  1.2493000  5.8420944  3.8975944\n129  6.2582273  5.8409649  6.5921242  7.0992488  3.0126658  3.9944964\n130  4.4214828  4.5579333  4.5693918  4.9163641  2.8106496  2.3587383\n131  3.5099612  3.3527243  2.7211695  2.9877953  7.0590809  4.9110412\n132  2.7580716  2.5839758  2.3493566  1.4611430  6.4752915  4.3207991\n133  2.9138484  2.5713077  2.6685524  3.2163265  4.1507713  1.7531809\n134  6.5708928  6.4493803  6.7891216  7.2604837  3.2152671  3.8071648\n135  4.1954924  3.9380351  4.6282605  5.0263838  3.3239166  3.5575519\n136  2.4799091  2.4888847  2.4899549  1.5001933  6.1567587  4.3005858\n137  2.7029334  2.2112401  2.1634766  1.1525440  6.7361038  4.5298681\n138  4.9143141  5.1437193  5.1611559  6.0913896  2.4713515  2.8497962\n139  6.4128051  6.2443997  6.6452864  6.7890203  2.5360529  3.4825382\n140  3.0325228  2.8353653  2.8819869  2.5637354  4.6786604  2.7496376\n141  5.2394617  5.1200894  5.4919479  6.2528122  2.3915458  3.1778110\n142  6.6216838  6.4307979  6.9274510  7.1776017  2.0470450  3.6434072\n143  4.8707957  3.9405170  5.0951777  5.3570965  3.0553985  2.9823757\n144  2.7097990  2.0489074  2.6939470  3.0840242  4.0177272  1.9707345\n145  2.8953566  3.0508496  3.1262661  3.7864500  3.1387637  1.3034111\n146  3.3988560  2.8791228  3.0365025  3.2361050  4.5644791  2.1705121\n147  7.2343039  7.1002652  7.4729044  7.5389919  3.3726479  4.3921744\n148  2.7697426  4.4638048  2.9913529  4.7409691  5.1820740  3.8175424\n149  6.5868308  5.6380544  6.3486092  6.4448874  5.6213008  4.5235385\n150  2.8259340  2.9048417  2.3704716  1.6148175  6.3317203  3.9600528\n151  3.4425422  3.4498448  2.9341873  2.8418171  6.2779946  3.9651334\n152  1.9918142  1.9377318  2.4196683  2.3278598  4.3605513  3.0121755\n153  4.6933966  5.3453830  4.8859337  6.0724378  3.6210498  3.0041174\n154  4.0651243  3.8824696  4.2966084  4.2188874  2.8395972  1.8456734\n155  3.6281171  3.9277834  3.6255668  4.2150415  3.6373967  1.7968195\n156  6.6649678  6.8596012  7.0253566  7.5256560  2.6230913  3.9497571\n157  6.1783765  6.1687721  6.4857233  6.8254893  2.2545210  3.7833511\n158  6.3223729  6.6386423  6.7334552  7.1552385  2.6239147  4.1963930\n            43         44         45         46         47         48\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44   4.0594155                                                       \n45   1.8756119  3.6229288                                            \n46   3.6351730  4.1927591  3.4425091                                 \n47   3.8099764  4.0487657  3.4667945  2.8883225                      \n48   3.8186351  5.8815761  4.4311219  5.7842891  6.3902927           \n49   3.8285203  4.3910653  3.6344182  1.9493261  2.2138024  5.6809050\n50   3.7049413  4.0193022  3.4552597  3.8017099  2.4462200  7.1189492\n51   3.7371145  2.8149386  3.1117679  3.0230102  3.4931452  4.7274135\n52   3.7052215  6.0833213  4.6163450  6.2291960  6.9260968  1.7449732\n53   3.8784750  6.5181391  4.7185808  6.7318313  7.4967301  3.2919468\n54   3.5219456  4.2139111  3.9429664  2.4603031  3.7778029  4.5649863\n55   4.7267798  5.4618276  4.5145318  3.6538175  2.3943898  7.0530117\n56   3.5652047  3.9055513  3.4398311  4.8934707  4.8597437  2.6536221\n57   4.6075947  6.0923619  5.0037451  7.0490110  7.8902266  3.3718634\n58   3.2701102  3.4761325  3.0979225  2.7556474  1.8402820  6.0987038\n59   3.5466568  5.9137771  4.2204983  6.2869340  6.7547760  1.6915520\n60   3.4727567  4.3663588  2.6006751  3.6191679  4.5084111  3.9788833\n61   2.0926988  3.3247561  1.6524514  3.2073043  2.9948277  5.5021408\n62   4.4665752  5.0354847  4.4757926  3.7444186  1.9046810  7.0325218\n63   3.8803845  3.6913107  3.9699941  3.1249143  3.1934017  5.0366282\n64   4.0716762  3.4927015  3.1236102  3.6634363  2.1252603  6.3331909\n65   2.0439091  3.1894812  1.9567020  3.0950919  2.1577949  5.0826118\n66   3.7481331  4.8974990  3.9183661  5.5861573  6.3071398  1.9393760\n67   4.1984767  6.8515088  5.2855157  6.1381018  6.9900800  2.7920723\n68   3.9417018  3.0001853  3.5566786  5.0130855  4.4700906  6.6281620\n69   2.7673335  2.5562660  2.9559408  3.9553436  3.6576678  5.1794824\n70   2.1578754  2.4885192  2.7712100  3.8340730  4.1505758  4.3036122\n71   2.5644313  2.7840506  2.6563444  2.4051802  2.8993849  5.6571277\n72   3.4604623  6.3542092  4.4996631  5.9921754  6.8901194  2.3843199\n73   2.9159978  5.6400991  3.4081170  5.6581831  6.4217323  3.6765553\n74   4.0177891  6.4648356  4.6501407  7.0131681  7.5209895  3.2117790\n75   2.3280962  3.4975056  1.0991519  3.7985457  4.0075538  3.9026802\n76   5.4768417  7.5825152  6.0004103  8.1103578  8.9128735  4.4298405\n77   2.2467873  2.6811018  1.9929554  2.8176322  3.0444261  5.1871262\n78   3.8164879  3.7909742  4.5107185  4.3485915  5.0252466  3.5921766\n79   3.3294935  4.4406077  3.6715751  3.1909028  2.4251905  6.3209084\n80   5.2382844  3.8421331  4.1426327  4.0246223  3.8500201  6.6883628\n81   3.9429386  4.4839731  3.8860380  3.8473823  5.5429666  5.8355751\n82   3.3106841  3.0859238  3.3823153  3.2186823  2.9749740  4.6246022\n83   3.0716357  3.6744759  3.3248898  2.5523547  1.8903014  5.1079153\n84   4.0941796  5.4669053  4.4578134  5.6326652  6.2129353  1.0610071\n85   2.9604011  3.3039221  2.0593288  4.4185994  4.7402327  4.1406881\n86   4.2030478  5.2025519  4.2282329  3.5459391  1.6317591  6.7383685\n87   3.5037153  3.4299358  4.0755922  3.2751443  4.0145361  4.4752713\n88   4.1641462  5.7120668  4.6300596  5.8074371  6.4314270  1.0659103\n89   4.1374480  6.6702525  4.6064162  6.1053497  7.2520571  2.6670298\n90   4.3733601  4.3522287  4.3798999  4.0350072  1.8548666  7.0574205\n91   3.8387171  4.5434972  3.5412062  3.2906411  1.9196090  6.7794733\n92   2.9042429  2.2660763  2.7877742  3.9534494  4.6890153  4.6169810\n93   5.1071445  5.7670617  5.3200317  4.3017730  2.6743244  7.8520896\n94   4.5923953  6.2618432  4.2369625  6.5346301  7.2971991  3.5254839\n95   4.2357606  4.1405505  3.2021417  4.9914990  5.8687538  4.6416590\n96   2.1468563  3.0207453  2.4828385  4.0322088  4.5353766  4.7643057\n97   3.4537617  4.1199145  3.2277466  4.4125676  4.5096332  3.0530554\n98   4.4597782  4.7476565  5.1734961  3.9089528  4.4470088  4.7104361\n99   3.7500570  4.3701382  3.6364093  4.9396138  5.3896157  2.3027566\n100  2.5144123  2.4273937  2.5731372  3.9606335  3.2417336  4.3970240\n101  4.3759425  4.6374498  4.3850718  3.6159946  1.6958947  7.0239544\n102  2.3067648  3.7500765  2.4462161  3.0942206  2.5717783  5.2038225\n103  2.9177691  3.3432576  2.3080026  3.7481278  2.3410704  5.8704991\n104  3.8966932  6.3414708  4.3336574  6.6207343  7.3958644  3.2542594\n105  3.1492213  5.5743924  4.0895583  5.6938154  6.4480150  2.4351076\n106  1.9604004  3.5945947  1.7774465  3.1518241  3.1016640  5.0037883\n107  4.8912572  5.6908378  5.1346287  4.2009998  2.3952864  7.7507101\n108  3.1920121  3.8966237  2.9358256  4.5151416  4.9710779  2.5348159\n109  3.7851631  6.4190665  4.8846950  6.2012776  7.1572216  2.3700809\n110  3.4599629  3.5935490  4.0107749  3.4909854  4.5475091  5.0584565\n111  3.9020907  2.5155729  3.3912364  4.0424999  3.0275365  6.5105244\n112  1.4899578  4.3172075  2.2986946  3.8315619  4.6607614  3.2835415\n113  1.6485978  3.4301591  2.3022345  2.9530479  3.0494814  4.1179245\n114  1.2621548  4.7598841  2.7925444  4.0629995  4.5157625  4.2024145\n115  3.0101978  2.4163200  2.7010403  3.7947872  3.3047720  5.5329465\n116  3.4073030  4.9939000  3.9237767  5.7309216  6.3959249  2.2455416\n117  3.6693891  5.6827846  3.9552560  6.2782970  6.9101896  2.7535977\n118  5.0904343  5.2195905  5.1953855  4.8887193  6.3710145  6.8026474\n119  3.9987266  4.3203967  4.0313330  5.5812020  6.1706044  2.5934003\n120  4.3601463  4.5776631  5.0332616  5.6341011  6.1604157  3.3912694\n121  3.1866695  4.4987309  2.2351389  4.2340601  4.6614769  3.8749863\n122  3.4930966  4.2646016  2.4656749  3.6283411  4.2809243  4.1292134\n123  3.9996974  5.4525023  3.8399288  3.3654509  3.0287999  5.7387100\n124  3.8590057  3.1848315  4.3189826  4.0448999  5.3248725  5.2367420\n125  3.6039581  4.6614949  3.0118629  3.9728179  5.0893421  3.3665405\n126  5.4357719  6.9720111  5.0972027  7.3631555  8.3412221  5.6385513\n127  3.4034564  4.6345097  3.6071224  5.1805629  5.9489534  1.7966975\n128  3.6952781  5.8871022  4.0852881  6.0506941  6.8285741  1.8383357\n129  4.7484403  3.3449533  4.1852483  3.7768650  3.0635215  6.5029629\n130  2.2520682  3.4861663  2.7332016  2.5836839  3.2573930  4.7205555\n131  4.5743044  6.6375718  5.0267986  6.9462809  7.9370427  4.1724131\n132  3.6663167  6.0852766  4.3937141  6.6495158  7.2607412  3.1870554\n133  2.3623170  3.4730064  1.9681286  4.3807546  4.8543346  3.7327341\n134  4.5022233  2.3803817  4.1064112  3.4870006  2.6895894  6.9261341\n135  3.6098285  4.1705891  3.8677413  3.4160250  4.2177667  4.1498422\n136  3.5588125  6.2039559  4.5947192  6.3405419  6.9676764  2.2377636\n137  4.0747867  6.6228869  4.6920255  6.7306005  7.6310099  2.9529046\n138  3.5208558  3.0562675  3.4526269  3.1449225  2.8310317  5.2547839\n139  3.4439377  3.8641649  3.4264419  3.0673275  1.8413516  6.6885037\n140  2.0141183  4.6276803  2.6209363  4.8885067  5.3717360  3.6368372\n141  3.7972773  3.9340022  3.5420960  2.5206936  2.9567169  5.4754427\n142  4.0586687  4.0807968  3.5349056  2.8903842  1.3097975  6.8936460\n143  3.7830087  4.1102231  2.7778442  3.3077657  3.8161386  5.1548255\n144  2.9300428  3.9190040  2.3210678  3.9188717  4.9456382  3.2493200\n145  2.0601718  2.7815912  2.1314121  3.7377686  3.7960798  3.3725163\n146  2.2625493  3.2916202  2.6442558  4.3985827  5.1331605  4.0228821\n147  4.3295051  5.2021309  4.3246906  3.9229931  2.6991512  7.4389291\n148  4.4871338  4.2932987  4.4673874  5.6981879  5.8422910  3.4797171\n149  5.1790829  4.3490442  5.3350050  4.8259305  5.9936979  6.8145508\n150  3.4751103  5.9986611  4.1783775  6.4500797  7.0267839  3.2125551\n151  3.6455832  4.8691211  4.4327898  6.0597169  6.8643585  3.8922577\n152  2.4059279  4.9075736  3.1553819  4.6876014  5.2752030  2.0028271\n153  4.0973842  2.4586792  4.0234323  4.1300650  3.8219114  5.0957410\n154  1.5169223  3.3443191  2.2192953  3.0591779  3.2467189  4.2586097\n155  2.1591726  3.3092992  2.1486464  4.2244865  4.0467175  4.3074550\n156  4.4277987  3.6746463  4.2499282  3.6943812  1.6692579  6.9097442\n157  3.5945847  4.3823347  3.9697762  2.5305195  2.1005780  6.2922344\n158  4.1128003  5.0355707  4.3950663  3.4207730  2.3754597  6.3902599\n            49         50         51         52         53         54\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50   3.8209208                                                       \n51   3.2313636  4.6654351                                            \n52   6.5024980  7.2008144  5.3169375                                 \n53   7.0388998  7.2261991  6.3354640  2.4439651                      \n54   2.8802512  4.9669760  2.8510768  5.1185086  5.8941460           \n55   2.1239901  3.6242822  4.9233256  7.8414301  8.0838156  4.7128841\n56   4.4962307  5.7126282  2.9885755  3.6408620  4.7925113  4.0229768\n57   7.3446200  7.7105620  6.0587281  2.8540406  1.8472212  6.1266131\n58   2.1140800  2.0143938  3.6459040  6.5558418  6.7628609  3.7823123\n59   6.3032491  6.9587755  5.3496918  1.5028940  1.9130415  5.3180438\n60   3.7598237  5.4998638  2.4545424  4.6682339  5.3135609  3.5998492\n61   3.6651812  2.4077328  3.4367915  5.4245647  5.5805754  4.1003346\n62   2.6918808  2.7134312  4.8797414  7.5953559  8.0343292  4.7849415\n63   3.4048320  4.6570720  2.0387235  5.4607254  6.8021586  2.4792864\n64   2.7814256  2.7529543  3.5219053  7.0013263  7.2911915  4.6522003\n65   2.9627208  2.4555063  3.0467849  5.3018905  5.7511723  3.6407266\n66   5.6513666  6.7158017  4.2584436  2.5283358  2.9913155  4.4905893\n67   6.5397455  7.6057081  5.5686540  2.0812584  3.9887975  4.9543083\n68   5.0851945  2.9547632  4.8398326  6.5904759  6.2642966  5.6831207\n69   3.7713256  3.0821454  3.7924233  5.3684480  5.4121717  4.2256716\n70   4.1882218  3.9144481  3.2066498  4.1732847  4.5673472  3.6055981\n71   2.9531008  2.6100305  3.2506746  5.7484492  5.8921891  3.2144466\n72   6.1852679  7.0569499  5.6790550  1.7956003  2.2906336  5.2728506\n73   6.0009029  6.2153618  5.2921755  3.0984478  2.4423910  5.3456641\n74   7.1962250  7.2285691  6.3527267  2.4427431  1.3089073  6.3752677\n75   4.0154225  4.3027994  2.6163835  4.1584622  4.5628684  3.9244632\n76   8.3886409  8.6171067  7.5070582  3.8255490  1.8593466  7.1789687\n77   3.3072127  3.0802541  2.5964827  5.2387930  5.5639750  3.3145741\n78   4.3462978  5.8701568  3.3361984  4.1492790  5.1503223  2.5068180\n79   2.5061596  2.3949199  4.5601877  6.7185590  6.7012772  3.9296652\n80   4.1552689  5.1144982  2.3621623  7.2475287  8.0671264  4.4867549\n81   5.0110013  5.4073972  4.1499057  5.6035234  5.7417199  4.6254635\n82   2.6254500  4.2487492  2.3067872  5.3732502  6.0205742  2.2477191\n83   1.6192256  3.2625801  3.0923852  5.7699341  6.4144655  2.7310027\n84   5.4628152  7.0403159  4.3062681  2.4940995  3.8659886  4.3319738\n85   4.5756224  4.9575079  3.0690838  4.4046575  4.5131347  4.3597070\n86   2.3533552  2.9285863  4.6774765  7.3629271  7.7423694  4.3329262\n87   3.7654571  4.9642241  2.5956470  4.7525920  5.8498748  1.8294792\n88   5.5557542  7.1974120  4.6963070  2.5605569  3.6371377  4.5191707\n89   6.3660036  7.6208929  5.6892861  2.6084320  2.8782355  5.5453861\n90   3.1706293  2.3721470  4.6084959  7.5019777  8.0161413  4.9048560\n91   2.4722832  2.1953144  4.4765092  7.2913671  7.2711533  4.3252700\n92   4.6514873  4.5446930  2.7911178  4.4223885  4.8485291  3.8443210\n93   3.7093638  2.9796708  5.7002218  8.2179175  8.7271287  5.3946521\n94   6.6722571  7.5922177  5.3993192  3.6946664  3.6394619  6.1967670\n95   5.5702002  6.1199507  3.3097395  4.7657588  5.1003063  4.9969802\n96   4.7820572  3.7938163  3.6675957  4.2883019  4.3254310  4.0412634\n97   3.8304152  5.4889146  3.0108278  4.2178112  4.7910631  3.4893991\n98   3.6507431  5.8520725  3.6719450  5.4597182  6.5317957  1.9890325\n99   4.9289861  6.3557498  2.9007562  3.1510287  4.5852297  3.8807939\n100  3.5986083  3.5499607  2.9724958  4.7674465  5.3197479  3.8092095\n101  2.6813059  2.4172864  4.6630103  7.5608729  7.8948294  4.3852842\n102  2.9235371  2.2066814  3.8156596  5.4787041  5.5825908  3.7654270\n103  3.4373386  1.8278983  3.6864507  6.1085779  6.3652126  4.6624787\n104  6.8459586  7.1865691  6.0316521  2.6830755  1.7057823  6.1807280\n105  6.1658046  6.6257993  4.8374054  1.1334108  2.6866934  4.7234094\n106  3.7885487  3.0296691  3.0567947  4.8844115  5.3212552  3.6073225\n107  3.5283525  2.7769922  5.6241927  8.1348183  8.5014162  5.1041719\n108  4.5606388  5.7183193  2.7082032  3.2495982  4.1781104  3.6237973\n109  6.6292023  7.3618184  5.6631659  1.0000692  2.4257570  5.1358705\n110  4.2891039  5.0151647  3.1466282  4.9680013  5.9112717  3.3481909\n111  3.7304642  2.3335873  3.9752385  6.8351341  6.9163659  4.8374111\n112  4.4027427  4.8334876  3.5237658  2.9746889  3.4906628  3.4232713\n113  3.3282673  3.5491113  2.6526647  4.1643952  5.0063060  2.6401459\n114  4.6069026  4.1249132  4.4683769  3.6797724  3.7337321  3.8120004\n115  3.4290396  2.8941552  3.5375718  5.8539741  5.8892362  4.3463562\n116  5.7826059  6.5170629  4.7590227  2.3921543  2.2035256  4.6941834\n117  6.6821921  6.8773366  5.1866942  1.9603559  2.1458999  5.5638344\n118  5.8736468  6.3279515  5.1259827  6.5475820  6.8717805  5.6641580\n119  5.6462294  6.5812025  3.9508413  3.1343254  3.6844738  4.4342074\n120  5.6195900  6.5486881  4.6027112  3.8105816  4.4305020  4.0269293\n121  4.4802026  5.3215324  2.9603261  4.2211214  5.0269105  4.4589214\n122  3.7744391  5.3012891  2.2710357  4.7839000  5.5375883  3.7831498\n123  2.0345669  4.5891552  4.2092873  6.6297002  6.9710063  3.8204207\n124  5.0455090  5.3583235  3.4229246  4.9438697  5.6223732  3.6245213\n125  4.2697480  6.0198433  2.7539061  4.0430012  4.9174221  3.7852733\n126  7.7974110  7.9529133  6.8094845  5.2463783  4.3878074  7.6147349\n127  5.2336542  6.4454430  3.8750872  2.4663908  3.1353934  4.1694015\n128  6.2905911  7.1440081  4.9625880  1.5459441  2.3556165  5.1709364\n129  3.9043638  4.4521779  2.4143770  6.9262569  7.8962982  3.8730197\n130  2.8173590  3.1832421  3.3889093  4.9767112  5.1013377  2.7109081\n131  7.2868209  7.6892195  6.4826141  3.7262744  1.9002147  6.1920240\n132  7.0405531  6.9527747  5.9266192  1.9949451  1.3429792  5.9485648\n133  4.6284699  4.7681197  3.3830253  3.8005035  3.5980920  4.1063548\n134  3.7719182  2.9656488  3.1856664  7.1756525  7.7759445  4.0386733\n135  4.0517935  5.3717849  2.3298060  4.4278034  5.8276360  2.1765638\n136  6.6033725  7.0575679  5.6462104  0.8224266  2.2548391  5.4282831\n137  7.0464838  7.5833220  6.1171854  2.1602650  1.4953619  6.0416838\n138  2.4651805  3.9816746  2.6725120  5.9621954  6.3580011  2.6075695\n139  2.8806620  1.1919855  4.2280198  6.9318286  7.0580184  4.0862922\n140  5.2877500  4.8426707  4.6219720  3.1543907  2.6086918  4.6806560\n141  2.0463282  4.4061630  2.8652687  6.2374461  6.6225531  2.3253577\n142  2.2497021  2.0607113  3.8459631  7.3986368  7.6863135  4.2494948\n143  3.6651156  4.9118748  2.1076455  5.6195228  6.3497131  3.5289439\n144  4.3124849  5.4041224  2.7606950  3.6386281  4.1767462  3.8079454\n145  3.7080827  4.2672295  2.4499755  3.7504905  4.3925721  3.1975374\n146  5.0154953  4.7224242  3.7264736  3.5866002  3.4757431  4.0123969\n147  3.1512444  2.3188131  5.3998382  7.8363509  7.7848940  5.1768020\n148  5.3297579  6.4112727  4.1170041  4.3881886  4.7620258  4.4332223\n149  5.5654436  6.0320199  4.5978275  6.7027736  7.2247258  5.3678716\n150  6.5979426  6.6738411  5.9585755  2.5595760  1.5339737  5.9541537\n151  6.6684449  6.3520977  5.3191750  2.8464885  2.7629364  5.2928941\n152  5.0095359  5.7122526  3.8829610  1.8109836  3.1215121  3.6925495\n153  3.4493748  4.6838045  2.9050489  5.8703261  6.3966144  3.4939195\n154  3.3820039  3.1610497  3.2340425  4.2663342  4.8089904  3.2874644\n155  3.9912439  3.7504854  3.8096632  4.5883425  4.2445773  4.1361801\n156  2.7885122  2.5829950  4.0474947  7.4757635  7.9240167  4.3733668\n157  2.0770369  2.8636580  4.0921732  6.7330103  7.0201785  3.0223450\n158  2.1069555  3.3651513  4.6796984  7.0812584  7.3521302  3.8550660\n            55         56         57         58         59         60\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56   5.8128998                                                       \n57   8.5734512  4.3732317                                            \n58   2.3546692  4.6180885  7.0921986                                 \n59   7.4313567  3.2826038  2.2027027  6.2506332                      \n60   5.2476223  3.2435102  5.1717895  4.4622107  4.4347693           \n61   4.3206647  4.4433827  5.9947492  2.5956720  5.2777534  3.6561478\n62   1.8827831  5.6541251  8.5693564  1.9691862  7.3618383  5.7477681\n63   4.9873881  3.7718079  6.8968234  3.8864136  5.8145988  3.8212866\n64   2.7941317  4.3600170  7.3562867  1.8593025  6.5001606  4.2862009\n65   3.5732346  3.8081451  6.1543354  2.0508736  5.0962347  3.5805358\n66   7.0667826  2.3620572  2.2786307  5.7702368  1.8931210  3.6063456\n67   7.9489606  4.6677388  4.7006239  6.9579493  3.2979956  4.9575225\n68   5.3631093  5.0296407  6.2276993  3.2610226  6.1113898  5.5789235\n69   4.4068049  3.7192292  5.4830345  2.3364332  4.9680813  4.5551567\n70   5.3388057  3.2929662  4.6111988  3.3552955  4.1040510  3.9148138\n71   3.9199481  4.5467525  6.1882863  1.9767877  5.6574752  3.9447513\n72   7.3527464  4.2458091  3.1501868  6.2735408  1.9624659  4.6052592\n73   6.9823493  4.5215868  3.0972968  5.7445949  2.7736225  3.8999122\n74   8.1515841  4.4785672  1.9404949  6.8086421  1.6075269  5.3682011\n75   5.1681325  2.7965147  4.5442922  3.7749832  3.8071958  1.9752690\n76   9.4106810  5.8636557  1.9265954  8.1462688  3.0878683  6.3217422\n77   4.3593210  4.0330765  5.7588071  2.6454002  5.1519316  3.0855947\n78   5.9644425  2.9507338  4.9533972  4.6432791  4.3086964  4.2504375\n79   2.2521276  5.2824714  7.2960209  1.3739754  6.3875781  5.0703681\n80   5.2743779  4.9019665  7.7919103  4.5222799  7.1620016  3.4331748\n81   6.3330433  5.5041844  5.7845602  4.9320077  5.7002298  3.8459097\n82   3.9343868  3.1139630  6.0183784  2.9118178  5.1388177  3.4023888\n83   2.6776597  3.8032793  6.7653758  1.7295905  5.6068247  4.0520784\n84   6.9293745  2.0432878  3.5651525  5.9192553  2.3013564  3.8783138\n85   5.6794638  3.0735795  4.1853561  4.3016310  3.8625444  2.2637532\n86   1.4654205  5.5209662  8.3426588  2.1210230  7.0845946  5.3153868\n87   5.4496893  3.6396669  5.9046403  4.0657211  5.1041413  4.0021883\n88   6.9434219  2.4023383  3.3937922  6.0063580  2.1605127  4.1019927\n89   7.6380131  4.4638523  3.2694486  6.7421666  2.4719932  4.0798077\n90   2.6365445  5.4499183  8.4322998  1.9516432  7.2968013  5.7597177\n91   1.7886101  5.4719635  7.7650300  1.4939846  6.8647197  4.9113234\n92   6.0270612  3.5386567  4.5494761  4.0757170  4.4545197  3.3672578\n93   3.0303014  6.6271074  9.3930883  2.9520940  8.1659982  6.7251741\n94   7.8130442  4.2190421  3.2269323  6.8451692  3.0134434  3.5535280\n95   6.9816105  3.7894214  4.4172385  5.5742439  4.5128581  2.4468700\n96   5.8848544  4.0186952  4.4941642  3.7984113  4.3015143  4.0651241\n97   5.0889186  1.6694125  4.5225622  4.2145382  3.5682877  2.8247916\n98   5.2017146  4.1195542  6.6717679  4.5142533  5.6590118  4.7736620\n99   6.4873162  1.5526454  4.1321450  5.3565906  3.1101551  2.6727299\n100  4.4200307  2.6812378  5.3497008  2.7444121  4.4199603  3.7930262\n101  2.1199917  5.5626769  8.3737051  1.7471109  7.3104840  5.6976713\n102  3.4166035  4.0738331  6.0864924  1.6917418  5.1466829  4.3388643\n103  3.5719200  4.2644738  6.6694510  1.9055732  5.7562878  4.3159146\n104  7.8696264  4.4902150  2.0597457  6.6140908  1.9832110  4.7065942\n105  7.5257568  3.7653806  3.1709037  6.1281833  2.1403707  4.2465524\n106  4.7436536  4.1210114  5.7677942  3.1686113  4.8991258  3.3783601\n107  2.6846951  6.5767338  9.2093240  2.7720409  8.0115639  6.5527765\n108  6.0451557  1.5406817  3.7688451  4.7931735  2.9255872  2.3874207\n109  7.9900329  4.3581562  3.1600005  6.7322809  2.1495682  4.9193069\n110  5.7716941  4.4605564  5.9991854  4.3143940  5.3527430  3.9602586\n111  3.9422272  4.6090505  6.9230732  1.9793114  6.4112253  5.0434263\n112  5.7040482  3.5109364  4.0426486  4.3155284  3.1372508  2.8726493\n113  4.5951380  3.3222484  5.4432539  3.0485775  4.3492371  3.3163666\n114  5.5330158  4.3969911  4.7243324  4.0384459  3.8119465  4.2990577\n115  3.9179286  3.8725586  5.8937605  1.9706437  5.3853884  4.1764708\n116  7.0258005  2.9233422  1.7701054  5.6551058  1.5460261  4.0989125\n117  7.9194642  3.7193597  2.0599435  6.4823392  1.6587752  4.2617011\n118  7.0412042  6.4837504  6.8857185  5.8219850  6.7391008  4.9897324\n119  7.0923714  2.1167371  2.7607774  5.6542073  2.6428961  3.7884537\n120  7.0316258  3.0751522  4.0219579  5.5522343  3.7184196  5.2080303\n121  5.7167956  3.1886329  5.0020657  4.7080695  4.0380312  1.6636231\n122  5.2005858  3.1764667  5.4068398  4.3860792  4.5635414  0.6870391\n123  2.2252699  4.9770245  7.4644437  3.1943111  6.2411833  3.7564569\n124  6.6292946  4.5734032  5.4139532  4.7864820  5.3402731  4.4144339\n125  5.8820050  3.0159429  4.7011018  4.9898025  3.9187025  1.0865706\n126  8.7113625  6.1449009  4.1365628  7.5321225  4.6918082  5.1489978\n127  6.6802553  2.1534006  2.6030560  5.4241718  1.9501501  3.1517829\n128  7.6193021  3.3899857  2.3182000  6.4308227  1.2941615  3.7827639\n129  4.9630318  4.7669698  7.8256064  4.0176328  6.9924322  4.1087632\n130  3.9994174  3.8371341  5.4734733  2.2040607  4.8189027  3.9795952\n131  8.3497370  5.3327794  1.9566593  7.1162124  2.9912752  5.1807284\n132  8.1310013  4.3983192  1.9839086  6.6441968  1.7403727  5.0923358\n133  5.7538977  3.0109969  3.4134781  4.1846872  3.2154276  2.7590330\n134  4.5921172  5.0371956  7.7587876  2.8741455  7.1729044  5.0941272\n135  5.8243527  3.5397487  5.9231687  4.6377183  4.9445665  3.3907313\n136  7.8080933  3.9819609  2.9737353  6.4999288  1.6434591  4.9546925\n137  8.2003706  4.6319789  2.0382782  6.9850747  1.8100412  4.7888264\n138  3.5415484  3.7485598  6.3752316  2.5989308  5.6338725  3.6066688\n139  2.8446462  5.3567506  7.5540311  1.2321922  6.7259530  5.1092868\n140  6.2452804  3.7556842  3.1977772  4.6260399  2.6973965  4.1103102\n141  3.3945399  4.3753896  6.7768896  3.0225458  5.9995125  3.1913976\n142  2.0675378  5.3189370  8.0242912  1.3477386  7.1158668  4.6950286\n143  5.0434900  4.0553180  6.3182673  4.2178877  5.5867760  1.7085308\n144  5.8275743  2.6641213  3.8862555  4.5159764  3.3779806  1.6755268\n145  4.9623574  2.0169840  4.3433899  3.4025614  3.4513959  2.7958794\n146  6.2492735  3.5891052  3.4377136  4.3533388  3.4628115  3.6700739\n147  1.9936173  6.2557712  8.3979744  1.9118879  7.4671623  5.8452502\n148  6.5652959  2.2278441  4.0236925  5.3169269  3.7230692  4.3975982\n149  6.7547754  6.0384921  7.0247546  5.3736284  6.8713323  5.1602248\n150  7.5303878  4.2609077  2.1940489  6.1573102  1.8712524  4.9872189\n151  7.9502740  4.2710002  2.6219018  6.0413526  3.0921311  5.2792055\n152  6.3935617  2.7970695  3.5834080  5.1001352  2.1921602  3.4245145\n153  4.5112474  3.1545868  6.0572032  3.2634366  5.4813939  4.1479204\n154  4.4696527  3.4065530  5.2608847  2.6405554  4.2830237  3.7897493\n155  4.6532873  3.2185840  4.3741560  3.0674948  3.8369582  3.7282991\n156  2.6003150  5.1078503  8.1586663  1.7355798  7.1931059  5.3795261\n157  2.5380578  5.2645511  7.5650839  1.8182420  6.6199973  4.9367436\n158  1.7175694  5.2815382  7.8979247  2.0128154  6.7768073  5.3340428\n            61         62         63         64         65         66\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62   3.9481351                                                       \n63   3.8268074  4.5310291                                            \n64   3.1423266  2.6999822  4.2430450                                 \n65   1.3743555  3.1466284  3.1470954  2.5341292                      \n66   5.0954283  7.1118160  5.1172155  5.8479286  4.8926337           \n67   5.8059887  7.7336542  5.1914842  7.5972350  5.6179030  4.0887764\n68   3.1313721  4.6977963  5.5573239  3.4418147  3.4102950  5.6590546\n69   2.7187387  3.8485619  4.3111107  3.0026073  2.4072511  4.5158412\n70   2.5517111  4.8537544  3.5601322  4.0540441  2.4209479  3.6955921\n71   1.7665267  3.6353146  3.5445710  3.1872051  1.9369620  5.1542777\n72   5.2358336  7.3144318  5.9680059  6.8936999  5.1981008  3.1316648\n73   4.1487337  7.1109761  5.9985057  6.2159415  4.5501848  3.4211824\n74   5.5844307  8.0009193  6.9079976  7.1097714  5.6909625  2.9591122\n75   2.3824430  5.1725633  3.7551178  3.5839277  2.5060322  3.2784611\n76   6.9985888  9.5293308  8.2024668  8.4900181  7.2077428  3.7290425\n77   1.2924232  4.2271451  3.1618742  3.2574586  1.5675418  4.6769678\n78   4.8412473  5.7774160  3.2726024  5.2761195  4.2345281  3.3393055\n79   3.1413853  2.1363299  4.4967621  2.9507995  2.6090390  6.1379444\n80   4.1923568  5.5233965  3.4201463  3.8848566  3.9842030  6.1457064\n81   3.5834481  6.4445715  4.9016015  5.5772306  4.1693717  5.2510797\n82   3.6076305  4.0623057  2.5557051  3.2586075  2.7216997  4.2557584\n83   3.1768394  2.3502405  2.8111347  2.6367088  2.1191200  5.1184476\n84   5.5794381  6.9008599  4.7662634  6.0102781  5.0882125  1.6385209\n85   3.0147376  5.8756211  4.4190815  4.0662030  3.1335663  3.1312422\n86   3.8762823  1.2554747  4.3123410  2.8106167  3.0156161  6.8436267\n87   3.9716588  5.0933520  1.8031132  4.8013589  3.4187968  4.3343837\n88   5.7612517  6.9989993  5.1907250  6.1712906  5.2743070  1.6732293\n89   5.5991935  7.8748493  6.3062733  7.1018128  5.6465191  3.1176870\n90   3.6280343  1.2177602  4.2452857  2.5224963  2.7671469  7.0256143\n91   3.1648586  2.0851274  4.6418208  2.2548319  2.7046737  6.4836638\n92   2.7385983  5.7413914  3.6914161  4.4516036  3.0603505  3.7029409\n93   4.5042607  1.4180905  4.9791460  3.8701629  3.8906219  8.0414448\n94   5.5069530  8.1734321  6.5759643  6.6718057  5.6604352  3.0367286\n95   4.1309006  7.1584377  4.8943733  5.2410946  4.5019788  3.5092625\n96   2.2079908  5.3773032  4.1133291  4.4826948  2.8062529  4.0039299\n97   4.3768634  5.3612141  3.9474189  4.0066356  3.7129440  2.4897944\n98   5.2868675  5.1351880  2.8584642  5.2997984  4.4274743  4.9066329\n99   4.6613312  6.4770614  3.6677935  5.2055861  4.2208422  2.1287335\n100  2.6503604  3.9407199  3.3610364  2.8662733  1.8142180  3.9691153\n101  3.8527918  1.0819757  4.3088459  2.6163359  3.0889909  6.9259164\n102  2.2911230  2.7771753  3.9290911  2.6002479  1.8581376  4.9143706\n103  1.8152654  2.8950429  4.0726298  1.8563106  1.4654077  5.5076940\n104  5.3182226  7.9174843  6.8181076  6.9208113  5.5515195  2.9535430\n105  4.6958403  7.2555937  4.9443011  6.6585148  4.7294036  2.8694900\n106  1.2852584  4.3430507  3.1491525  3.6432178  1.6477322  4.7217332\n107  4.3610090  1.4502309  4.9165043  3.7622531  3.7149032  7.8729356\n108  4.0660641  6.0714429  3.6731365  4.6507792  3.7129497  1.7809009\n109  5.5347148  7.7807372  5.7038940  7.3820989  5.5220778  3.1432174\n110  3.4851432  5.5014686  2.9885686  5.1223794  3.3342218  4.9113399\n111  2.9156451  3.2956518  4.5530700  1.9903638  2.6417918  5.7843975\n112  2.7678032  5.6007111  3.8197114  4.9517153  2.9450086  3.2175592\n113  2.1479933  4.1065249  2.3017612  3.7883307  1.7349492  4.1195840\n114  2.7361552  5.1206423  4.3041858  5.0511838  2.9537286  4.2188265\n115  2.4676955  3.6415109  4.3507578  2.3271924  2.2024477  4.8319832\n116  4.9528798  7.0317590  5.4933062  5.9536973  4.8185966  1.1013336\n117  4.9712128  7.7722695  5.8155650  6.6272080  5.1289162  2.2497918\n118  4.6391276  7.1325402  5.6669155  6.4807244  5.0585419  6.4126364\n119  5.0952856  7.0410559  4.8662231  5.6307510  4.8422561  0.9634716\n120  5.6531949  6.7056850  4.7834742  6.0119678  5.1849281  2.7625593\n121  3.2901823  5.8414113  4.0247163  4.3989493  3.3239940  3.6791475\n122  3.4952673  5.5836419  3.6157055  4.0515657  3.3725039  3.7931141\n123  4.1031862  3.5294647  4.4467510  3.5277211  3.3884917  5.9114077\n124  3.9187792  6.2790215  3.6421810  5.5880286  4.0601287  4.6109895\n125  4.0900509  6.2269755  3.9550274  4.8853827  3.9920857  3.1044370\n126  5.9067773  9.0072665  8.0551059  7.5545130  6.5062070  4.9334600\n127  4.7273913  6.7601545  4.7436854  5.5467545  4.4913441  0.6866483\n128  5.1916005  7.6511085  5.5968079  6.6219072  5.1726043  1.8897753\n129  3.8110786  4.7826617  2.3122595  3.7995622  3.3029430  6.1671005\n130  2.7201117  3.6563373  3.5996739  3.4206994  2.5276144  4.3011006\n131  5.9507939  8.6277410  7.3141494  7.5335342  6.2257261  3.2864308\n132  5.1768129  7.8984210  6.3744224  7.0403130  5.3796336  2.9136051\n133  2.9685638  5.8277541  4.5097609  4.2913613  3.1634805  2.5776773\n134  3.3279634  3.8143683  3.0952374  3.1122246  3.0293396  6.3491435\n135  4.0612519  5.5411740  1.5318478  5.0811009  3.6629829  4.2491740\n136  5.3016489  7.5023198  5.7668974  7.0217913  5.2474931  2.9831322\n137  5.6333807  8.2527598  6.7362802  7.3757135  5.8166239  2.9186548\n138  3.5245493  3.8820893  3.0997344  3.0000273  2.7337786  4.7553490\n139  2.5326693  2.0953693  4.0993706  2.5742826  2.2359431  6.3847510\n140  3.3096816  5.9728628  5.1364589  5.1110624  3.5823418  3.0274469\n141  3.7088397  4.1278146  3.2935221  3.4558812  3.1534548  5.1341657\n142  2.9780781  1.9275866  4.0264277  1.7996966  2.5114213  6.6012909\n143  3.2834495  5.3968300  3.2038584  4.0261129  3.3053430  4.8070694\n144  3.4448543  5.9717855  4.0592967  4.5038974  3.5282585  2.5007565\n145  2.7461225  4.7720832  3.1155686  3.5751968  2.2355062  2.8149206\n146  2.9286500  6.0051065  4.4234940  4.9104894  3.3232391  3.0482633\n147  3.6472135  1.8345290  5.4178479  3.0088470  3.2935829  7.2880630\n148  5.4790615  6.5629051  4.9574446  5.1692781  4.9538274  2.3147995\n149  4.6854275  6.7039811  5.1176245  6.0143725  4.8362457  6.3243829\n150  5.0266523  7.3984658  6.5780712  6.5861616  5.1906654  2.9703138\n151  4.8032181  7.4739210  5.7210898  6.6630353  5.0945476  3.1567315\n152  3.9863794  6.1434848  3.9660897  5.5202759  3.8114424  2.4100914\n153  4.2099731  4.5265600  3.5930402  3.3222056  3.3999651  4.4063498\n154  2.0087538  3.7873109  3.2150200  3.5164443  1.7610463  4.1284367\n155  2.7588695  4.6062631  4.5975211  3.3339785  2.5341569  3.4545501\n156  3.6710698  1.8528800  3.8922270  2.0760455  2.7691194  6.6371470\n157  3.4216746  2.3534771  3.6633744  3.3293638  2.8236800  6.1992847\n158  4.1669252  1.7756783  4.4090431  3.1289200  3.3271573  6.4303733\n            67         68         69         70         71         72\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68   7.6938334                                                       \n69   6.3019628  1.9865860                                            \n70   4.9410387  3.1438484  1.8386836                                 \n71   6.0843081  3.1605771  2.2343826  2.2693984                      \n72   2.6625914  6.5175344  5.2094821  4.2497539  5.4717737           \n73   3.9946276  5.5732996  4.7615889  3.8769169  4.7114840  2.3177297\n74   4.2499201  6.0632197  5.3005437  4.5655373  6.1017822  2.4130175\n75   4.9015924  4.0509629  3.3441499  2.7723785  3.2109025  4.2862468\n76   5.3958645  7.3463653  6.7091785  5.9469641  7.2840380  3.8262862\n77   5.5415078  3.5029903  2.6841168  2.1126165  1.4169760  5.1302226\n78   4.5854263  5.4983720  3.8562111  3.0285225  4.0223021  4.6259200\n79   6.9989402  3.9377951  2.9362739  3.8265325  2.4673528  6.2874285\n80   7.3242227  5.7639229  5.2562729  4.9125947  4.3300480  7.5229015\n81   5.7509135  5.2296150  4.5466334  3.5092232  3.4504033  5.1460512\n82   5.6986988  4.6211675  3.1632736  3.0073616  2.9516445  5.5615681\n83   5.9424778  4.3294570  2.7863540  3.1807288  2.4949499  5.6521157\n84   3.5894990  6.4150558  4.9857081  4.2384062  5.5940906  3.2622405\n85   5.3315868  4.1456054  3.4659201  2.8294493  3.4839403  4.3897880\n86   7.4008349  5.0676603  4.1229608  4.8971883  3.6527909  7.1076334\n87   4.6896573  5.2956118  3.8454351  2.7730338  3.2327777  5.2173276\n88   3.7401103  6.5031231  5.0274343  4.3838175  5.7037247  3.0341205\n89   3.2791206  7.0642292  5.8594706  4.9032949  5.8979475  1.6463122\n90   7.7106910  4.1678246  3.4287115  4.3915016  3.3862633  7.3475681\n91   7.5973246  4.0793028  3.4926481  4.4341565  2.8889718  6.9511508\n92   5.1506439  3.6304356  2.8989317  1.5839066  2.6576190  4.6090115\n93   8.1700849  5.3208413  4.6557049  5.4961874  4.2273562  7.9856751\n94   4.7954870  6.6785837  5.8955137  5.1038030  6.1240197  3.4324596\n95   5.5993577  5.2719248  4.8527691  3.9512297  4.6378243  5.0393780\n96   5.0843972  3.0662005  2.5969704  1.5554952  2.4508950  4.4330805\n97   5.1174928  5.0831712  3.7449435  3.5442213  4.3004788  4.5119274\n98   5.2714181  6.3955831  4.6838600  4.1455639  4.3458768  5.7650330\n99   3.8596533  5.8686638  4.5979365  3.6128282  4.8489377  3.9208094\n100  5.5405579  3.1008305  1.6824806  1.7971946  2.6260879  4.8797377\n101  7.7674949  4.4191585  3.6535584  4.6504246  3.3585562  7.4093570\n102  6.0628017  2.9951562  2.0352806  2.7819528  2.2719538  5.3168064\n103  6.6914418  2.6562126  2.3398865  3.1217139  2.5421416  6.0074742\n104  4.2239820  6.0877210  5.2821001  4.5283527  5.8163556  2.0452530\n105  1.9489711  6.1750290  5.0383023  3.6358017  5.1009954  2.0203547\n106  5.0883131  3.8953340  3.2811940  2.5907831  2.3589886  5.0033592\n107  8.0937922  5.2751169  4.5990527  5.4143822  4.0488440  7.9101258\n108  4.1904911  5.1358475  3.9717911  3.0861969  4.2681547  3.8669410\n109  1.7608632  6.8876691  5.6357940  4.3664698  5.7595225  1.4873826\n110  4.7910254  5.1894289  3.9068794  2.5250187  2.9836386  4.9565490\n111  7.6388299  1.8421855  1.8144968  3.2014781  2.5609438  6.7509737\n112  3.2851062  4.8508574  3.7205672  2.4845587  3.1874870  2.8800836\n113  4.2678271  4.2251182  2.9774446  2.1045936  2.3240814  4.3786689\n114  3.9824122  4.4962172  3.5369971  2.7044614  3.1359634  3.5633751\n115  6.6996707  2.0754929  1.0911423  2.3713165  2.0979407  5.5977417\n116  4.1084707  5.3477957  4.2100530  3.4535138  4.9779930  2.6647501\n117  3.5889830  5.9178348  5.1637689  4.0470655  5.5956337  2.7039092\n118  6.4779767  6.1666467  5.4244322  4.4200849  4.3882220  5.9591853\n119  4.6366053  5.3516269  4.2834789  3.4971426  5.0445858  3.8975743\n120  4.9702915  5.5495783  4.2109945  3.6415063  5.0882317  4.5709051\n121  4.6240368  5.3025129  4.4857655  3.7222538  4.1771865  4.2935085\n122  5.0696052  5.4546717  4.5098185  3.8808853  3.9649058  4.8313432\n123  6.5474035  5.9412443  4.6673435  4.9662952  3.8838062  6.0905546\n124  5.1883561  4.8643474  3.8719564  2.3889886  3.2352374  5.1586812\n125  4.3721036  5.8843092  4.7617072  3.8974367  4.3635372  4.0725047\n126  6.3366072  6.7000713  6.4062303  5.7619896  6.5670925  4.4823230\n127  3.8288530  5.5164459  4.2486716  3.3540875  4.7662385  2.8537212\n128  2.9731433  6.4075555  5.3288375  4.2274646  5.6469824  2.0736582\n129  6.8517799  5.4230563  4.7108540  4.2688649  3.7418813  7.3021100\n130  5.5241329  3.4552437  2.1660993  2.3873091  1.9422187  4.8412377\n131  5.0656890  6.5625997  5.8442082  5.0217312  6.1444748  3.3734616\n132  3.6766843  5.9224310  5.1708161  4.1308348  5.7217969  2.3345143\n133  4.9383194  3.8244564  3.0953164  2.3934487  3.3387711  3.8390273\n134  7.4798148  3.7447285  3.4162172  3.6244861  2.7545302  7.4487744\n135  4.0855632  5.9431178  4.6752353  3.5128023  3.8916878  5.0975694\n136  2.3333413  6.4212624  5.2525797  4.1526814  5.6894511  1.4433340\n137  3.5453175  6.6718371  5.7279080  4.7335857  6.0655674  1.6877546\n138  6.3091948  4.4354731  3.1211509  3.2286503  2.7138393  5.9742281\n139  7.2121692  3.3970002  2.8858227  3.6968137  2.1030607  6.7287724\n140  4.4092611  4.0539019  3.3720526  2.6771129  3.8601992  3.1356122\n141  6.2976915  5.2786729  4.0436068  3.9992856  2.9224814  6.0649693\n142  7.6205285  4.0426795  3.5075592  4.3121449  2.7486912  7.1857443\n143  5.6222339  5.5534693  4.7858473  4.1658942  3.6790937  5.7515901\n144  4.4440610  4.8263531  3.8384143  2.9947156  3.7334307  3.6658890\n145  4.5564713  3.8021633  2.4401849  1.6372487  2.8194597  3.9955630\n146  4.6197292  3.6839232  2.9387967  1.6945683  3.0127389  3.6956109\n147  8.0934340  4.2844124  3.7681577  4.8902163  3.3249437  7.3044829\n148  5.7469470  5.2599279  4.1530029  3.9576040  5.2031160  5.0364408\n149  6.7805798  5.6747753  4.8433168  3.9408482  4.1332355  6.3547392\n150  4.2372420  5.5164815  4.6836042  4.0746856  5.4468045  2.0315154\n151  4.3122526  4.9768919  4.2904282  3.1613180  4.8698960  3.3522569\n152  2.4994382  5.5431460  4.3099616  3.1692015  4.3558072  2.5751914\n153  6.5685108  4.1703535  2.7731710  3.0498791  3.3718984  6.0077444\n154  4.7459049  3.3926238  2.0937589  1.7146272  2.0569011  4.2252421\n155  5.7048245  2.8469353  1.8615171  2.2134752  2.8452197  4.4292175\n156  7.7879138  3.9779131  3.1951563  4.1547773  3.1073721  7.4144477\n157  6.7478391  4.6318641  3.4975990  3.9632973  2.4820925  6.4985779\n158  7.2322767  4.9552424  3.7533530  4.6287787  3.4738749  6.7602856\n            73         74         75         76         77         78\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74   2.6752113                                                       \n75   3.2958972  4.4223475                                            \n76   3.6757964  2.3662321  5.7336828                                 \n77   4.1399784  5.6890282  2.2697263  6.8999578                      \n78   5.1306072  5.4512272  4.1779901  6.2961407  4.0079072           \n79   5.8346898  6.8982100  4.4539171  8.0932742  3.2235011  4.9313620\n80   6.6556505  8.0604558  3.8034734  9.0841631  3.5056747  5.4239246\n81   4.2763057  5.9415696  3.8151762  6.8486520  3.0511176  5.1900264\n82   5.3417415  6.2026515  3.3110392  7.1887784  2.7642693  2.5586589\n83   5.6301524  6.4912233  3.7013298  7.8602749  2.8784734  3.5303150\n84   4.2819397  3.7569116  3.8481711  4.8296509  5.1692753  3.1624288\n85   3.1739785  4.3901777  1.4561962  5.4129888  2.6040959  4.1864384\n86   6.8210493  7.8164431  4.9065307  9.1718247  4.0290416  5.5884063\n87   5.3942786  6.1350151  3.8609683  7.1762484  3.1108427  1.9451665\n88   4.1698430  3.5766215  4.1079869  4.5525357  5.3639075  3.3095697\n89   2.4175274  2.9548789  4.2338531  3.8764639  5.4180762  5.1504569\n90   7.0384733  7.9159119  4.9870716  9.5010808  3.8489950  5.5796298\n91   6.2345624  7.3998715  4.3487562  8.5576007  3.3577560  5.5588953\n92   3.7882911  4.8868476  2.4442677  6.0164630  2.0627834  3.4871076\n93   7.8480589  8.7321811  6.0553185 10.3213997  4.8993468  6.4986554\n94   2.7282079  3.3248315  3.5997582  4.0259734  5.3256944  5.7546549\n95   3.9116039  4.9866356  2.3156175  5.7499633  3.6671363  4.9545966\n96   3.5342278  4.4426183  2.6295647  5.6738485  2.1640789  3.9620771\n97   4.5112282  4.7484833  2.7734047  5.7002997  3.8548499  3.0091089\n98   6.3421018  6.8965721  5.0406345  7.7723587  4.4608120  2.1117796\n99   4.1591891  4.4281437  2.7487423  5.5806828  4.0923508  3.0590512\n100  4.5558721  5.1114328  2.6431966  6.6411704  2.4248353  3.3026234\n101  7.0891253  7.9646463  5.0698054  9.3130962  4.0011003  5.4225848\n102  4.9379511  5.5716827  3.1956152  7.0238196  2.7001498  4.3763656\n103  5.2315221  6.1506573  2.9904556  7.7438612  2.4642585  5.1418035\n104  1.8243678  1.3626192  4.0646962  2.6745447  5.3444944  5.4883931\n105  2.6351410  2.8341506  3.6540248  4.1830246  4.4883679  4.0474145\n106  4.1382960  5.3839671  2.1616011  6.7539846  1.5400530  4.4617319\n107  7.6524275  8.5985412  5.9058859 10.0291725  4.6916364  6.3228972\n108  3.7656211  4.0722373  2.0962929  5.1580419  3.5437407  2.9952041\n109  2.9668063  2.7544004  4.5419950  3.9018034  5.3421532  4.3880981\n110  4.8277687  6.0726304  3.8230429  7.2956383  2.6184324  3.4889555\n111  6.0327982  6.7663077  3.9009693  8.1267726  3.0646127  4.9656423\n112  2.3025100  3.7492173  2.1827340  5.0233741  2.5365037  3.6608808\n113  4.1109095  5.1345051  2.4406558  6.5632435  1.8717356  3.2197278\n114  3.1658481  4.0834828  3.2010835  5.3948156  2.9492473  4.2173885\n115  4.8567901  5.7719270  3.1248295  7.1160759  2.3826081  4.2796708\n116  2.9179351  2.2987362  3.5082597  3.1153498  4.6248685  3.4967647\n117  2.5540719  1.8268501  3.4094299  3.0999550  4.9048848  4.8115338\n118  5.2964068  6.9879149  5.1058499  7.9771243  4.0892486  6.0019799\n119  4.0456329  3.5988659  3.3416032  4.3369598  4.5930212  3.0180936\n120  5.2597483  4.6325123  4.6972568  5.3134854  5.1190418  2.0688218\n121  3.5673992  4.7762441  1.4844156  6.1664133  3.1472588  4.8001443\n122  4.1546070  5.4794558  1.7851889  6.5849273  3.0227889  4.4271082\n123  5.7139524  7.1501361  4.2457408  8.2157143  3.8430830  5.1722275\n124  4.8843763  5.8820137  4.0452593  6.8200102  3.0334215  3.2954431\n125  3.7541554  4.8973263  2.2489143  5.9284696  3.5599606  4.1005081\n126  3.1533334  4.0847774  4.8149281  4.6144875  5.9572176  7.3048110\n127  3.1473185  3.0974225  2.9510548  4.0581297  4.2564156  3.1037400\n128  2.4919485  2.2464962  3.4716465  3.3607570  4.9571139  4.4404354\n129  6.6865650  7.9330600  4.0219382  9.1027904  3.1187101  4.7053122\n130  4.5835386  5.3598698  3.2124142  6.5133715  2.5446327  3.3607559\n131  2.6997382  2.5973128  4.7780423  1.6615896  5.7214196  5.5821253\n132  2.4017858  1.0789441  4.1027677  2.6624597  5.2172482  5.1046874\n133  2.6880291  3.6206881  1.5990153  4.5864135  2.6510616  3.8398299\n134  6.8487419  7.8139282  4.3390431  9.0644355  3.0021470  4.6609907\n135  5.2238565  6.0680263  3.4682668  7.1568237  3.3621095  2.8825257\n136  2.8354385  2.1465861  4.2783551  3.8231425  5.2557736  4.5194770\n137  1.9669564  1.5771813  4.3433585  2.5146659  5.5492113  5.3376074\n138  5.5238945  6.5758940  3.5397455  7.4741916  2.6720120  3.1704843\n139  6.0763947  7.1819066  4.2288914  8.4818579  2.8306448  5.1566356\n140  2.5208115  2.5850012  2.7200248  4.0235723  3.5150286  4.4071361\n141  5.5387263  6.9602691  3.6995868  7.7476164  2.9204606  3.7832467\n142  6.5106418  7.7244714  4.1910872  9.0322043  3.0805984  5.5129738\n143  4.8277275  6.4708487  2.3968383  7.4488709  2.6859729  4.6855241\n144  3.0906888  4.1090057  1.5631824  5.2428156  3.0236155  3.7855173\n145  3.6610140  4.3134627  1.7969396  5.6713022  2.2512992  2.7057102\n146  2.7744387  3.6664632  2.4919032  4.6988793  2.6008559  3.5500309\n147  6.6994341  7.8651539  5.2054021  9.1785670  4.0226186  6.2688078\n148  5.1992000  4.7117555  3.9789733  5.3338949  4.9531494  2.9737613\n149  5.8838919  7.2811705  5.1661638  8.3436667  3.9034228  5.3548390\n150  2.1146133  1.1195048  4.0586509  2.9074390  5.1361402  5.1232317\n151  3.3368353  2.9092820  4.1823499  3.9396799  4.6578196  4.1602586\n152  2.9987726  3.2381629  2.7777509  4.6332173  3.8078400  3.3415646\n153  5.8087226  6.4095847  3.8820706  7.4193175  3.4114928  2.7806067\n154  4.0429981  4.7986042  2.6220128  6.4156488  2.1668638  3.5279477\n155  3.6689562  4.1918470  2.4613811  5.3615165  2.7068542  3.8630131\n156  7.0239376  7.8953937  4.7474233  9.2684176  3.5859925  5.0350813\n157  6.1976501  7.3295276  4.5921905  8.4459697  3.2147061  4.4614724\n158  6.6437258  7.5284765  5.0242345  8.7386818  4.1110214  4.9410470\n            79         80         81         82         83         84\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80   5.4435585                                                       \n81   5.3155462  4.9780741                                            \n82   3.3034141  3.7092268  4.8970751                                 \n83   2.1156296  4.4228690  5.0286213  1.9797999                      \n84   6.2706656  6.3338632  5.9655098  4.2488710  4.9053549           \n85   4.9351986  4.0688673  3.6563017  3.5219757  4.2552212  4.0599035\n86   1.8975118  5.1950312  6.3272938  3.6151594  2.2138844  6.6400865\n87   4.4190220  4.3988994  4.3946352  2.3119649  2.9641814  4.2165637\n88   6.2664582  6.7187427  6.0843461  4.4275437  5.0296207  0.6828777\n89   6.8799930  7.3012867  5.0233482  5.8789792  6.1413119  3.4186550\n90   2.4101843  5.1673655  6.2473879  3.8572571  2.4231673  6.9037912\n91   1.3704952  4.8628106  5.6510008  3.4908243  2.5218994  6.6580965\n92   4.7661769  4.2622374  2.9015736  3.4766953  4.0436364  4.4946953\n93   2.9186554  6.2938065  7.0019413  5.0190008  3.2931835  7.7834213\n94   7.2381217  6.5584712  5.1577364  5.9215536  6.5318592  3.8637130\n95   6.3326617  4.1114902  3.6935198  4.6401520  5.5063564  4.4650309\n96   4.2341352  5.1195102  3.4503552  3.8510205  4.0065349  4.7906844\n97   4.6537711  4.6699120  5.3429807  2.4517836  3.4424018  2.5008038\n98   4.5567271  5.3628999  5.7775493  2.4592011  3.0524020  4.3727330\n99   5.9530547  4.7000562  5.0250817  3.5065979  4.4371048  1.9014643\n100  3.4336301  4.4448239  4.6523122  2.4243488  2.4221989  4.1947655\n101  1.8177851  5.2904614  6.4203971  3.6078004  2.2183712  6.8218432\n102  2.0037433  5.1047429  4.7767769  3.1578540  2.1063101  5.1136573\n103  2.7717680  4.3840899  4.9455225  3.5490896  2.7559528  5.7752809\n104  6.7606407  7.6430812  5.2675548  6.0918382  6.3884198  3.8398107\n105  6.3127266  6.6370511  4.8165387  5.0509562  5.4526765  3.0859989\n106  3.6003590  3.9659491  3.6427520  3.3609100  3.2179595  5.0832637\n107  2.4375597  6.1297827  6.9425969  4.6666112  3.1178073  7.6902453\n108  5.3609257  4.4870109  4.5957243  3.1078379  4.0623363  2.1432366\n109  6.7847313  7.5734226  5.4009430  5.6830149  5.9736367  3.1822567\n110  4.7143584  4.4878639  2.7107556  3.5280537  3.7295958  5.0954906\n111  2.9908181  4.7532567  5.2821897  3.5788450  3.0303066  6.2060316\n112  4.5179330  5.0903626  3.3571584  3.6776963  3.8907405  3.6538120\n113  3.4201667  4.2471625  3.9905268  2.5765751  2.4425783  4.1588014\n114  3.8924653  5.9849255  4.1813724  4.0895380  3.8496809  4.5746180\n115  2.8097862  4.6450077  4.4619275  3.0679914  2.7679334  5.3294507\n116  5.8576040  6.6517283  5.1986112  4.4315370  5.1399239  2.2875242\n117  6.7789598  6.7925113  5.1421241  5.4943388  6.0545865  3.1511041\n118  6.1910332  5.7223289  1.7837894  5.8255999  5.8475151  6.9893970\n119  6.1433149  5.8412600  5.3524365  3.9959476  5.0172287  2.0069259\n120  5.8028080  6.7434053  6.1901134  3.8954838  4.7001709  2.8170122\n121  5.3841557  3.8956218  3.9034096  4.0916892  4.3889111  3.9503641\n122  5.0783583  3.1297948  3.9009174  3.4354904  3.9808345  4.0056040\n123  3.0846150  4.6738750  5.5305810  3.3025354  2.7948581  5.7431371\n124  5.2544675  4.9536085  2.8869400  3.9019869  4.4492358  5.1249182\n125  5.5830621  4.0847667  3.7771024  3.8073488  4.3947741  3.2951625\n126  7.8789241  7.7570300  4.9702858  7.3994955  7.7078081  6.0385878\n127  5.8099533  5.7758093  4.7816753  3.9289936  4.7325223  1.6254303\n128  6.6846504  6.6604096  5.1313579  5.2056455  5.8073237  2.4221732\n129  4.8074708  1.9011340  5.1106807  3.0802480  3.6181291  6.2029277\n130  2.4091872  5.0729238  4.2607137  2.7011379  2.2157244  4.5657270\n131  7.0647679  7.9370378  5.4436528  6.2001912  6.9153093  4.5550503\n132  6.7645473  7.6216646  5.4015517  5.9302336  6.3008369  3.7462488\n133  4.6242316  4.7611942  3.7003084  3.4685646  4.1648974  3.6906967\n134  3.8060324  3.6261366  5.2274123  3.3042562  3.1425896  6.5619830\n135  5.0921667  4.0104163  4.5544518  2.9643023  3.5710813  3.9616964\n136  6.6156869  7.5516277  5.6199230  5.6397316  5.8319459  3.0398804\n137  7.0760082  7.7712174  5.3606952  6.1698655  6.5720808  3.6667136\n138  2.9394288  3.6647562  4.8362279  0.8616690  2.0715232  4.9186019\n139  1.4692972  4.8798263  5.2409732  3.4850609  2.3638096  6.5835511\n140  4.7683957  6.2247013  4.2644156  4.4797176  4.5795251  3.8879094\n141  3.2674517  3.5348775  4.6740563  1.9423378  2.4824430  5.2440637\n142  2.0553448  4.0567786  5.3552567  3.4056518  2.3872057  6.7019072\n143  4.9057694  2.2421006  4.0905898  3.3495789  3.9758506  5.0252577\n144  5.1481334  4.3445636  3.4035378  3.6441156  4.1728624  3.1523507\n145  3.9652004  4.2248246  4.0299263  2.2600453  2.8468079  3.1927311\n146  4.7232466  5.3246599  3.3832501  3.8169580  4.3218368  4.0730682\n147  1.4984186  5.8742876  6.0516269  4.4757371  3.1158429  7.4078674\n148  5.7625109  5.9053001  6.2833499  3.5259464  4.6623942  2.6691683\n149  5.8587120  5.3029166  2.5492827  5.1812633  5.3587538  6.8132018\n150  6.2560086  7.7084249  5.4589596  5.7983979  5.9578321  3.7483339\n151  6.3123524  7.2167896  4.9522645  5.4023700  5.8097970  4.0514659\n152  5.3388142  5.7535016  4.7730396  3.9691391  4.3123562  2.3824738\n153  3.8995453  4.2764828  5.2670278  1.7198475  2.7338934  4.5682177\n154  3.0942395  4.9377205  3.9731941  3.0973891  2.4128333  4.2837568\n155  3.3518547  5.1789095  4.4774768  3.0257955  3.2508307  4.2009440\n156  2.3752044  4.5402417  6.0141458  3.1066531  2.1010047  6.6203622\n157  1.3777204  4.9967951  5.3218472  2.8702758  1.7868032  6.1752619\n158  1.4987717  5.5961089  6.2507609  3.2563343  1.9189508  6.2342795\n            85         86         87         88         89         90\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86   5.6317052                                                       \n87   4.1617696  4.8708157                                            \n88   4.2096171  6.7338744  4.5236928                                 \n89   4.1463242  7.5925639  5.6771584  3.2147794                      \n90   5.5870821  1.9468570  4.8218329  7.0492818  7.9695851           \n91   4.8929746  1.6083692  4.9244952  6.7150585  7.3214534  2.4193925\n92   2.2429185  5.7275567  3.0966918  4.7276885  4.8607892  5.2755734\n93   6.8582362  2.1763152  5.5928801  7.9163566  8.6778201  1.8399146\n94   3.2089218  7.8561898  6.2189499  3.8213656  2.3718049  8.1229154\n95   1.9918996  6.9131934  4.7796140  4.7553936  4.4786842  6.9059736\n96   2.9330865  5.3559120  3.5110170  4.9724841  5.0156802  4.9912850\n97   2.9409103  4.9525211  3.6634120  2.6336456  4.5480693  5.3457307\n98   5.3215238  4.7602474  2.0443003  4.5291267  6.2516634  5.1318681\n99   2.9093588  6.1989431  3.4784193  2.4023372  3.8101538  6.2912039\n100  2.8088792  4.0109603  3.1836609  4.3605440  5.4014982  3.4230265\n101  5.7239634  1.2612160  4.7371850  6.9337622  7.9838275  1.4324365\n102  3.9405734  2.7997630  3.8842273  5.2109972  5.9126933  2.8257174\n103  3.6794702  3.0798684  4.5040685  5.9537652  6.4642235  2.4558642\n104  3.9297363  7.7157007  6.1164836  3.6274928  2.1279634  7.8800757\n105  3.9381729  7.0089231  4.2779944  3.2441566  2.7646931  7.0915948\n106  3.0810294  4.0653870  3.3737377  5.3620785  5.3701902  4.0670510\n107  6.6554648  1.6512610  5.4314406  7.8022983  8.5799417  1.9114631\n108  2.2371433  5.7625937  3.3393871  2.5250799  3.7805697  5.9040168\n109  4.7567941  7.5391588  4.8793182  3.1589599  2.4416925  7.7372438\n110  3.8775462  5.4322732  2.3492584  5.3213696  5.3176304  5.0962719\n111  4.1053955  3.7368883  4.5922677  6.3306366  7.2004963  2.7374840\n112  2.6548177  5.2868250  3.3296392  3.7987779  3.2538946  5.4935355\n113  3.2611915  3.8717791  2.2856938  4.4317569  4.9562217  3.8739714\n114  3.8822472  4.8360304  3.7653540  4.6581617  4.4337165  5.0615977\n115  3.1727428  3.8615161  4.1621439  5.3806880  6.0435510  3.2086985\n116  3.2951505  6.7853880  4.5414574  2.0933940  3.0228295  6.9373329\n117  3.4748111  7.5156688  5.2033639  3.2733576  2.7968459  7.6109154\n118  4.7524430  7.1404900  5.1992840  7.0817943  5.8583119  6.8215533\n119  3.0966728  6.8371544  4.0611053  2.1879499  3.9560034  6.8492945\n120  4.7014794  6.5887046  3.5780774  2.8645546  5.2231280  6.5457172\n121  2.1768738  5.5581524  4.3815460  4.2594155  3.9029502  5.7211904\n122  2.2778966  5.1869053  4.0019425  4.2988748  4.3624686  5.5466614\n123  4.6480584  2.7444726  4.7861711  5.7355780  6.1154123  4.0093336\n124  3.9417399  6.2637949  2.5640083  5.3527359  5.5577784  5.8352624\n125  2.4958974  5.8775166  3.9625154  3.5567929  3.4497734  6.2350129\n126  4.2817327  8.8621571  7.5697324  5.9228094  3.7288534  8.9069761\n127  2.7731612  6.5026433  3.9981046  1.6672712  2.8088453  6.6668660\n128  3.5357807  7.3280821  5.0032581  2.4653310  1.9481449  7.5840387\n129  4.3517221  4.5252547  3.3723549  6.5974107  7.4019903  4.2389372\n130  3.8052609  3.5554657  3.0708128  4.6338355  5.4421299  3.7396269\n131  4.3351790  8.2263482  6.2841600  4.2939404  3.2443899  8.5994759\n132  4.1441642  7.6928148  5.5832463  3.7060570  2.9740430  7.7433733\n133  1.3091604  5.5504166  3.9740803  3.7860757  3.8975625  5.6089906\n134  4.7302256  4.0273643  3.5260311  6.8712632  7.8508867  3.1542237\n135  4.1107984  5.1939474  1.6521946  4.4148110  5.3441285  5.3815074\n136  4.5317079  7.3359113  5.0721781  3.0154691  2.5977439  7.4095287\n137  4.2268619  7.9817549  5.9583379  3.4770060  1.7363862  8.2200387\n138  3.6086796  3.4066357  2.8650233  5.0346253  6.2372616  3.6840142\n139  4.8390731  2.1701582  4.3092471  6.7048252  7.2903736  1.9746031\n140  3.0863029  5.7856087  4.4852214  3.9183259  3.7430275  5.8609138\n141  3.8263239  3.5232402  3.2685044  5.3441400  6.0621177  4.2398045\n142  4.8340959  1.8160426  4.6301582  6.8582141  7.5107982  1.9958095\n143  2.9595031  4.9156599  3.8667246  5.3539855  5.4561566  5.2920824\n144  1.7089307  5.7495879  3.7951395  3.3649910  3.3116283  5.8907330\n145  1.9847759  4.5924395  2.6941431  3.4099176  4.3418724  4.4584048\n146  2.3614671  5.8814361  3.5594354  4.1825034  4.1349662  5.6875854\n147  5.7433604  1.9575854  5.6611524  7.4131830  7.8327652  2.2560860\n148  3.8323347  6.3318896  4.2404763  2.7488141  5.2498754  6.4033571\n149  4.8030009  6.7985464  4.6066576  6.9416325  6.4890815  6.2208468\n150  4.0479195  7.2748266  5.8213414  3.5205492  2.7730278  7.3353771\n151  4.1629510  7.4642727  4.6803936  4.1018376  4.1823768  7.1849203\n152  3.3753044  5.8406290  3.4830636  2.6527629  3.1060492  6.0811058\n153  3.6740945  4.4554186  3.1405282  4.6752257  6.3281078  4.1259828\n154  3.3579216  3.8675295  2.9783831  4.4538640  4.9167503  3.6074495\n155  2.4624236  4.4239816  4.0653137  4.1852080  4.8569310  4.4022949\n156  5.1702832  2.1454516  4.3283494  6.7743789  7.9057526  1.2899347\n157  5.1606343  1.9375712  3.6716762  6.2533181  7.0482015  2.6389973\n158  5.6067496  1.3372589  4.5516194  6.2429242  7.3161677  2.4446208\n            91         92         93         94         95         96\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92   5.0814018                                                       \n93   3.0426432  6.4093420                                            \n94   7.3385438  4.6586811  9.1509489                                 \n95   6.2090460  2.8043940  8.0457120  3.1943107                      \n96   4.6635830  1.6511355  5.8707108  5.0723494  3.7056858           \n97   4.7431357  3.8364096  6.4783024  4.2515747  3.8340965  4.2071393\n98   5.1488244  4.7632625  5.7360343  7.0154930  6.1092921  5.0823368\n99   6.1207196  3.4143914  7.3729086  3.5789788  3.1258474  4.1348282\n100  3.7332815  2.6789262  4.8330250  5.3161737  4.3160852  2.8101728\n101  1.6932969  5.5086914  1.8549699  8.2135449  7.0047565  5.0975306\n102  2.3740643  3.7500539  3.5198188  6.1317036  5.1767987  3.0215406\n103  2.5524689  3.6993512  3.6709855  6.2097345  4.9479860  3.2784694\n104  7.2119133  4.5934922  8.7148200  2.4340498  4.4006158  4.3735612\n105  6.8615487  3.7246380  7.8076794  3.7034291  4.2706251  3.5677919\n106  3.6439116  2.7295797  4.8192765  5.3538745  3.8869874  2.1929129\n107  2.4834054  6.3311059  1.0123696  9.0077596  7.9227355  5.7493416\n108  5.4927416  2.9277945  7.0096846  3.3916200  2.6838426  3.5032506\n109  7.4472461  4.6000511  8.3247889  4.0077773  5.1136626  4.3951591\n110  5.2809041  2.5338038  5.9430506  5.8328981  4.4122234  3.2391720\n111  2.9320449  3.7662412  4.1133175  6.9280579  5.3862494  3.6095106\n112  4.9355824  2.5468197  6.2243600  3.7941097  3.4361557  2.3072851\n113  3.8039799  2.6437867  4.6260592  5.3433421  4.2139870  2.3372405\n114  4.5237671  3.4032380  5.4939023  5.1716475  4.8273369  2.2612900\n115  3.0445097  2.9917360  4.6045042  5.8215620  4.6166723  2.9699448\n116  6.3518232  3.7138538  7.9194253  3.2406214  4.0022049  3.7239185\n117  7.1123961  3.9364599  8.4995970  2.6744320  3.6079581  3.7922067\n118  6.6087512  3.9215603  7.6038581  6.1188060  4.9128006  4.6755144\n119  6.4513658  3.4364259  7.9709674  3.6651734  3.4521765  3.9045480\n120  6.4312372  4.2108185  7.4397569  5.7225648  5.3658064  4.2981371\n121  5.2465905  3.2533366  6.6955141  3.2147370  2.3237398  3.7295080\n122  4.8466636  3.3574507  6.5376415  3.7352924  2.3955025  4.0396851\n123  2.8463328  5.4054216  4.5983411  6.3312610  5.8827726  5.4929841\n124  5.8431010  1.9921561  6.7220586  5.9173812  4.1679817  2.7718251\n125  5.5880898  3.3780356  7.1536672  3.1087424  2.3449714  4.1286655\n126  8.0541903  5.2289808  9.8613222  2.7264200  4.2128876  5.4224467\n127  6.1759080  3.3544844  7.6978611  2.8472940  3.2744404  3.7872131\n128  7.0558368  4.1627471  8.4516718  2.2683180  3.7107394  4.2563952\n129  4.4723137  4.0093447  5.3415092  7.0791010  4.8808770  4.6687180\n130  3.0656896  3.2010244  4.3326897  5.8867853  4.8217458  2.6212290\n131  7.5120658  4.9743189  9.4638455  3.3111932  4.7154639  4.7807430\n132  7.2812165  4.3207779  8.5378233  3.4155180  4.5785457  3.8503781\n133  4.7855854  2.1653131  6.7406258  3.3002760  2.4456673  2.2766526\n134  3.6787763  3.7659803  4.1841810  7.7219912  5.5109352  3.9168945\n135  5.3353148  3.4286625  6.0198743  5.7611794  4.2976135  3.8147034\n136  7.2421788  4.5160310  8.0792390  3.8280208  5.0567428  4.2120625\n137  7.5715199  4.7580246  9.0005079  2.5836134  4.5754722  4.5993434\n138  3.0306054  3.6446630  4.8940713  6.1932715  4.8317419  4.0005358\n139  1.5203180  4.4171190  2.5692847  7.4592135  6.0845171  3.8331045\n140  5.2161957  3.1523371  6.6343576  3.9202312  3.9146896  2.2522244\n141  3.1473197  4.1033705  5.0972150  6.1432182  4.8655249  4.5092169\n142  1.2881527  4.7836257  2.8011000  7.4356140  5.9137665  4.5864589\n143  4.5287325  3.5036758  6.1951664  4.9486262  3.0732708  4.0998331\n144  5.2528347  2.4642975  6.8866969  2.8819310  1.8800434  3.1011094\n145  4.2515432  2.0795079  5.6614730  4.2559364  3.2901330  2.4146585\n146  5.1760473  1.5304858  6.6736878  4.1539139  3.1802129  1.2364761\n147  1.4478485  5.7023586  2.4773043  8.0458152  7.1057258  5.1621129\n148  6.0081978  4.2410227  7.5835674  4.9332968  4.5189822  4.5809865\n149  6.3475171  3.6010905  7.1976219  6.6341810  5.0684595  4.5509037\n150  6.8140822  4.4296585  8.1345017  3.3109115  4.8001811  3.9613920\n151  6.9659183  3.2833604  7.9992168  4.6584969  4.5487922  2.8294940\n152  5.7568580  3.4214228  6.7775126  3.8018696  3.9311752  3.2046525\n153  4.1473963  3.5226652  5.5706850  6.1772462  4.8726550  4.1716586\n154  3.6516637  2.6519799  4.3359505  5.3750926  4.4483804  2.0454254\n155  3.6445437  2.9510261  5.5828386  4.6211936  4.0393033  2.5787617\n156  2.2088646  4.9090275  2.7295295  7.9177155  6.4643732  4.8395684\n157  1.9370584  4.7470910  2.8893731  7.5308190  6.3525933  4.3732098\n158  1.8830841  5.5714834  2.7113802  7.7536951  6.9334297  5.2185615\n            97         98         99        100        101        102\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98   3.7498625                                                       \n99   2.2305095  4.2035890                                            \n100  2.9434590  4.0967865  3.4742230                                 \n101  5.1378905  4.7789888  6.3746599  3.8015605                      \n102  3.7746996  4.5370905  4.8024172  2.4205193  2.6250440           \n103  4.2508600  5.4344467  4.9936877  2.2832735  2.8978108  1.8169419\n104  4.6529958  6.9010346  4.2748276  5.1373840  7.9384588  5.5848624\n105  4.2959613  5.2985034  3.1250419  4.4381567  7.1960596  5.1279611\n106  4.0607721  4.7913174  4.1046627  2.8321980  4.1650776  2.5419864\n107  6.2299906  5.4866459  7.2769731  4.7378977  1.4253892  3.3615112\n108  1.7174277  4.1769351  1.0112523  2.9995846  5.8954050  4.1395230\n109  4.7841999  5.5902383  3.7827755  5.1776219  7.7596725  5.6947504\n110  4.6372142  3.8566782  4.0995850  3.4828339  5.4257714  4.2623605\n111  4.5402367  5.4276635  5.5138596  2.4549767  3.0132917  2.4532263\n112  3.5346316  4.5247083  3.0901783  3.2288717  5.5105578  3.5194410\n113  3.4211821  3.5560890  3.3947586  2.2937332  3.9291709  2.4551375\n114  4.3208867  4.7973004  4.4144024  3.5094651  4.9602495  2.9210110\n115  3.7516302  4.9684255  4.6960927  1.8330602  3.4804842  2.2684229\n116  2.9442005  5.1009828  2.9397387  3.9198209  6.8407716  4.7180737\n117  4.1055103  6.2531158  3.2104252  4.6484907  7.6718283  5.3387135\n118  6.4933599  6.4965370  5.9925746  5.4539065  7.2318748  5.9159831\n119  2.4281797  4.7338524  2.0796503  3.6979061  6.7828968  4.8970760\n120  3.2877477  3.7362123  3.5119882  3.9501003  6.3231670  4.8276934\n121  3.3443125  5.4888831  2.6703519  3.5894654  5.9287822  4.2210238\n122  2.9005443  4.8839306  2.6537518  3.6450623  5.5710672  4.2078024\n123  4.0678862  4.5103115  5.2395498  4.2164373  3.6194999  3.7882393\n124  4.7955863  4.2491503  4.2519410  3.8165049  6.0105508  4.5989409\n125  2.8704484  4.7848400  2.1711680  3.9770422  6.2367789  4.6212115\n126  6.1891506  8.6445971  5.7370174  6.3827660  9.1061922  6.8596625\n127  2.3114347  4.6094813  1.8011955  3.6292303  6.6303710  4.6603219\n128  3.6734011  5.7577428  2.7048862  4.7045613  7.6079213  5.4468402\n129  4.7164258  4.4409618  4.6936433  3.7504462  4.4655695  4.5573025\n130  3.4382494  3.6990941  4.3735960  2.7088634  3.3157161  1.5985861\n131  4.9483385  6.9456103  4.9540314  5.8364192  8.4133923  6.1764578\n132  4.7479856  6.5580799  4.1345635  4.9188078  7.8127431  5.4199196\n133  2.7469679  5.0780904  3.0001560  2.8044800  5.5913232  3.4850335\n134  5.0629894  4.6675228  5.4906864  3.2699658  3.3053779  3.4755345\n135  3.6825248  2.9202713  2.9635542  3.7731517  5.2871612  4.2959719\n136  4.5833516  5.8205237  3.6784553  4.7764244  7.5242282  5.3853355\n137  4.8017763  6.7150358  4.1616586  5.4355762  8.2426301  5.9513994\n138  2.9350722  3.0038923  4.1552358  2.6548694  3.3648902  3.1659214\n139  4.9903953  4.9287751  5.9855751  3.2871724  1.6190016  1.9298330\n140  3.7462967  5.5446710  3.8884099  3.4296225  5.8332506  3.3226249\n141  3.4512009  3.3430629  4.4372092  3.5744803  3.7428158  3.6604554\n142  4.8219833  5.0708264  5.9219094  3.6055408  1.7156419  2.5406466\n143  3.7375668  4.8752576  3.5754734  3.9765499  5.2325762  4.3666392\n144  2.6661400  4.8614725  2.2015910  3.2906101  5.9184905  3.9630646\n145  2.1481175  3.7627731  2.3431794  1.4136410  4.5843753  2.8247865\n146  3.7038057  4.9727521  3.5238581  2.9461828  5.7356954  3.5923842\n147  5.7198065  5.8535737  6.9961745  4.3181445  1.9112850  2.8278638\n148  2.0264773  4.3745890  2.9447561  3.6464138  6.1887863  4.7482883\n149  6.1290977  5.8318630  5.7745411  4.9158576  6.7229752  5.6649819\n150  4.5235664  6.5703045  4.3445938  4.6673312  7.3958442  5.0496586\n151  4.8163282  5.8933281  4.2216448  4.3845344  7.2555737  5.0142095\n152  3.1626176  4.3797987  2.3639449  3.5630126  6.0410284  4.0661239\n153  2.8354425  3.3282939  3.9073347  2.3044836  4.1382381  3.6802188\n154  3.6100849  4.1068175  3.8436699  2.0026026  3.7427961  1.7107171\n155  2.7681702  4.8009576  3.8928193  2.0059387  4.3446426  2.3229618\n156  4.8061093  4.5826148  5.9208340  3.1333979  1.4426969  2.8351294\n157  4.6542020  3.7113187  5.7579940  3.6746670  1.8094740  2.5123287\n158  4.6060595  4.1399489  6.0447537  3.9069381  1.4922592  2.7248074\n           103        104        105        106        107        108\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104  6.0493140                                                       \n105  5.6279002  2.8403475                                            \n106  2.4694093  5.2361365  4.1190918                                 \n107  3.6207917  8.5961788  7.7102445  4.6271905                      \n108  4.4216195  3.9436944  3.1415405  3.5573335  6.8492278           \n109  6.4227914  2.7925603  1.1968103  5.0316711  8.2354297  3.8451963\n110  4.4743946  5.7639503  4.2202671  3.2065859  5.9224274  3.9649010\n111  1.9177697  6.7055367  6.4347439  3.6595156  4.0712430  4.8866068\n112  3.9714472  3.4096228  2.1871818  2.3075216  6.0473950  2.6847802\n113  2.8478036  5.0611017  3.5272778  1.5052635  4.4758393  3.0310432\n114  3.7633944  4.1103216  3.0275918  2.3319980  5.2657327  3.8966837\n115  1.9591522  5.5624110  5.4577974  3.2507465  4.5107665  4.0737237\n116  5.4178790  2.3981429  2.7153188  4.7082315  7.7212973  2.4886949\n117  5.7323891  2.0074364  1.9684501  4.5037816  8.3633033  2.9109565\n118  5.8719441  6.2842735  5.7623098  4.8476474  7.6333860  5.7523217\n119  5.3892247  3.6992709  3.3606365  4.7373170  7.8141144  1.7047550\n120  5.7663562  5.0300980  4.1019968  5.2864835  7.2781867  3.2888796\n121  3.9323792  4.2441334  3.7393520  2.9345624  6.6371788  2.4301677\n122  4.0503262  4.9059504  4.3465940  3.1472604  6.4024802  2.3722642\n123  4.0284157  6.7097921  6.3099895  4.2435033  4.1949719  4.8986769\n124  4.9390158  5.6452813  4.2245693  3.6689901  6.6726194  3.9911666\n125  4.7773613  4.2623184  3.7189489  3.6854515  7.0576404  2.0417596\n126  6.8289893  3.0161457  5.0010741  6.1229792  9.7818504  5.3599553\n127  5.1955564  2.9034392  2.6660329  4.3907863  7.5500024  1.5019742\n128  5.9166982  1.9974964  1.8033705  4.7156248  8.3140524  2.6319552\n129  3.9577441  7.7325115  6.2823530  3.4792271  5.1579553  4.5209085\n130  2.9459423  5.2609897  4.6074951  2.7684307  4.1398746  3.7359044\n131  6.8550662  2.2718861  3.7648915  5.7743501  9.1471864  4.4287004\n132  5.9608408  1.6756431  2.0567258  4.8422594  8.4014662  3.8054277\n133  3.6785446  3.3083215  3.3788281  2.8969600  6.4966481  2.1465050\n134  3.0903594  7.7591979  6.5870676  3.4472347  4.1169120  5.0633088\n135  4.7246103  5.9705946  3.8959998  3.1543000  5.8912001  3.0110170\n136  5.9931818  2.4117733  1.3057849  4.8926405  8.0269297  3.6969692\n137  6.4992099  0.9781922  2.3562212  5.3840583  8.8558576  3.9556318\n138  3.4582280  6.3887630  5.5850044  3.5021464  4.4666762  3.6640306\n139  2.1064085  7.0863433  6.4000176  3.0468990  2.2183324  5.3956980\n140  4.0137356  2.7284120  2.8318117  3.1034795  6.4624988  3.1752722\n141  3.9998330  6.5842819  5.8050837  3.6841192  4.6528991  4.0093330\n142  2.2585672  7.4880695  6.9030095  3.4070183  2.5400395  5.3871071\n143  4.0212596  5.9355576  4.9797247  2.8920261  5.9810484  3.3175591\n144  4.1615336  3.5407581  3.2772342  3.2051316  6.7978512  1.6711600\n145  3.0458024  4.2378972  3.3869760  2.5190791  5.4996003  1.7406962\n146  3.9582193  3.5038740  2.9578338  2.8544981  6.4997811  2.8723110\n147  2.9570188  7.6605383  7.4033861  4.3384228  2.1351729  6.4250433\n148  5.3148843  4.9348918  4.6777626  5.2030755  7.3578371  2.5297576\n149  5.5648074  6.7317072  5.9911184  4.8812072  7.2565905  5.5687649\n150  5.6113296  1.1131807  2.7659794  5.0011704  8.0316017  3.9683280\n151  5.6006606  3.1914514  2.5965610  4.5952637  7.9309210  3.8665261\n152  4.6900396  3.3134729  1.5316964  3.3142173  6.6431521  2.2074998\n153  3.8350712  6.3172439  5.6657164  4.3151130  5.3626654  3.5254212\n154  2.4088529  4.7584261  3.7773535  1.9916330  4.3349973  3.3615014\n155  2.7696105  4.1637732  4.3311833  3.0306774  5.3025405  3.0604457\n156  2.6169989  7.8505573  7.0823565  4.0369315  2.5240234  5.4693582\n157  3.3239645  7.1963767  6.2787418  3.5484584  2.3803675  5.2700281\n158  3.5043775  7.4326915  6.8187811  4.3904734  2.2470864  5.5889854\n           109        110        111        112        113        114\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110  4.8910092                                                       \n111  7.1501331  4.7571748                                            \n112  2.9433605  3.0529431  4.8613997                                 \n113  4.3233206  2.6997355  3.7385557  1.9853285                      \n114  3.5715680  3.7571867  4.7049629  1.7929022  2.1234248           \n115  6.1280818  4.0585928  1.4496268  3.8989292  3.1716556  3.9637974\n116  2.8594721  4.9439025  5.6723067  3.0764656  4.1228500  3.7839145\n117  2.4714922  5.1959948  6.4413342  2.9212715  4.3369197  3.7705473\n118  6.2774168  3.0371165  6.0974453  4.5498480  5.0436374  5.3849894\n119  3.7844308  4.8085700  5.4399079  3.5659004  4.1148716  4.5155633\n120  4.2496099  4.9748720  5.4928155  4.3299934  4.2397916  4.5982995\n121  4.5754542  4.0167249  5.0199008  2.6101125  3.1527320  3.9711689\n122  5.1015871  3.9330732  4.8887769  2.9776452  3.2072440  4.3332374\n123  6.7273638  5.0485678  4.7459783  4.5519472  3.9741767  4.8765368\n124  4.8909275  1.7096471  4.8264235  3.3626163  3.2380757  3.9573104\n125  4.3121180  3.8540319  5.4811330  2.7641002  3.4716620  4.3161143\n126  5.2409226  6.5369732  7.4113089  4.8161074  6.4778410  5.8233985\n127  3.0019673  4.3952138  5.5318759  2.8066576  3.7465002  3.9763541\n128  2.0437039  5.0398627  6.6618903  2.8167964  4.3017766  3.9403409\n129  7.2145178  3.7634266  4.2669695  4.7942872  3.4525145  5.3875111\n130  5.0766451  3.6885270  3.0577911  3.0574921  2.2352754  2.6523819\n131  3.6812551  6.1503795  7.2326482  4.0344224  5.6763644  4.6026518\n132  2.2177207  5.4523628  6.6303865  3.1673257  4.6203854  3.5492875\n133  4.1325975  3.9311558  4.0998918  2.1931176  2.9756406  3.0764107\n134  7.4288451  4.0559809  2.5865602  5.0236974  3.4133237  5.0138792\n135  4.6088254  2.9304455  5.2909352  3.0754447  2.2694897  3.8095579\n136  1.0376182  5.0829028  6.7616118  2.9896591  4.2350434  3.4947800\n137  2.1178864  5.6769361  7.1897441  3.3404582  5.0944337  4.1409817\n138  6.2129129  3.7468444  3.3116561  3.9991407  2.9734004  4.3361771\n139  7.0475807  4.5910054  2.3689359  4.5528459  3.1558939  3.9379419\n140  3.3666186  4.4288122  4.6937758  2.1994244  3.1351411  2.0896432\n141  6.3505928  3.9707856  4.1069885  4.0184990  3.2799140  4.5688928\n142  7.5926998  4.8582366  2.6806834  4.9803609  3.6142006  4.7688085\n143  5.8363883  3.9153639  4.8437237  3.3614136  3.1181742  4.4727078\n144  3.9478329  3.6412667  4.7122040  2.2244953  3.0649158  3.6597749\n145  4.1653282  3.0785476  3.4545285  2.2099799  1.8984202  2.9911075\n146  3.7102509  3.3271913  4.1894659  1.9748313  2.7387000  2.5367876\n147  7.9130553  5.6988460  3.2818416  5.5449003  4.4485590  4.9193424\n148  5.0531752  5.4726046  5.0856060  4.5260246  4.4819145  5.1179936\n149  6.5880918  2.5685548  5.4784774  4.8526386  4.8521470  5.5685714\n150  2.7222724  5.6357912  6.1923830  3.3163930  4.6996249  3.6383224\n151  2.9477255  4.6746715  5.7758031  3.2524117  4.1662062  3.4469333\n152  2.1882781  3.9941322  5.5240506  1.7373255  2.5769250  2.5559604\n153  6.2596886  3.9904612  3.1476131  4.5149954  3.6051314  4.9919956\n154  4.4203493  3.0835682  3.1033895  2.3298336  1.3677063  2.1050537\n155  4.9275779  4.2784503  2.9511641  3.0028794  2.9463961  3.0612047\n156  7.7497711  4.8705508  2.3143324  5.4239559  3.8096844  5.1849594\n157  6.7671871  4.3441593  3.4943205  4.5054382  3.1307661  4.0123487\n158  7.2175958  5.3019806  3.6847515  5.1923728  3.9025769  4.7195595\n           115        116        117        118        119        120\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110                                                                  \n111                                                                  \n112                                                                  \n113                                                                  \n114                                                                  \n115                                                                  \n116  4.6287593                                                       \n117  5.5178711  2.1824310                                            \n118  5.3004626  6.3330185  6.3000405                                 \n119  4.6158685  1.6761691  2.7891843  6.4740836                      \n120  4.8783038  2.7816438  4.3172302  7.2057969  2.3587062           \n121  4.2285553  4.1044456  3.5944822  4.9838315  3.8944614  5.4832631\n122  4.1319268  4.3222995  4.3064891  5.0273962  3.9116789  5.3546178\n123  4.1732127  5.9644722  6.7597806  6.2983666  6.1028362  6.4133366\n124  4.1808165  4.6037522  4.9695990  3.5385378  4.3463542  4.3923301\n125  4.5834508  3.7079007  3.7120217  4.8886572  3.3660108  4.8998114\n126  6.3136156  4.7041918  4.0257288  5.7144676  5.4387995  7.2398199\n127  4.5064008  1.2888454  2.3452301  5.8693179  1.2664175  2.9359760\n128  5.6029010  1.9967989  1.2839513  6.2060569  2.6824387  4.1967470\n129  4.3300987  6.5410992  6.7667524  5.7051261  5.8015177  6.1786743\n130  2.5293380  4.1022872  5.0259981  5.5074271  4.2754991  3.9456721\n131  6.1198501  2.6555195  2.8881924  6.5910527  3.9140940  5.0574381\n132  5.6592524  2.3428152  1.2930276  6.4690408  3.4759517  4.4727466\n133  3.1015075  2.4789818  2.8653590  5.0378790  2.6492664  4.0150071\n134  3.2546346  6.4811430  6.9871288  5.9617052  5.8912226  5.7308926\n135  4.8410635  4.6842689  4.8266093  5.5278768  4.1231516  4.2199220\n136  5.7622346  2.6164054  2.1359473  6.5027599  3.5943535  4.1764649\n137  6.0509889  2.4530214  1.8481648  6.3362142  3.7198764  4.9523339\n138  2.8382206  4.8426300  5.9482467  5.7267512  4.5105266  4.4702930\n139  2.6963500  6.2070972  6.8316734  6.1426321  6.2678610  6.0396219\n140  3.8389015  2.5127032  2.4185368  5.6071180  3.3601692  4.1106435\n141  3.6359424  5.3069597  6.2445238  5.5782793  5.0570771  5.2404115\n142  2.9754251  6.6091011  7.2211215  6.2269453  6.4809378  6.5498142\n143  4.2965516  5.2482990  5.2256590  5.1757478  4.8003356  5.8972743\n144  3.7681238  2.9161992  3.0272444  4.6788903  2.6895073  4.2766981\n145  2.5987568  2.9107309  3.5540614  5.0752352  2.6533153  3.3631039\n146  3.2780005  2.7177722  2.9915735  4.6130909  3.0238080  3.7519229\n147  3.4071776  7.0272222  7.7910185  6.7933441  7.3007936  7.1226533\n148  4.4538536  2.7065997  4.2392760  7.4077963  1.7376589  2.1456456\n149  4.7582686  6.2787709  6.5794402  1.7756495  6.1856400  6.6333617\n150  5.1058340  2.1839994  2.2364280  6.4806522  3.6023433  4.5114825\n151  4.9421776  2.6213228  2.6756812  5.9777325  3.2253207  3.5161104\n152  4.6866012  2.5048720  2.3010496  5.9278315  2.8244767  3.4940885\n153  2.6767861  4.5392668  5.8785706  5.9804961  3.9238391  3.7810850\n154  2.5272351  3.9657428  4.3490163  5.0205402  4.1320011  4.1612019\n155  2.0130851  3.1077815  4.0436497  5.6496110  3.4232112  3.9633353\n156  2.8962645  6.6282266  7.4751774  6.6703187  6.3560027  6.0531968\n157  3.4331177  6.0695604  6.9593729  6.1994796  6.1459383  5.6232660\n158  3.6373202  6.3011920  7.3635411  7.0924302  6.4158619  5.8819678\n           121        122        123        124        125        126\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110                                                                  \n111                                                                  \n112                                                                  \n113                                                                  \n114                                                                  \n115                                                                  \n116                                                                  \n117                                                                  \n118                                                                  \n119                                                                  \n120                                                                  \n121                                                                  \n122  1.2751052                                                       \n123  4.5004938  3.8572995                                            \n124  4.5613693  4.4648218  6.0370002                                 \n125  1.6036563  1.2107520  4.4251151  4.2603764                      \n126  4.6614054  5.3106721  7.5583368  6.4414166  4.8583572           \n127  3.2497940  3.3530259  5.4643635  4.2507977  2.6251264  4.7422220\n128  3.3975948  3.9365391  6.2389075  5.0228902  3.1506384  4.1620168\n129  4.3509239  3.8165942  4.6009084  4.3836210  4.6133941  8.3204317\n130  4.2742996  4.0440782  3.9365026  3.6931377  4.2022474  6.6824295\n131  5.2140475  5.5000259  7.0822444  5.7456128  4.8808580  3.7603477\n132  4.4566013  5.1862138  7.0932741  5.1919305  4.6104755  4.1603388\n133  2.6871215  2.8479182  4.8365821  3.7267882  2.8457623  4.2724352\n134  5.2231458  4.8755518  5.1121436  4.1486229  5.5309183  8.5218291\n135  3.6365795  3.3068678  4.8581455  3.2993632  3.2897100  7.3580058\n136  4.3928172  5.0659783  6.6885834  5.1103305  4.3895734  5.0500209\n137  4.4002291  5.0181841  6.8815623  5.5781217  4.2584673  3.5008732\n138  4.3926444  3.6746661  3.0573333  4.1161922  4.1581517  7.4637498\n139  5.2241314  5.0029125  3.7742639  5.0249710  5.6563451  8.0362699\n140  3.5810214  4.1224400  5.5632138  4.2568291  3.9215443  4.5003512\n141  4.2761855  3.3628570  2.3857895  4.5272034  3.8658965  7.4207738\n142  4.9501531  4.5240944  3.1498743  5.4759763  5.3512767  8.1705256\n143  2.3738466  1.5896660  3.8164712  4.4280007  2.5168791  6.2835159\n144  1.7019548  1.7615269  4.6785900  3.7074364  1.2970886  4.2730231\n145  2.7099648  2.7337082  4.1945266  3.2793918  2.8320382  5.5993796\n146  3.4958874  3.7719822  5.5183499  2.7687117  3.6077323  4.6807343\n147  6.0303936  5.7879847  3.5416086  6.3356156  6.4440112  8.4686670\n148  4.7654390  4.5034448  5.8509345  5.0789027  4.2606140  6.7211345\n149  5.2628686  5.1580487  6.2786890  2.9025480  5.1076184  6.5547921\n150  4.5086082  5.1576877  6.5926523  5.4676837  4.6204907  3.8429574\n151  4.9078945  5.3986115  7.2468025  3.8575732  4.8923487  5.1071740\n152  3.0599395  3.4721215  5.2444069  4.1389422  3.0197820  5.4548838\n153  4.7315219  4.1800118  4.2930552  4.0635813  4.4424099  7.4492560\n154  3.4805403  3.6873287  4.2509842  3.4233832  3.8431608  6.1683584\n155  3.7044593  3.7528031  4.2682184  4.2780689  3.9671259  5.4172798\n156  5.5852018  5.1989103  3.8098212  5.4466911  5.9012703  8.8229682\n157  5.4288712  4.9552749  3.0644367  4.9105957  5.4428383  8.4159291\n158  5.8018618  5.3377523  2.8012868  5.9513315  5.8341005  8.7663618\n           127        128        129        130        131        132\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110                                                                  \n111                                                                  \n112                                                                  \n113                                                                  \n114                                                                  \n115                                                                  \n116                                                                  \n117                                                                  \n118                                                                  \n119                                                                  \n120                                                                  \n121                                                                  \n122                                                                  \n123                                                                  \n124                                                                  \n125                                                                  \n126                                                                  \n127                                                                  \n128  1.8045093                                                       \n129  5.7870595  6.7028134                                            \n130  4.0534822  4.9983989  4.5854769                                 \n131  3.4087630  2.9521690  8.0574552  5.5742401                      \n132  2.9958055  1.9822109  7.4252154  5.0997412  2.6408144           \n133  2.3529255  3.0578697  4.8604651  3.1539013  3.5337508  3.3419667\n134  6.0643789  7.1565016  2.5497958  3.5587452  8.1110883  7.3731979\n135  3.9253037  4.5633960  3.3174703  3.6499687  6.3248255  5.4439389\n136  2.8788257  1.9244641  7.1865467  4.9825651  3.7532037  1.7615341\n137  2.8696071  1.6096449  7.7809201  5.5015145  2.2003570  1.5426656\n138  4.4180987  5.6980132  3.1412419  2.7794988  6.3826830  6.3346425\n139  6.0772503  6.9175397  4.1228141  2.5695433  7.4963282  6.9196626\n140  2.9537914  2.9361595  6.0047979  3.1739828  3.3439569  2.2719245\n141  4.7367811  5.8689246  3.3017589  3.1964762  6.5610789  6.7068586\n142  6.2493877  7.1686593  3.6931322  3.1994588  7.9500373  7.5131368\n143  4.4075657  4.9723991  2.9408723  4.1265760  6.3103757  6.0217555\n144  2.0648175  2.7870552  4.6995884  3.5218812  4.2102689  3.8197104\n145  2.4361067  3.5093641  3.8148172  2.6001725  4.7979753  4.0127698\n146  2.8332800  3.3701291  5.0449321  2.9792747  3.7406728  3.1085251\n147  6.9433507  7.7361326  5.3864478  3.6060250  8.1462525  7.7828519\n148  2.5555847  4.0818665  5.7822099  4.2059011  4.9564111  4.7604443\n149  5.8026159  6.4784295  5.1159687  5.1940198  7.0075679  6.7704317\n150  2.9616079  2.4026654  7.5955744  4.7592117  2.7046579  1.5167275\n151  3.2162595  3.2495569  6.8269274  4.3298011  3.6637199  2.2743140\n152  2.1989343  2.0716705  5.3800398  3.6221273  4.1335319  2.7297564\n153  4.1147262  5.6745481  3.7734012  3.2991791  6.4894049  6.2639624\n154  3.7829737  4.4331904  4.2673824  1.7144075  5.5845250  4.4423719\n155  3.2637620  4.2023546  4.9419543  2.4166887  4.4645445  4.1605379\n156  6.3072513  7.4401018  3.6397583  3.4779100  8.3156135  7.7161337\n157  5.8631679  6.7923480  4.1714690  2.4154857  7.4105666  7.0627917\n158  6.1166704  7.1050026  4.9324572  3.0158823  7.8011359  7.4543303\n           133        134        135        136        137        138\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110                                                                  \n111                                                                  \n112                                                                  \n113                                                                  \n114                                                                  \n115                                                                  \n116                                                                  \n117                                                                  \n118                                                                  \n119                                                                  \n120                                                                  \n121                                                                  \n122                                                                  \n123                                                                  \n124                                                                  \n125                                                                  \n126                                                                  \n127                                                                  \n128                                                                  \n129                                                                  \n130                                                                  \n131                                                                  \n132                                                                  \n133                                                                  \n134  4.8471694                                                       \n135  4.0218545  4.1166388                                            \n136  3.9072850  7.2813638  4.8453645                                 \n137  3.6275163  7.9761670  5.7354092  2.0013731                      \n138  3.6211292  3.2001900  3.5977517  6.1708905  6.5315875           \n139  4.6434879  2.7513702  4.8482721  6.8403297  7.4078197  3.1773272\n140  2.0268404  5.5902382  4.5312443  2.9956358  3.0574538  4.7414655\n141  4.0095441  3.7712507  3.6131111  6.4535643  6.6582799  1.5795246\n142  4.8868142  2.9131829  4.9413911  7.3944425  7.8359031  3.0300122\n143  3.4841260  4.2497166  3.1003490  5.8886406  5.9884703  3.4589231\n144  1.7802604  5.0822942  3.4393561  3.8757571  3.7101872  3.9501768\n145  1.8128753  3.8507201  2.9645828  3.9084635  4.4056073  2.6786008\n146  1.5586864  4.6525301  3.8587018  3.5844718  3.6768839  4.0137582\n147  5.5952413  4.3097235  6.1869304  7.6629363  8.0754378  4.0451609\n148  3.3958584  5.6662561  4.5727636  4.7982836  5.0683355  4.0057154\n149  5.0425264  5.2010325  5.2447198  6.7299704  6.8274969  5.0942366\n150  3.2611139  7.3502455  5.8398929  2.1221285  1.6504772  6.1108109\n151  3.2870805  6.2812226  4.9155487  2.7431087  3.1966849  5.7858284\n152  2.8185573  5.6483558  2.9982649  2.1064908  3.0720805  4.5551947\n153  3.7478660  3.4545006  4.1079939  6.0838078  6.5326457  1.7758190\n154  2.9339935  3.5298661  3.2975559  4.1674951  4.9816669  3.3665158\n155  1.8285797  4.3563300  4.5512886  4.5449456  4.6119698  3.0286114\n156  5.2383646  2.5032003  5.0328439  7.4886332  8.1661789  2.8140296\n157  4.9022544  3.3430596  4.3243666  6.7442275  7.3622941  2.6081150\n158  5.3753038  4.1751132  5.1433888  7.0659682  7.6865823  3.0288888\n           139        140        141        142        143        144\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110                                                                  \n111                                                                  \n112                                                                  \n113                                                                  \n114                                                                  \n115                                                                  \n116                                                                  \n117                                                                  \n118                                                                  \n119                                                                  \n120                                                                  \n121                                                                  \n122                                                                  \n123                                                                  \n124                                                                  \n125                                                                  \n126                                                                  \n127                                                                  \n128                                                                  \n129                                                                  \n130                                                                  \n131                                                                  \n132                                                                  \n133                                                                  \n134                                                                  \n135                                                                  \n136                                                                  \n137                                                                  \n138                                                                  \n139                                                                  \n140  4.8539510                                                       \n141  3.5161197  5.1657709                                            \n142  1.5328516  5.4595266  3.1703916                                 \n143  4.5818964  4.7914605  2.9607683  4.0901347                      \n144  5.1526202  2.9107367  3.9451018  5.0895596  2.8742937           \n145  3.9407411  2.7063945  3.3263160  4.1711588  3.2698934  2.1258601\n146  4.6049933  1.9273477  4.4692197  5.2178330  4.1100059  2.5109081\n147  1.8245248  5.7390113  4.2150372  1.9682863  5.5404492  6.0469260\n148  5.9952765  4.1766787  4.8156004  6.1255493  5.2213131  3.7076482\n149  5.7883145  5.7717452  5.3254731  5.8360716  5.2358227  4.8215476\n150  6.5778823  2.2999494  6.4621284  7.1371568  6.0804910  3.7527866\n151  6.2708802  2.5704020  6.3261310  7.0485175  5.9254062  3.8600834\n152  5.4057539  2.4738335  4.7410336  5.8106063  4.1061306  2.6188543\n153  4.0520217  4.9087165  3.0036311  3.9988911  4.3471135  4.0310218\n154  2.8864361  2.6995682  3.7848663  3.5351367  3.9153721  3.1277746\n155  3.5718277  2.3556198  3.7789613  3.9874582  4.2579260  3.0361591\n156  1.9770394  5.8033036  3.4886614  1.7735012  4.9011547  5.5624361\n157  1.7207085  5.1695065  2.7159670  2.1546453  4.5369380  5.1827004\n158  2.3543136  5.5955249  3.2705214  2.3566152  5.1844867  5.6213852\n           145        146        147        148        149        150\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110                                                                  \n111                                                                  \n112                                                                  \n113                                                                  \n114                                                                  \n115                                                                  \n116                                                                  \n117                                                                  \n118                                                                  \n119                                                                  \n120                                                                  \n121                                                                  \n122                                                                  \n123                                                                  \n124                                                                  \n125                                                                  \n126                                                                  \n127                                                                  \n128                                                                  \n129                                                                  \n130                                                                  \n131                                                                  \n132                                                                  \n133                                                                  \n134                                                                  \n135                                                                  \n136                                                                  \n137                                                                  \n138                                                                  \n139                                                                  \n140                                                                  \n141                                                                  \n142                                                                  \n143                                                                  \n144                                                                  \n145                                                                  \n146  2.1384694                                                       \n147  5.0310619  5.7786258                                            \n148  3.0249846  3.9432585  6.9159857                                 \n149  4.7443085  4.5754095  6.5276039  6.8538970                      \n150  3.9514698  3.2189872  7.1752992  4.6396421  6.7566930           \n151  3.6609649  2.2303921  7.4038297  4.3794119  5.9290381  2.6652413\n152  2.4636760  2.7550033  6.4447482  3.8194579  6.0367630  3.1090457\n153  2.6059317  4.0374321  4.9126697  3.2321151  5.0613370  5.9620816\n154  2.0113520  2.5724974  4.0325802  4.4697077  4.8319666  4.2472061\n155  1.9064007  2.4012819  4.3215452  3.4037700  5.4058558  3.7243026\n156  4.0957086  5.4133494  2.6220276  5.7650999  5.9611507  7.3630043\n157  4.0135462  4.8783221  2.4210832  5.7698001  5.8063879  6.7371074\n158  4.4229960  5.6113127  2.1527929  5.7853583  6.6518450  6.9312950\n           151        152        153        154        155        156\n2                                                                    \n3                                                                    \n4                                                                    \n5                                                                    \n6                                                                    \n7                                                                    \n8                                                                    \n9                                                                    \n10                                                                   \n11                                                                   \n12                                                                   \n13                                                                   \n14                                                                   \n15                                                                   \n16                                                                   \n17                                                                   \n18                                                                   \n19                                                                   \n20                                                                   \n21                                                                   \n22                                                                   \n23                                                                   \n24                                                                   \n25                                                                   \n26                                                                   \n27                                                                   \n28                                                                   \n29                                                                   \n30                                                                   \n31                                                                   \n32                                                                   \n33                                                                   \n34                                                                   \n35                                                                   \n36                                                                   \n37                                                                   \n38                                                                   \n39                                                                   \n40                                                                   \n41                                                                   \n42                                                                   \n43                                                                   \n44                                                                   \n45                                                                   \n46                                                                   \n47                                                                   \n48                                                                   \n49                                                                   \n50                                                                   \n51                                                                   \n52                                                                   \n53                                                                   \n54                                                                   \n55                                                                   \n56                                                                   \n57                                                                   \n58                                                                   \n59                                                                   \n60                                                                   \n61                                                                   \n62                                                                   \n63                                                                   \n64                                                                   \n65                                                                   \n66                                                                   \n67                                                                   \n68                                                                   \n69                                                                   \n70                                                                   \n71                                                                   \n72                                                                   \n73                                                                   \n74                                                                   \n75                                                                   \n76                                                                   \n77                                                                   \n78                                                                   \n79                                                                   \n80                                                                   \n81                                                                   \n82                                                                   \n83                                                                   \n84                                                                   \n85                                                                   \n86                                                                   \n87                                                                   \n88                                                                   \n89                                                                   \n90                                                                   \n91                                                                   \n92                                                                   \n93                                                                   \n94                                                                   \n95                                                                   \n96                                                                   \n97                                                                   \n98                                                                   \n99                                                                   \n100                                                                  \n101                                                                  \n102                                                                  \n103                                                                  \n104                                                                  \n105                                                                  \n106                                                                  \n107                                                                  \n108                                                                  \n109                                                                  \n110                                                                  \n111                                                                  \n112                                                                  \n113                                                                  \n114                                                                  \n115                                                                  \n116                                                                  \n117                                                                  \n118                                                                  \n119                                                                  \n120                                                                  \n121                                                                  \n122                                                                  \n123                                                                  \n124                                                                  \n125                                                                  \n126                                                                  \n127                                                                  \n128                                                                  \n129                                                                  \n130                                                                  \n131                                                                  \n132                                                                  \n133                                                                  \n134                                                                  \n135                                                                  \n136                                                                  \n137                                                                  \n138                                                                  \n139                                                                  \n140                                                                  \n141                                                                  \n142                                                                  \n143                                                                  \n144                                                                  \n145                                                                  \n146                                                                  \n147                                                                  \n148                                                                  \n149                                                                  \n150                                                                  \n151                                                                  \n152  3.0658355                                                       \n153  5.4618827  4.7596242                                            \n154  3.8403454  2.9089008  3.6300745                                 \n155  3.9491281  3.5195709  3.0926819  2.4729616                      \n156  7.0532202  5.9954542  3.2578203  3.6716055  4.0663810           \n157  6.4549713  5.2264139  3.8017486  3.1902152  3.9470649  2.3751211\n158  7.0593166  5.6728636  3.9439867  3.7541356  4.1308025  2.3365693\n           157\n2             \n3             \n4             \n5             \n6             \n7             \n8             \n9             \n10            \n11            \n12            \n13            \n14            \n15            \n16            \n17            \n18            \n19            \n20            \n21            \n22            \n23            \n24            \n25            \n26            \n27            \n28            \n29            \n30            \n31            \n32            \n33            \n34            \n35            \n36            \n37            \n38            \n39            \n40            \n41            \n42            \n43            \n44            \n45            \n46            \n47            \n48            \n49            \n50            \n51            \n52            \n53            \n54            \n55            \n56            \n57            \n58            \n59            \n60            \n61            \n62            \n63            \n64            \n65            \n66            \n67            \n68            \n69            \n70            \n71            \n72            \n73            \n74            \n75            \n76            \n77            \n78            \n79            \n80            \n81            \n82            \n83            \n84            \n85            \n86            \n87            \n88            \n89            \n90            \n91            \n92            \n93            \n94            \n95            \n96            \n97            \n98            \n99            \n100           \n101           \n102           \n103           \n104           \n105           \n106           \n107           \n108           \n109           \n110           \n111           \n112           \n113           \n114           \n115           \n116           \n117           \n118           \n119           \n120           \n121           \n122           \n123           \n124           \n125           \n126           \n127           \n128           \n129           \n130           \n131           \n132           \n133           \n134           \n135           \n136           \n137           \n138           \n139           \n140           \n141           \n142           \n143           \n144           \n145           \n146           \n147           \n148           \n149           \n150           \n151           \n152           \n153           \n154           \n155           \n156           \n157           \n158  1.5038563"
  },
  {
    "objectID": "W13.html#calculate-hierarchical-clustering-in-r-1",
    "href": "W13.html#calculate-hierarchical-clustering-in-r-1",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Calculate hierarchical clustering in R",
    "text": "Calculate hierarchical clustering in R\n\nhclust from base-R\n\n\nowid_df <- as.data.frame(owid_BoxCoxData) # base-R data.frame \nrow.names(owid_df) <- owid_inds$iso_code # base-R way to have row names for hclust\nhc_owid <- owid_df |> dist(method = \"euclidean\") |> hclust()\nhc_owid\n\n\nCall:\nhclust(d = dist(owid_df, method = \"euclidean\"))\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 158 \n\nglimpse(hc_owid)\n\nList of 7\n $ merge      : int [1:157, 1:2] -84 -66 -60 -20 -2 -19 -52 -25 -39 -82 ...\n $ height     : num [1:157] 0.683 0.687 0.687 0.76 0.809 ...\n $ order      : int [1:158] 126 13 94 57 76 131 59 8 128 40 ...\n $ labels     : chr [1:158] \"AFG\" \"ALB\" \"DZA\" \"ATG\" ...\n $ method     : chr \"complete\"\n $ call       : language hclust(d = dist(owid_df, method = \"euclidean\"))\n $ dist.method: chr \"euclidean\"\n - attr(*, \"class\")= chr \"hclust\""
  },
  {
    "objectID": "W13.html#plot-the-dendrogram",
    "href": "W13.html#plot-the-dendrogram",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Plot the dendrogram",
    "text": "Plot the dendrogram\n\nplot(hc_owid, cex = 0.4) # base-R way of plotting hclust object"
  },
  {
    "objectID": "W13.html#plot-height-of-hierarchie-steps",
    "href": "W13.html#plot-height-of-hierarchie-steps",
    "title": "W#13: Probability, Cluster Analysis",
    "section": "Plot height of hierarchie steps",
    "text": "Plot height of hierarchie steps\n\ntibble(height = hc_owid$height |> tail(10), # last ten values of height\n       num_cluster = 10:1) |> \n ggplot(aes(num_cluster,height)) + geom_point() + geom_line()"
  },
  {
    "objectID": "W13.html#countries",
    "href": "W13.html#countries",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Countries",
    "text": "Countries\n\ncutree(hc_owid, k = 3) |> sort()\n\nAFG DZA AZE BGD BEN BOL BWA BRA BFA BDI KHM CMR CPV CAF DJI EGY GNQ ERI SWZ ETH \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \nGAB GMB GHA GTM GIN GUY HTI HND IND IDN IRN IRQ JOR KEN KGZ LAO LBR LBY MDG MWI \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \nMLI MNG MOZ MMR NPL NIC NER PAK PRY PHL STP ZAF SDN SUR TJK TZA TLS TGO UGA UZB \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \nVEN YEM ZMB ZWE ALB ARG ARM AUS AUT BRB BLR BEL BIH BGR CAN CHL CHN COL CRI HRV \n  1   1   1   1   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \nCYP CZE DNK DOM ECU SLV EST FIN FRA GEO DEU GRC HUN ISL IRL ISR ITA JAM JPN KAZ \n  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \nLVA LBN LTU LUX MYS MLT MEX MDA MNE MAR NLD NZL MKD NOR PAN PER POL PRT ROU RUS \n  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \nSGP SVK SVN KOR ESP LKA SWE CHE THA TUN TUR UKR GBR USA URY VNM ATG BHS BHR BLZ \n  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   3   3   3   3 \nBTN BRN COM FJI GRD KIR KWT MUS OMN QAT LCA VCT SAU SYC SLB TON TTO ARE \n  3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3"
  },
  {
    "objectID": "W13.html#comparison-with-5-k-means-clusters",
    "href": "W13.html#comparison-with-5-k-means-clusters",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Comparison with 5 k-means clusters",
    "text": "Comparison with 5 k-means clusters\n\nassignments |> filter(k==5) |> \n mutate(continent = owid_inds$continent, location = owid_inds$location,\n        hcluster = cutree(hc_owid, k = 3)) |> \n select(continent, location, .cluster, hcluster) |> \n ggplot(aes(x = .cluster, fill = factor(hcluster))) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJU-F22-MDSSB-DSCO-02: Data Science Concepts"
  },
  {
    "objectID": "W13.html#distributions-based-on-empirical-data",
    "href": "W13.html#distributions-based-on-empirical-data",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Distributions based on empirical data",
    "text": "Distributions based on empirical data\nEmpirical data is always finite, so why bother with theoretical continuous distributions?\n\n\nEach new data point would usually create a new discrete value.\nA discrete view is conceptually (theoretically) unfavorable.\nWe assume that there is a continuous distribution underlying."
  },
  {
    "objectID": "W13.html#probability-mass-function-does-not-work",
    "href": "W13.html#probability-mass-function-does-not-work",
    "title": "W#13: Probability, Cluster Analysis",
    "section": "Probability mass function does not work",
    "text": "Probability mass function does not work"
  },
  {
    "objectID": "W13.html#plot-height-of-hierarchy-steps",
    "href": "W13.html#plot-height-of-hierarchy-steps",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Plot height of hierarchy steps",
    "text": "Plot height of hierarchy steps\n\nNow we start looking at the height from the top.\nHeight at \\(x\\) is the Euclidean distance which needs to be bridged to join the closest two clusters from the \\(x+1\\)-cluster solution to get the \\(x\\)-cluster solution. We should not cut where the increase in height is marginal.\n\n\ntibble(height = hc_owid$height |> tail(10), # last ten values of height\n       num_cluster = 10:1) |> \n ggplot(aes(num_cluster,height)) + geom_point() + geom_line()"
  },
  {
    "objectID": "W13.html#dendrogram-with-cutpoints",
    "href": "W13.html#dendrogram-with-cutpoints",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Dendrogram with cutpoints",
    "text": "Dendrogram with cutpoints\n\nplot(hc_owid, cex = 0.4) # base-R way of plotting hclust object\nrect.hclust(hc_owid, k = 3) # base-R way of plotting in an existing plot\nrect.hclust(hc_owid, k = 8)"
  },
  {
    "objectID": "W13.html#visualization-clusters-and-continents-1",
    "href": "W13.html#visualization-clusters-and-continents-1",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Visualization: Clusters and continents",
    "text": "Visualization: Clusters and continents\nUse cutree.\n\nowid_inds |> mutate(hclust3 = cutree(hc_owid, k = 3),\n                    hclust8 = cutree(hc_owid, k = 8)) |> \n pivot_longer(c(hclust3,hclust8)) |> \n ggplot(aes(value, fill = continent)) + geom_bar() + facet_wrap(~name, scales = \"free_x\")"
  },
  {
    "objectID": "W13.html#probability-mass-function-pmf",
    "href": "W13.html#probability-mass-function-pmf",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Probability mass function (pmf)",
    "text": "Probability mass function (pmf)\nFor discrete random variables the probability mass function gives us the probabilities for each number. Mathematically it is\n\\(f_X(x) = \\text{Pr}(X = x)\\) while \\(F_X(x) = \\text{Pr}(X \\leq x)\\)\nAssume the discrete values with positive probability are \\(x_1 < x_2 < \\dots < x_n\\).\nThen it is easy to see the the probability mass function is the diff-function of the distribution function.\n\\(f_X(x_i) = F_X(x_i) - F_X(x_{i-1})\\)"
  },
  {
    "objectID": "W13.html#pmf-for-empirical-examples",
    "href": "W13.html#pmf-for-empirical-examples",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "pmf for empirical examples",
    "text": "pmf for empirical examples\n\n\nWorks well for the discrete random variable.\n\ness |> count(euftf) |> drop_na() |> mutate(freq = n/sum(n)) |>\n add_row(euftf = -4, freq = 0, .before = TRUE) |> add_row(euftf = 14, freq = 0) |> \n ggplot(aes(euftf,freq)) + geom_col() + \n scale_x_continuous(breaks = 0:10) + theme_minimal(base_size = 24)\n\n\n\n\n\nDoes not work so well for the continuous random variable\n\ngalton |> mutate(freq = 1/n()) |>  \n add_row(Estimate = 800, freq = 0, .before = TRUE) |> add_row(Estimate = 1600, freq = 0) |>\n ggplot(aes(Estimate, freq)) + geom_col() +\n theme_minimal(base_size = 24)"
  },
  {
    "objectID": "W13.html#uniform-pmf-theoretical-vs.-samples",
    "href": "W13.html#uniform-pmf-theoretical-vs.-samples",
    "title": "W#13: Probability, Cluster Analysis",
    "section": "Uniform pmf theoretical vs. samples",
    "text": "Uniform pmf theoretical vs. samples\n\nThe distribution function of a sample of 50 random variables.\n\nunif <- runif(50) \nunif_cdf <- tibble(x = unif) %>% \n  arrange(x) %>% # We sort the data by size\n  mutate(cdf = (1:length(unif))/length(unif)) # cumulative probabilities\nunif_cdf |> ggplot(aes(x, y = cdf)) + geom_step() +\n geom_function(fun = punif, color = \"red\")\n\n\n\n\n\nThe pmf approached with a histogram with small binwidth.\n\n\n\n\n\nNote, in a pmf the height of each bin with one observation would be \\(\\frac{1}{50}\\)!\n\n\n::::"
  },
  {
    "objectID": "W13.html#uniform-pmf-theoretical-vs.-samples-1",
    "href": "W13.html#uniform-pmf-theoretical-vs.-samples-1",
    "title": "W#13: Probability, Cluster Analysis",
    "section": "Uniform pmf theoretical vs. samples",
    "text": "Uniform pmf theoretical vs. samples\n\nThe distribution function of a sample of 50 random variables.\n\nunif <- runif(50) \nunif_cdf <- tibble(x = unif) %>% \n  arrange(x) %>% # We sort the data by size\n  mutate(cdf = (1:length(unif))/length(unif)) # cumulative probabilities\nunif_cdf |> ggplot(aes(x, y = cdf)) + geom_step() +\n geom_function(fun = punif, color = \"red\")\n\n\n\n\n\nThe pmf approached with a histogram with small binwidth.\n\n\n\n\n\nNote, in the real pmf the height of each point with positive probability would be only \\(\\frac{1}{50}\\)\n\n\n::::"
  },
  {
    "objectID": "W13.html#solution",
    "href": "W13.html#solution",
    "title": "W#13: Probability, Cluster Analysis",
    "section": "Solution:",
    "text": "Solution:"
  },
  {
    "objectID": "W13.html#uniform-distribution-theoretical-vs.-samples",
    "href": "W13.html#uniform-distribution-theoretical-vs.-samples",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Uniform distribution theoretical vs. samples",
    "text": "Uniform distribution theoretical vs. samples\n\nThe distribution function of a sample of 50 random variables.\nEmpirical and theoretical distribution function\n\nunif <- runif(50) \nunif_cdf <- tibble(x = unif) %>% \n  arrange(x) %>% # We sort the data by size\n  mutate(cdf = (1:length(unif))/length(unif)) # cumulative probabilities\nunif_cdf |> ggplot(aes(x, y = cdf)) + geom_step() +\n geom_function(fun = punif, color = \"red\")\n\n\n\n\n\nEmpirical pmf approached with a histogram with small binwidth.\n\n\n\n\n\n(Note, in a pmf the height of each bin with one observation would be \\(\\frac{1}{50}\\)!)\n\n\n::::"
  },
  {
    "objectID": "W13.html#normal-distribution-theoretical-vs.-samples",
    "href": "W13.html#normal-distribution-theoretical-vs.-samples",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Normal distribution theoretical vs. samples",
    "text": "Normal distribution theoretical vs. samples\n\nThe distribution function of a sample of 50 random variables.\nEmpirical and theoretical distribution function\n\nnormal <- rnorm(50) \nnormal_cdf <- tibble(x = normal) %>% \n  arrange(x) %>% # We sort the data by size\n  mutate(cdf = (1:length(normal))/length(normal)) # cumulative probabilities\nnormal_cdf |> ggplot(aes(x, y = cdf)) + geom_step() +\n geom_function(fun = pnorm, color = \"red\") + xlim(c(-3,3))\n\n\n\n\n\nEmpirical pmf approached with a histogram with small binwidth.\n\n\n\n\n\nThis type of pmf does not show the characteristics of the distribution.\n\n\n::::"
  },
  {
    "objectID": "W13.html#approach-the-solution",
    "href": "W13.html#approach-the-solution",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Approach the solution",
    "text": "Approach the solution\nThe theoretical distribution is approached better with\n\nlarger samples and\nlarger (but not too large) binwidth\n\n\n\n\n\nnormal <- rnorm(5000) \nnormal_cdf <- tibble(x = normal) %>% \n  arrange(x) %>% # We sort the data by size\n  mutate(cdf = (1:length(normal))/length(normal)) # cumulative probabilities\nnormal_cdf |> ggplot(aes(x, y = cdf)) + geom_step() +\n geom_function(fun = pnorm, color = \"red\") + xlim(c(-4,4))\n\n\n\n\n\n\nnormal_cdf |> \n ggplot(aes(x)) + geom_histogram(binwidth = 0.01) \n\n\n\n\n\n\n::::"
  },
  {
    "objectID": "W13.html#solution-probability-density-function",
    "href": "W13.html#solution-probability-density-function",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Solution: Probability density function",
    "text": "Solution: Probability density function\n\nWhen we have a functional form, the derivative of the distribution function is the probability density function (pdf) \\(f_X(x) \\frac{d}{dx}F_X(x)\\).\nConsequently, \\(F_X(x) = \\int_{-\\infty}^x f_X(\\xi)d\\xi\\).\n\\(\\int_a^bf(x)dx\\) is the probability that a value from the random variable \\(X\\) lies between \\(a\\) and \\(b\\): \\(\\text{Pr}(X \\geq a \\ \\&\\ X \\leq b)\\) or \\(\\text{Pr}(X \\in [a,b])\\)\n\nThe pdf is the analog of the pmf for continuous random variables."
  },
  {
    "objectID": "W13.html#normal-distribution",
    "href": "W13.html#normal-distribution",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Normal distribution",
    "text": "Normal distribution\n\ntibble(x = rnorm(1000)) |> \n ggplot(aes(x)) + \n geom_histogram(aes(y =..density..), binwidth = 0.1) + \n geom_density() + # This is a data-driven ksdensity function (no details here)\n geom_function(fun = dnorm, color = \"red\") +\n xlim(c(-5,5)) \n\n\n\n# Lognormal distribution\ntibble(x = rlnorm(1000)) |> \n  ggplot(aes(x)) + geom_histogram(aes(y =..density..), binwidth = 0.1) + geom_density() +\n  geom_function(fun = dlnorm, color = \"red\") +\n  xlim(c(-1,10))"
  },
  {
    "objectID": "W13.html#lognormal-distribution",
    "href": "W13.html#lognormal-distribution",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Lognormal distribution",
    "text": "Lognormal distribution\n\ntibble(x = rlnorm(1000)) |> \n  ggplot(aes(x)) + geom_histogram(aes(y =..density..), binwidth = 0.1) + geom_density() +\n  geom_function(fun = dlnorm, color = \"red\") +\n  xlim(c(-1,10))"
  },
  {
    "objectID": "W13.html#distribution-parameters",
    "href": "W13.html#distribution-parameters",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Distribution parameters",
    "text": "Distribution parameters\nAs empirical samples of numbers also theoretical distributions have an expected value or mean and a variance (and a standard deviation). In theoretical distributions they often become (related to) parameters of the distribution.\nThe normal distribution has the parameters mean and sd\n\nggplot() + \n  geom_function(fun = function(x) dnorm(x, mean = 2, sd = 1)) +\n  geom_function(fun = function(x) dnorm(x, mean = -3, sd = 3), color = \"red\") +\n  geom_function(fun = function(x) dnorm(x, mean = 7, sd = 0.5), color = \"blue\") +\n  geom_function(fun = function(x) dnorm(x, mean = -1, sd = 6), color = \"green\") +\n  xlim(-10,10)"
  },
  {
    "objectID": "W13.html#measures-of-samples",
    "href": "W13.html#measures-of-samples",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Measures of samples",
    "text": "Measures of samples\nHere are some examples of mean and standard deviation:\n\nx <- rnorm(1000, mean = 2, sd =5)\nmean(x)\n\n[1] 2.135658\n\nsd(x)\n\n[1] 4.949219\n\nx <- rnorm(10000, mean = 2, sd =5)\nmean(x)\n\n[1] 2.042562\n\nsd(x)\n\n[1] 5.040028\n\nx <- runif(10000)\nmean(x) # This should be 0.5\n\n[1] 0.5001556\n\nsd(x) # This should be 1/sqrt(12) = 0.2886751\n\n[1] 0.2897921"
  },
  {
    "objectID": "W13.html#the-zoo-of-distributions",
    "href": "W13.html#the-zoo-of-distributions",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "The zoo of distributions",
    "text": "The zoo of distributions\nThere are many probability distributions (implemented in R or not):\nhttps://en.wikipedia.org/wiki/List_of_probability_distributions\n\nMore important than knowing many is to learn to extract the idea of the underlying probabilistic model.\n\nExample: Binomial distribution as the number of successes in repeated Bernoulli trials.\n\n\nWhat is the underlying model of a normal distribution?"
  },
  {
    "objectID": "W13.html#galtons-data",
    "href": "W13.html#galtons-data",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Galtons data",
    "text": "Galtons data\n\ngal_mean <- mean(galton$Estimate)\ngal_sd <- sd(galton$Estimate)\ngalton |> ggplot(aes(Estimate)) + \n geom_histogram(aes(y =..density..), binwidth = 5) + \n geom_density(color = \"blue\") +\n geom_function(fun = dnorm, args = list(mean = gal_mean, sd = gal_sd), color = \"red\")\n\n\nA normal distribution fits “OK”."
  },
  {
    "objectID": "W13.html#test-with-sum-of-uniform-samples",
    "href": "W13.html#test-with-sum-of-uniform-samples",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Test with sum of uniform samples",
    "text": "Test with sum of uniform samples\n\nn <- 10000\ntibble(X1 = runif(n),X2 = runif(n),X3 = runif(n),X4 = runif(n),X5 = runif(n),\n       X6 = runif(n),X7 = runif(n),X8 = runif(n),X9 = runif(n)) %>% \n  mutate(S2 = X1 + X2,\n         S5 = X1 + X2 + X3 + X4 + X5,\n         S9 = X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 ) %>% \n  ggplot() + \n  geom_histogram(aes(x = S2, y =..density..), binwidth = 0.1, alpha = 0.5) + \n  geom_histogram(aes(x = S5, y =..density..), binwidth = 0.1, fill = \"red\", alpha = 0.5) + \n  geom_histogram(aes(x = S9, y =..density..), binwidth = 0.1, fill = \"blue\", alpha = 0.5) +   xlim(c(-0.5,9))"
  },
  {
    "objectID": "W13.html#why-are-sums-of-random-variables-important",
    "href": "W13.html#why-are-sums-of-random-variables-important",
    "title": "W#13: Probability, Cluster Analysis",
    "section": "Why are sums of random variables important?",
    "text": "Why are sums of random variables important?\n\nSums of random variables are the standard approach linear models \\(Y = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_nX_n\\)\nThey appear also in generalized linear models as for the logistic regression.\n\nConclusions:\n\nWhen the response variable \\(Y\\) is really a sum of several features (which all have relevant/non-zero coefficients) then \\(Y\\) should look normally distributed.\nIf \\(Y\\) strongly deviates from normality this is a sign that this model is not a really well representation of reality. This does not exclude using a linear model for prediction, but probably these can be improved."
  },
  {
    "objectID": "W13.html#sums-of-random-variables-important",
    "href": "W13.html#sums-of-random-variables-important",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Sums of random variables important?",
    "text": "Sums of random variables important?\nWhy are sums of random variables important?\n\nSums of random variables are the standard approach linear models \\(Y = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_nX_n\\)\nThey appear also in generalized linear models as for the logistic regression.\n\nConclusions:\n\nWhen the response variable \\(Y\\) is really a sum of several features (which all have relevant/non-zero coefficients) then \\(Y\\) should look normally distributed.\nIf \\(Y\\) strongly deviates from normality this is a sign that this model is not a really well representation of reality. This does not exclude using a linear model for prediction, but probably these can be improved."
  },
  {
    "objectID": "index.html#week-13-nov-24-probability-distributions-cluster-analysis",
    "href": "index.html#week-13-nov-24-probability-distributions-cluster-analysis",
    "title": "Data Science Concepts / Tools",
    "section": "Week 13, Nov 24: Probability distributions, Cluster Analysis",
    "text": "Week 13, Nov 24: Probability distributions, Cluster Analysis\nSlides Week 13"
  },
  {
    "objectID": "W13.html#distribution-functions-in-r",
    "href": "W13.html#distribution-functions-in-r",
    "title": "W#13: Probability distributions, Cluster Analysis",
    "section": "Distribution Functions in R",
    "text": "Distribution Functions in R\nIdentifiers for distributions:\nunif uniform distribution\nnorm normal distribution lnorm lognormal distribution binom binomial distribution (Note: This is a discrete distribution.)"
  },
  {
    "objectID": "hw-instructions/hw-06-instr.html",
    "href": "hw-instructions/hw-06-instr.html",
    "title": "Homework 06",
    "section": "",
    "text": "Important\n\n\n\nHomework 06 is due by the end of the semester. This homework is similarly structured as the final exam will be. However, it has only some example questions and is not a full mock exam, regarding the topics and the number of questions or points. This will change.\n\n\nAll information about the homework is provided in your personalized repository in the course’s GitHub-organization https://github.com/JU-F22-MDSSB-MET-01/.\nYour repository has the name hw-06-ind-USERNAME, where USERNAME is the GitHub-username you used for the other homework.\nThis would also be the workflow for the exam:\n\nGo to your repository’s page on GitHub.com in the browser. Read the main instructions in the README.md. Leave the page open to check back and to see if the commits you push appear as they should.\nClone the repository to your local computer. (If anything goes wrong, the instructions for Homework 01, Section 2 “Workflow” should help.)\nOpen the file hw-06.qmd in RStudio and Render it with quarto.\nWork in the quarto-file hw-06.qmd. Write all the required answers into this file. During your work:\n\n\nRender your file with quarto. This will create a file hw-06.html and shows a preview in RStudio. You can also open the file in your browser to see a larger version. You should do this repeatedly to check if your file renders well!\nAdd, commit, and push. Add the files hw-06.qmd and hw-06.html to stage them for a commit. (It is also okay to add all files in the Git pane of RStudio.) Make a commit in which you write a short commit message. Push your commit to GitHub. You should do this occasionally and check on GitHub.com, if the files are there how they should.\n\n\nBefore the deadline: Submit your solutions with a final rendering with quarto and add/commit/push with git. Submissions by email will be excepted only in case of emergency.\n\nAssessment:\n\nThe normal way of assessment will be that your repository will be cloned and the files in it will be assessed for solutions.\nIdeally, only the file hw-06.html needs to be looked at. However, also relevant content in the file hw-06.qmd may be assessed for points when this accidentally has not made it to the rendered html."
  },
  {
    "objectID": "hw-instructions/hw-06-instr.html#question-8-how-well-can-life-satisfaction-predict-voting-behavior",
    "href": "hw-instructions/hw-06-instr.html#question-8-how-well-can-life-satisfaction-predict-voting-behavior",
    "title": "Homework 06",
    "section": "1 Question 8: How well can life satisfaction predict voting behavior?",
    "text": "1 Question 8: How well can life satisfaction predict voting behavior?\nLike in our last tools session you should repeat the build of a logistic regression model using both statsmodels and sklearn packages. We want to predict voters with their satisfaction with life. We start simple and try to modify and enhance the model afterwards.\n\nClean the data set, so you remove all values above 10.\nRearrange the values from the vote columns. The default data from ESS is 1=voter and 2=non-voter. Modify to: 0= non-voter , 1=voter.\nCreate a model with vote ~ stflife\nUse sklearn, statsmodel.api (mind: Intercept), or the statsmodel.formula.api\nPlot the logistic regression function of the model, either build the function from scratch with numpy or use expit in scipy.special.\n\nDescribe in text what we can see in the function and what does it say about the predictions?\nPlot the crosstab (pd.crosstab(..)) as an barplot as validation."
  },
  {
    "objectID": "hw-instructions/hw-06-instr.html#question-9-how-can-we-improve-the-prediction-of-voters",
    "href": "hw-instructions/hw-06-instr.html#question-9-how-can-we-improve-the-prediction-of-voters",
    "title": "Homework 06",
    "section": "2 Question 9: How can we improve the prediction of voters?",
    "text": "2 Question 9: How can we improve the prediction of voters?\nNow we want to enhance the model with many variables 'polintr','euftf','stflife', 'trstplc', 'imueclt', 'atchctr'. Also we want to use a training data set and a test data set.\nWrite a short sentence why we use a split of training and test data.\n\nLook up the range for polintr to clean the data set.\nCreate a train and a test data set with train_test_split from sklearn.model_selection.\nTrain the model (Train data set) and then test it (Test data set). Use the predict function on the model.\nPrint out the coefficients, accuracy, and the confusion matrix (metrics from sklearn)\n\nDescribe what the model is predicting?\n\n2.1 Question 9.1: How do country variables change the prediction?\nNow we integrate countries in the model. We go back to the model in Question 1 vote ~ stflife but add the country values.\n\nSplit the data in a training and a test set that includes the Country column.\nCreate a simple model with statsmodel.formula.api.\nFit the model and print the confusion matrix\nSave the coefficients for later.\n\n\n\n2.2 Question 9.2: As Question 9.1 but with sklearn\nFor comparison:\n\nCreate dummy variables pd.get_dummies() for the country column (Attention: after cleaning the data)\nUse pd.concat(..,axis=1) to attach the stflife and vote variables back to the country dummies data set.\nSplit the training and test data set.\nCreate a LogisticRegression model from sklearn.linear_model and fit the data to it.\nPrint the confusion matrix\nDescribe how do both models (9.1 and 9.2) perform. What are the differences? If there are differences, where do they come from?\nFinally create 2 horizontal bar plots (matplotlib.pyplot.barh) with the coefficients from Question 9.1 and 9.2 and compare them.\n\n\n\n2.3 Submit the new version of your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "hw-instructions/hw-06-instr.html#new-york-city-flights-2013",
    "href": "hw-instructions/hw-06-instr.html#new-york-city-flights-2013",
    "title": "Homework 06",
    "section": "3 New York City Flights 2013",
    "text": "3 New York City Flights 2013\nUse your coding and modelling skills to create a model to predict flight delays with nycflights2013 in R. You find hints in the codebase of the “Data Science Tools with R” course.\nTo that end, continue to work in the repository hw-02-ind-USERNAME on the file “hw-02-R.qmd”.\nFirst make some modifications to the file:\n\nIn the YAML (header of the file) change the title to “Homework 02/05 - New York City Flights 2013”\nAt the end of the file make a new main headline “# Predicting flight delays”. Test if the file renders to html nicely. Write all you analysis below, and structure your report with second order headlines “## YOUR HEADLINE”.\n\n\n3.1 Data Preparation\nCreate a big data set from the data frames in the New York City flights package. Join the data sets so that you get as little missing values as possible. (Use inner_join().)\nRename the variables so that you can distinguish them in the final data set. (Ignore the airport data set though, we don’t need it.)\nCreate a dummy variable for the arrival delay being bigger than 15 minutes. This will be the response in the analysis below.\nCreate factors for weekdays and months.\nConvert all character variables into factors, and drop factors with more than 100 observations for faster computation.\nEliminate variables with more than 10,000 missing values, and eliminate observations with missing values. (Use na.omit().)\n\n\n3.2 Split the dataset\nSplit the dataset into the training set (60% of observations) and a test set (40% of observations).\n\n\n3.3 Exploratory analysis\nIn this stage, make an informed selection of eight predictor variables. Select 8 predictor variables that you think have an impact on flight delays, and formulate expectations about how they may influence flight delays.\nThen, take a random sample of 10,000 observations from the training dataset to conduct exploratory analysis.\nMake plots that show the connection between arr_delay, or the dummy variable you created, and the potential predictors.\nThink about and document whether you want to change one predictor after looking at the plots.\n\n\n3.4 Model development\nTrain your model on the training dataset imitating a simple algorithm called forward selection.\nFit one model with a single predictor for all eight variables selected respectively. Use the code from the slides, and save each of the eight models as an object.\nCompare the models in one integrated table, and use AIC or BIC as the main performance criterion. The lower the value of the criterion, the better the performance of the model.\nSelect the best model with a single predictor, and test the remaining seven variables as the second predictor. Again, choose the model that performs best, and try the remaining six variables as predictor number three. Continue until there are no further decreases in AIC or BIC.\n(AIC and BIC are somewhat similar to adjusted R-squared in linear regression: They point at the performance of the model in describing the dataset, while punishing model complexity to avoid overfitting. BIC has a stricter penalty for complexity than AIC, and this will lead to models with fewer predictors.)\nOnce you found the best-performing model, interpret the coefficients against the background of your previous expectations.\n\n\n3.5 Prediction\nUse the best-performing model to predict delays of 15 minutes or more in the test data set.\nCreate the confusion matrix.\nCalculate accuracy, sensitivity, and specificity; interpret the values. Create the ROC plot and interpret it.\n\n\n3.6 Submit your report\nMake sure that your rendered html file reads nicely as a report. Polish the formatting if necessary. Commit your qmd-file and the rendered html-file. Push to GitHub."
  },
  {
    "objectID": "index.html#week-9-oct-27-classification-problems-logistic-regression-prediction",
    "href": "index.html#week-9-oct-27-classification-problems-logistic-regression-prediction",
    "title": "Data Science Concepts / Tools",
    "section": "Week 9, Oct 27: Classification Problems, Logistic Regression, Prediction",
    "text": "Week 9, Oct 27: Classification Problems, Logistic Regression, Prediction\nSlides Week 9\nDownload instructions: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf\nHomework 04 due in 10 days."
  }
]