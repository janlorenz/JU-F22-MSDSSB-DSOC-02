---
title: "W#8 Homework topics: Typical Data Issues, More linear models and Interpretation"
author: Jan Lorenz
format: 
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: false
    logo: img/JACOBS_LOGO_RGB_Internet.jpg
    footer: "[JU-F22-MDSSB-DSCO-02: Data Science Concepts](https://janlorenz.github.io/JU-F22-MSDSSB-DSOC-02/)"
---


# Errors in data

```{r}
#| label: data
library(tidyverse)
# If there is no data, or data needs to be updated, first run the script Download_corona_data.R
owid <- read_csv("data/owid-covid-data.csv", 
                 col_types = cols( 
                   iso_code = col_character(),
                   continent = col_character(),
                   location = col_character(),
                   date = col_date(format = ""),
                   total_cases = col_double(),
                   new_cases = col_double(),
                   new_cases_smoothed = col_double(),
                   total_deaths = col_double(),
                   new_deaths = col_double(),
                   new_deaths_smoothed = col_double(),
                   total_cases_per_million = col_double(),
                   new_cases_per_million = col_double(),
                   new_cases_smoothed_per_million = col_double(),
                   total_deaths_per_million = col_double(),
                   new_deaths_per_million = col_double(),
                   new_deaths_smoothed_per_million = col_double(),
                   reproduction_rate = col_double(),
                   icu_patients = col_double(),
                   icu_patients_per_million = col_double(),
                   hosp_patients = col_double(),
                   hosp_patients_per_million = col_double(),
                   weekly_icu_admissions = col_double(),
                   weekly_icu_admissions_per_million = col_double(),
                   weekly_hosp_admissions = col_double(),
                   weekly_hosp_admissions_per_million = col_double(),
                   total_tests = col_double(),
                   new_tests = col_double(),
                   total_tests_per_thousand = col_double(),
                   new_tests_per_thousand = col_double(),
                   new_tests_smoothed = col_double(),
                   new_tests_smoothed_per_thousand = col_double(),
                   positive_rate = col_double(),
                   tests_per_case = col_double(),
                   tests_units = col_character(),
                   total_vaccinations = col_double(),
                   people_vaccinated = col_double(),
                   people_fully_vaccinated = col_double(),
                   total_boosters = col_double(),
                   new_vaccinations = col_double(),
                   new_vaccinations_smoothed = col_double(),
                   total_vaccinations_per_hundred = col_double(),
                   people_vaccinated_per_hundred = col_double(),
                   people_fully_vaccinated_per_hundred = col_double(),
                   total_boosters_per_hundred = col_double(),
                   new_vaccinations_smoothed_per_million = col_double(),
                   new_people_vaccinated_smoothed = col_double(),
                   new_people_vaccinated_smoothed_per_hundred = col_double(),
                   stringency_index = col_double(),
                   population = col_double(),
                   population_density = col_double(),
                   median_age = col_double(),
                   aged_65_older = col_double(),
                   aged_70_older = col_double(),
                   gdp_per_capita = col_double(),
                   extreme_poverty = col_double(),
                   cardiovasc_death_rate = col_double(),
                   diabetes_prevalence = col_double(),
                   female_smokers = col_double(),
                   male_smokers = col_double(),
                   handwashing_facilities = col_double(),
                   hospital_beds_per_thousand = col_double(),
                   life_expectancy = col_double(),
                   human_development_index = col_double(),
                   excess_mortality_cumulative_absolute = col_double(),
                   excess_mortality_cumulative = col_double(),
                   excess_mortality = col_double(),
                   excess_mortality_cumulative_per_million = col_double()
                 ))
who <- read_csv("data/WHO-COVID-19-global-data.csv", 
                col_types = cols(
                  Date_reported = col_date(format = ""),
                  Country_code = col_character(),
                  Country = col_character(),
                  WHO_region = col_character(),
                  New_cases = col_double(),
                  Cumulative_cases = col_double(),
                  New_deaths = col_double(),
                  Cumulative_deaths = col_double()
                ))
who_owid <- who |>  
  left_join(owid, by=c("Date_reported" = "date", "Country" = "location")) |> 
  rename(Date = Date_reported, New_cases_who = New_cases, New_cases_owid = new_cases, 
         Total_cases_who = Cumulative_cases, Total_cases_owid = total_cases) |> 
  select (Date, Country, New_cases_who, New_cases_owid, Total_cases_who,
          Total_cases_owid)
```


## Strange Airports (Homework 02) {.smaller}

```{r}
#| fig-height: 4
#| echo: true
library(nycflights13)
ggplot(data = airports, mapping = aes(x = lon, y = lat)) + geom_point(aes(color = tzone)) 
```

```{r}
#| echo: true
airports %>% filter(lon >= 0) 
```


## Airport errors {.smaller}

```{r}
airports %>% filter(lon >= 0)
```
Correct locations (internet research and location of maps):

:::: {.columns}
::: {.column width='55%'}
- Deer Valley Municipal Airport: Phoenix   
33°41′N 112°05′W [Missing minus for lon (W)]{style='color:red;'}
- Dillant Hopkins Airport: New Hampshire   
42°54′N 72°16′W [lon-lat switched, minus (W)]{style='color:red;'}
- Montgomery Field: San Diego   
32°44′N 117°11″W [Missing minus for lon (W)]{style='color:red;'}
- Eareckson As: Alaska    
52°42′N 174°06′E [No error: Too west,it's east!]{style='color:red;'}
:::

::: {.column width='45%'}
![](https://upload.wikimedia.org/wikipedia/commons/e/ef/FedStats_Lat_long.svg)
:::

::::

## Conclusions on data errors {.smaller}

- In real-world datasets errors like the 3 airport are quite common.
- Errors of this type are often hard to detect and remain unnoticed. 
  - This can (but need not) change results drastically!

. . .

Conclusions

- Always remain **alert for inconsistencies** and be ready to check the **plausibility** of results. 
- Skills in **exploratory data analysis** (EDA) are essential to find errors and explore their nature and implication
- Errors are unpredictable, of diverse types, and often deeply related to the reality the data presents. 
  - One reason why EDA can not be a fully formalized and automatized process. 

# Corona questions (Homework 03)

## Same phenomenon different data? {.smaller}

Question: **Is the data of OWiD and WHO the same?**

. . .


:::: {.columns}

::: {.column width='40%'}
No.  
Why?   
What are the data sources of WHO and OWiD?  
:::

::: {.column width='60%'}
```{r}
#| fig-height: 4
who_owid |> 
  pivot_longer(c("New_cases_who","New_cases_owid"), 
               names_to = "data_source", 
               values_to = "new_cases") |> 
  filter(Date > '2020-02-01', Date < '2020-07-01', Country == "Germany") |> 
  ggplot(aes(x = Date, y = new_cases, col = data_source)) +
  geom_line() +
  facet_wrap(~Country, ncol=1, scales = "free") +
 theme_minimal(base_size = 24)
```
:::

::::

- [OWiD documentation](https://docs.owid.io/projects/covid/en/latest/dataset.html) refers to have data from [CSSE at Johns Hopkins University](https://github.com/CSSEGISandData/COVID-19) which document various data sources (e.g., a newspaper from Germany)
- [WHO documentation](https://covid19.who.int/data) says: "WHO collected the numbers of confirmed COVID-19 cases and deaths through official communications under the International Health Regulations (IHR, 2005), complemented by monitoring the official ministries of health websites and social media accounts." For Germany this is the Robert-Koch-Institut [RKI](https://github.com/robert-koch-institut/SARS-CoV-2-Infektionen_in_Deutschland). 

## Good reasons for different data? {.smaller}

- During the pandemic daily new case numbers were relevant for decisions about safety measures.
- In reality, data comes with delays.

Example: Recent new cases in Germany (RKI). Notice many new cases several days ago. 
![](img/RKI-corona-dashboard.png)

## Conflict day-to-day consistency and correctness {.smaller}

- Fixing daily cases is useful to record the numbers on which daily safety decisions are based. 
- Corrected cases (which also change data from the past) are better for analysis in retrospect. It reflects the actual pandemic better. 

## Reported cases and real cases? {.smaller}

Case numbers are to inform us about real cases. What type of data analysis question is this? (Descriptive, Exploratory, Inferential, Predictive, Causal, Mechanistic)

. . . 

- Inferential: "Quantify whether the discovery is likely to hold in a new sample."
  - Here: What do reported cases tell us about cases in the whole population? 
- Limitations 
  - We cannot test all
  - Tests are not on a random sample
  - Mild/asymptomatic cases remain unnoticed even to individuals
  - ...
  
The unknown: **What is the dark figure?**


## Excercise for German new case counts {.smaller}

![](img/RKI-corona-dashboard.png){height="250"}

- Can we infer the real *incidence* (= new cases per 100,000)?
- What can we infer the *trend* of the real incidence?

. . .

**Incidence:** Not really, we would need a either a random sample (then we can infer the fraction of infected), or an idea how to estimate the dark figure. 

**Trend:** Yes! Under the assumptions that reported cases do reflect a relevant part of the pandemic and the limitation remain mostly constant during the observed trend. 


## Smoothing time series {.smaller}

```{r}
#| fig-height: 3
who |> filter(Date_reported > '2020-02-01', Date_reported < '2020-07-01', 
              Country == "Germany") |>
 mutate(Smooth03 = (New_cases + lag(New_cases) + lag(New_cases,2))/3,
        Smooth07 = (New_cases + lag(New_cases) + lag(New_cases,2) + lag(New_cases,3) + lag(New_cases,4) + lag(New_cases,5) + lag(New_cases,6))/7,
        Smooth10 = (New_cases + lag(New_cases) + lag(New_cases,2) + lag(New_cases,3) + lag(New_cases,4) + lag(New_cases,5) + lag(New_cases,6)+ lag(New_cases,7) + lag(New_cases,8) + lag(New_cases,9))/10,
        New_r = New_cases,
        Smooth03_r = zoo::rollmean(New_cases,3, fill = TRUE),
        Smooth07_r = zoo::rollmean(New_cases,7, fill = TRUE),
        Smooth10_r =  zoo::rollmean(New_cases,10, fill = TRUE)) |> 
 pivot_longer(c(New_cases, Smooth03, Smooth07, Smooth10, 
                New_r, Smooth03_r, Smooth07_r, Smooth10_r), names_to = "Smooth",
                  values_to = "Cases") |> 
 mutate(smooth = if_else(word(Smooth,2, 2,"_") == "r", "Centered","Lagged", 
                         missing = "Lagged"), 
        window = word(Smooth, 1,1,"_") |>  
         fct_recode("No" = "New", "3 days" = "Smooth03",
                    "7 days" = "Smooth07", "10 days" = "Smooth10")) |> 
 filter(Date_reported > '2020-03-01', Date_reported < '2020-05-01') |>
 ggplot(aes(Date_reported, Cases, color = window)) +
  geom_line() + facet_wrap(~smooth, nrow=2) + theme_minimal()
```


```{r}
#| echo: true
#| output-location: column
x <- c(1, 2, 5, 3, 0)
x
```
```{r}
#| echo: true
#| output-location: column
zoo::rollmean(x, k = 3, na.pad = TRUE) # for centered window
```
```{r}
#| echo: true
#| output-location: column
(x + lag(x, n = 1) + lag(x, n = 2))/3 # for lagged window
```

. . . 

**Centered:** Leaves smoothed data close to real data.   
**Lagged**: Lags the smoothed data, but can be consistently computed for the newest day

Remember: **Data with weekly seasonality is best smoothed with a weekly window!**


## Total death per million and human development

```{r}
owid |> 
 group_by(location) |> 
 filter(date == "2022-08-31", !is.na(human_development_index), 
        !is.na(total_deaths_per_million), !is.na(continent)) |> 
 ggplot(aes(x = human_development_index, y = total_deaths_per_million, 
            color = continent, size = population)) +
 geom_point() + scale_size_area(max_size = 10) +
 theme_minimal()
```

What findings? Explanations?

# ESS questions (Homework 03)

## Level of measurement {.smaller}

- *Nominal*: the data can only be categorized
- *Ordinal*: the data can be categorized and ranked
- *Interval*: the data can be categorized, ranked, and evenly spaced
- *Ratio*: the data can be categorized, ranked, evenly spaced, and has a natural zero.

**What is the difference of *level of measurement* and *data type*?**

. . . 

Mainly perspective:

- **Level of measurement** is about the variable/the thing which is measured.
- **Data type** is more about the technical way to store data. 


## Scales in surveys/questionaires {.smaller}

**Likert scale**: Strongly disagree ... [scale steps] ... Strongly agree  
**Rating scales**: Extreme statement ... [scale steps] ... Opposite statement

What level of measurement do these questions have?

![](img/ess-atchctr.png){height=350}
![](img/ess-euftf.png){height=350}

. . . 

*Ordinal* clearly, *interval* assuming scale steps are equal, *ratio* assuming 5 as natural zero

## Dealing with missing values {.smaller}

Coding in the ESS data: 

- Interval data coded numerically 0, 1, ..., 10
- Missing values with numerical codes 77, 88, 99

What is the reason for missing data? This can be important for inferential questions!

![](img/ess_missing.png){height="100"}

```{r}
ess_raw <- read_csv("data/ESS-Data-Wizard-subset-2022-09-17.csv",
                col_types = cols(
                 name = col_character(),
                 essround = col_double(),
                 edition = col_double(),
                 proddate = col_character(),
                 idno = col_double(),
                 cntry = col_character(),
                 dweight = col_double(),
                 pspwght = col_double(),
                 pweight = col_double(),
                 euftf = col_double(),
                 gincdif = col_double(),
                 lrscale = col_double(),
                 polintr = col_double(),
                 stflife = col_double(),
                 trstplc = col_double(),
                 vote = col_double(),
                 imueclt = col_double(),
                 atchctr = col_double(),
                 atcherp = col_double(),
                 crmvct = col_double(),
                 pray = col_double(),
                 rlgdgr = col_double(),
                 gndr = col_double(),
                 age = col_double()
                ))
ess <- ess_raw
```

**For numerical computations these must be filtered out or coded as NA!**

```{r}
#| echo: true
ess |> select(euftf) |> 
 mutate(euftf_na = euftf  |> na_if(77) |> na_if(88) |> na_if(99)) |> 
 summarize(across(.fns = function(x) mean(x, na.rm = TRUE)))
```

```{r}
ess <- ess |> mutate(atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),
                     atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),
                     euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99))
```

<!-- ## What is the ranking of European countries with respect to of average satisfaction with life? -->

## Emotional attachment {.smaller}

Question: What is the relation of the emotional attachment of Europeans to their **own country** and to **Europe**?

```{r}
#| echo: true
#| output-location: column
#| fig-height: 7
ess |> 
 filter(essround == 9) |> 
 count(atchctr, atcherp) |> 
 na.omit() |> 
 ggplot(aes(atchctr, atcherp, 
            size = n, color = n)) + 
 geom_point() +
 geom_smooth(aes(weight = n), 
             method = 'loess', 
             formula = 'y ~ x') +
 scale_color_continuous(type = "viridis") +
 scale_size_area(max_size = 10) + ylim(c(0,10)) +
 coord_fixed() + 
 guides(size = "none") +
 theme_classic(base_size = 24)
```


::: aside
Differences between `lowess` used in `seaborn.regplot` and `loess`, the default in `ggplot::geom_smooth`: <https://stats.stackexchange.com/questions/161069/difference-between-loess-and-lowess>
:::



## Emotional attachment EU integration {.smaller}

**Question:** What is the relation of the emotional **attachment to the own country** to **attachment to Europe** compared to the attitude about **European integration**?

```{r}
#| fig-height: 7
library(patchwork)
g1 <- ess |> 
 filter(essround == 9) |> 
 count(atchctr, atcherp) |> 
 na.omit() |> 
 ggplot(aes(atchctr, atcherp, 
            size = n, color = n)) + 
 geom_point() +
 geom_smooth(aes(weight = n), 
             method = 'loess', 
             formula = 'y ~ x') +
 scale_color_continuous(type = "viridis") +
 scale_size_area(max_size = 10) + ylim(c(0,10)) +
 coord_fixed() + 
 labs(title = "Attachment Europe") +
 guides(size = "none",color = "none") +
 theme_classic(base_size = 24)
g2 <- ess |> 
 filter(essround == 9) |> 
 count(atchctr, euftf) |> 
 na.omit() |> 
 ggplot(aes(atchctr, euftf, 
            size = n, color = n)) + 
 geom_point() +
 geom_smooth(aes(weight = n), 
             method = 'loess', 
             formula = 'y ~ x') +
 scale_color_continuous(type = "viridis") +
 scale_size_area(max_size = 10) + ylim(c(0,10)) +
 coord_fixed() + 
 labs(title = "European Integration") +
 guides(size = "none",color = "none") +
 theme_classic(base_size = 24)
g1 + g2
```

- Emotional attachment to the own country and Europe is positively related. ("Positive" here means the sign of the correlation. It does not mean "good"!)
- No compensation like "Emotion must be split between both."
- Relation of country attachment to EU integration is weak but non-linear. 

## Weighting after `count` {.smaller}

How many rows does the the data frame `ess |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit()` have? 

. . . 

11 times 11 = 121 (when each combination has a non-zero number of cases)

How many observations (not NA) do we have in the data set?

```{r}
#| echo: true
ess |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> summarize(n = sum(n))
```

Weighting points correctly is important!

## Three different smooth plots {.smaller}


::: {.panel-tabset}

###  A: Counts unweighted

```{r}
#| echo: true
#| fig-height: 3
#| fig-width: 3
ess |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> 
 ggplot(aes(atchctr, euftf)) + geom_smooth() 
```

Makes no sense because, just 121 points in a square. 

### B: Counts weighted

```{r}
#| echo: true
#| fig-height: 3
#| fig-width: 3
ess |> filter(essround == 9) |> count(atchctr, euftf) |> na.omit() |> 
 ggplot(aes(atchctr, euftf, weight = n)) + geom_smooth() 
```

Makes sense weighted by the counts.


### C: Individual cases

```{r}
#| echo: true
#| fig-height: 3
#| fig-width: 3
ess |> filter(essround == 9) |> 
 ggplot(aes(atchctr, euftf)) + geom_smooth()
```

Similar to B. Lower uncertainty! Different y-axis limits!   
Note, `geom_smooth` uses `stats::loess` for less than 1,000 cases (as before), otherwise (as here) `mgcv::gam()` because it is more efficient computationally. We omit details here. 

:::


## Significance of nonlinear relation? {.smaller}

```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 3
ess |> filter(essround == 9) |> 
 ggplot(aes(atchctr, euftf)) + geom_smooth()
```

- Taking into account the real number of cases the uncertainty range indicates that the non-linear relationship is fairly certain, although small in magnitude.
- Note, we have not looked at uncertainty measures in detail yet. 
- Main message here: **Low uncertainty and large effect size is not the same!**
  - In statistics the first is called *significant*. 
  - In common language the second is often called *significant*. 
- This dual use of *significant* is a source of confusion in science communication!


# Linear models and R-squared

## Linear models {.smaller}

```{r}
#| echo: true
library(tidymodels)
linear_reg() |> set_engine("lm") |> 
 fit(atcherp ~ atchctr, data = ess) |> tidy()
```

Interpretation?

. . . 

When `atchctr = 0` the average `atcherp` is 2.62. For an increase of country attachment by one there is an average increase of 0.414 in European attachment. 

. . .

For EU integration attitude. 

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(euftf ~ atchctr, data = ess) |> tidy()
```


## R-squared of a fitted model {.smaller}

$R^2$ is the percentage of variability in the response explained by the regression model. 

R-squared is also called **coefficient of determination**. 

**Definition**: 

$R^2 = 1 - \frac{SS_\text{res}}{SS_\text{tot}}$

where $SS_\text{res} = \sum_i(y_i - f_i)^2 = \sum_i e_i$ is the *sum of the squared residuals*, and   
$SS_\text{tot} = \sum_i(y_i - \bar y)^2$ the *total sum of squares* which is proportional to the variance of $y$. ($\bar y$ is the mean of $y$.)

$R^2$ is the square of the correlation coefficient, hence the name. (No math on this today.)

![](https://upload.wikimedia.org/wikipedia/commons/8/86/Coefficient_of_Determination.svg)



## Linear models R-squared {.smaller}

```{r}
#| echo: true
library(tidymodels)
linear_reg() |> set_engine("lm") |> 
 fit(atcherp ~ atchctr, data = ess) |>
 glance()  # glance shows summary statistics of model fit
```

Interpretation R-square?

. . . 

12.2% of the variance of European emotional attachment can be explained by a linear relation with country emotional attachment. 

. . .

For EU integration attitude. 

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(euftf ~ atchctr, data = ess) |> 
 glance()
```

# Linear models with more predictors

## Linear model with more predictors {.smaller}

```{r}
#| echo: true
library(tidymodels)
linear_reg() |> set_engine("lm") |> 
    fit(euftf ~ atchctr + atcherp, data = ess) |> tidy()
```

- Note, that `atchctr` now has a negative coefficient!
- The tiny bit of positive relation explained by `atchctr` in a one predictor model can better be explained by `atcherp` (which we know is correlated with `atchctr`). 




## Corona deaths vs. Human development {.smaller}

```{r}
owid |> 
 group_by(location) |> 
 filter(date == "2022-08-31", !is.na(human_development_index), 
        !is.na(total_deaths_per_million), !is.na(continent)) |> 
 ggplot(aes(x = human_development_index, y = total_deaths_per_million)) +
 geom_point(aes(color = continent)) + scale_size_area() +
 geom_smooth(method = "lm") +
 theme_minimal()
```

Note: Not weighted by population!


## Linear model: Total deaths vs. HDI {.smaller}

```{r}
#| echo: true
owid_aug22 <- owid |> 
 filter(date == "2022-08-31", !is.na(human_development_index), 
        !is.na(total_deaths_per_million), !is.na(continent))
linear_reg() |> set_engine("lm") |> 
    fit(total_deaths_per_million ~ human_development_index, data = owid_aug22) |> tidy()
```

## Adding a main effect of continents {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
    fit(total_deaths_per_million ~ human_development_index + continent, 
        data = owid_aug22) |> tidy()
```

A **main effect** by categorical dummy variables allows for different intercepts per continent. 

## Adding as interaction {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
    fit(total_deaths_per_million ~ human_development_index * continent, 
        data = owid_aug22) |> tidy()
```

- Note the `*` for interaction effect!
- Also main effects for both variables are in as coefficients.
- Africa has been chosen as reference category (because it is first in the alphabet).
- An **interaction effect** allows for different slopes for each continent!


## Regression lines by continent {.smaller}

```{r}
owid |> 
 group_by(location) |> 
 filter(date == "2022-08-31", !is.na(human_development_index), 
        !is.na(total_deaths_per_million), !is.na(continent)) |> 
 ggplot(aes(x = human_development_index, y = total_deaths_per_million, 
            color = continent)) +
 geom_point() + scale_size_area() +
 geom_smooth(method = "lm") +
 theme_minimal()
```

The relation between deaths and human development is reverse in Europe. 


## Simpson's paradox

Slopes for all groups can be in the opposite direction of the main effect's slope!

![](https://upload.wikimedia.org/wikipedia/commons/f/fb/Simpsons_paradox_-_animation.gif)
