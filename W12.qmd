---
title: "W#12: Probability, Principal Component Analysis"
author: Jan Lorenz
format: 
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: false
    logo: img/JACOBS_LOGO_RGB_Internet.jpg
    footer: "[JU-F22-MDSSB-DSCO-02: Data Science Concepts](https://janlorenz.github.io/JU-F22-MSDSSB-DSOC-02/)"
---


# Math: Probability {background-color="yellow"}

```{r}
library(tidyverse)
library(tidymodels)
library(readxl)
galton <- read_excel("data/galton_data.xlsx") |> mutate(true_value = 1198)
```

## Probability Topics for Data Science {.smaller}

Today concepts and topics

  * The concept of probability form the mathematical perspective.
  * What are probabilistic events, probability functions and random variables.
  * How do random variables relate to data?
  * (Binomial distribution)
  
# Events as subsets of a sample space 

## Sample space, atomic events, events {.smaller}

In the following, we say $S$ is the **sample space** which is a set of **atomic events**. 

Example for sample spaces:

  * For a coin toss the atomic events are $H$ (for HEADS) and $T$ for TAILS, and the sample space is $S = \{H,T\}$.  
  * For the selection of one person of a group of $N$ individuals labeled $1,\dots,N$, the sample space is $S = \{1,\dots,N\}$. 
  * For two successive coin tosses the atomic events are $HH$, $HT$, $TH$, and $TT$. The sample space is $\{HH,HT, TH, TT\}$. **Important:** The atomic events for two coin tosses tare *not* $H$ and $T$. 

An **event** $A$ is a *subset* of the sample space $A \subset S$.   

[Important: Note the difference of *atomic events* and *events*. ]{style='color:red;'}


## Example events for one coin toss {.smaller}

- The set with one atomic event is a subset $\{H\} \subset \{H,T\}$. 
- Also the sample space $S = \{H,T\} \subset \{H,T\}$ is an event. It is called the *sure event*. 
- Also the empty set $\{\} = \emptyset \subset \{H,T\}$ is an event. It is called the *impossible event*. 
- In interpretation, the event $\{H,T\}$ means: The coin comes up HEAD *or* TAIL. 
- The empty set is interpreted as the event that it comes up *neither* HEADS *nor* TAILS. 

## More example events {.smaller}

- 2 coin tosses: The event $\{HH, TH\}$ means "The first toss comes up HEAD or TAIL and the second is HEADS."   
- The event $\{HT, TH, HH\}$ means that "We have HEAD once or twice and it does not matter what coins." 
- The event $\{TT, HH\}$ means "Both coins show the same side."

**Quiz questions for three coin tosses:**
  
  * What is the event "The coins show one HEAD"? [$\{HTT, THT, TTH\}$]{.fragment}
  * What is the event "The first and the third coin are not HEAD?  [$\{THT, TTT\}$]{.fragment}
  * How many *atomic events* exist? [$2^3=8$]{.fragment}

. . .

**For selecting one random person:**  
The event $\{2,5,6\}$ means that the selected person is *either* 2, 5, or 6. (Not all three people which is a different random variable!)


# The set of all events and the probability function

## The set of all events {.smaller}

The **collection of all events** is called a sigma-algebra. (This is a mathematical term which linguistic meaning we do not analyze deeper here.)

**Definition:** A **sigma-algebra** $\mathcal{F}(S)$ is a collection of subsets of a sample space $S$ when it has the following properties   

  (i) The empty set (the impossible event) is part of it $\emptyset \in \mathcal{F}(S)$
  (ii) When $A \in \mathcal{F}(S)$ then its complement $A^c \in \mathcal{F}(S)$. That means: For any event $A$ also its opposite $A^c = S \setminus A$ (read $S$ minus the elements of $A$) is an event. 
  (iii) $\mathcal{F}(S)$ is closed under the countable set union of its members. That means if $A_1,A_2,A_3, \dots \in \mathcal{F}(S)$ the $\bigcup_{i}^\infty A_i = A_1 \cup A_2 \cup A_3 \cup \dots \in \mathcal{F}(S)$. 
  
 [The mathematical technicality is not central here. Important is: The sigma-algebra is the set of all possible events and this is usually larger / more complex then one may naively think. ]{style='color:blue;'}


## The power set  {.smaller}

:::: {.columns}

::: {.column width='50%'}

- A sigma-algebra $\mathcal{F}(S)$ is a subset of the **set of all subsets** (also called **power set**) of the sample space sometimes denoted $\mathcal{P}(S)$ or $2^S$. 
- The notation $2^S$ matches the fact that the power set of a set with $n$ elements has $2^n$ elements. 
:::

::: {.column width='40%'}
![](https://upload.wikimedia.org/wikipedia/commons/e/ea/Hasse_diagram_of_powerset_of_3.svg)

Example powerset of a three element set.
:::

::::

## Example for the set of all events  {.smaller}

  * For 3 coin tosses: How many events exist? [$2^3=8$ atomic events $\to$ $2^8=256$ event]{.fragment} 
  * How is it for four coin tosses? [$2^{(2^4)} = 65536$]{.fragment}
  * We select two out of five people at random (without replacement). How many atomic events? How many events?  
  
. . . 

Atomic events: 12, 13, 14, 15, 23, 24, 25, 34, 35, 45 (here the order does not matter)

The number can be computed by "n choose k" ${n \choose k} =\frac{n!}{(n-k)!k!}$. Here: ${5\choose 2}$. 

```{r}
#| echo: true
choose(5,2)
```
  
Thus there are $2^{10} = 1024$ events. 
  
Example event: "Person 1 is among the selected." = $\{12, 13, 14, 15\}$

[These are typical problems of *combinatorics*, the **theory of counting**, which is basic for many probability models. We do not go deeper into it here.]{style='color:blue;'} 



## Probability function {.smaller}

**Definition:** For a collection of events (in a sigma-algebra $\mathcal{F}(S)$) a function $\text{Pr}: \mathcal{F}(S) \to \mathbb{R}$ is a **probability function** when 

  (i) The probability of any event is between 0 and 1: $0\leq \text{Pr}(A) \leq 1$. (So, actually a probability function is a function $\text{Pr}: \mathcal{F}(S) \to [0,1]$.)
  (ii) The probability of the event coinciding with the whole sample space (the sure event) is 1: $\text{Pr}(S) = 1$. 
  (iii) For events $A_1, A_2, \dots, A_n \in \mathcal{F}(S)$ which are *pairwise disjoint* we can sum up their probabilities:  
$$\text{Pr}(A_1 \cup A_2\cup\dots\cup A_n) = \text{Pr}(A_1) + \text{Pr}(A_2) + \dots + \text{Pr}(A_n) $$

[This captures the essence of how we think about probabilities mathematically. Most important: We can only easily add probabilities when they do not share atomic events.]{style='color:blue;'}


## Some basic probability rules {.smaller}

  * We can compute the probabilities of all events by summing the probabilities of the atomic events in it. So, the probabilities of the atomic events are building blocks for the whole probability function. 
  * $\text{Pr}(\emptyset) = 0$
  * For any events $A,B \subset S$ it holds
    * $\text{Pr}(A \cup B) = \text{Pr}(A) + \text{Pr}(B) - \text{Pr}(A \cap B)$
    * $\text{Pr}(A \cap B) = \text{Pr}(A) + \text{Pr}(B) - \text{Pr}(A \cup B)$
    * $\text{Pr}(A^c) = 1 - \text{Pr}(A)$

**Recap from the motivation of logistic regression:** When the probability of an event is $A$ is $\text{Pr}(A)=p$, then its **odds** (in favor of the event) are $\frac{p}{1-p}$. The logistic regression model "raw" predictions are **log-odds** $\log\frac{p}{1-p}$.  


# Random variables to assign number to atomic events

## Random variable {.smaller}

- A random variable is a numerical function where values come with probabilities. 
- In some statistical model, we consider variables in a data frame as *random variables*, for example the response variable in a generalized linear model. 

Formally, a **random variable** is

- a function $X: S \to \mathbb{R}$ 
- which assigns a value to each atomic event in the sample space. 

Together with a probability function $\text{Pr}: \mathcal{F}(S)\to [0,1]$ probabilities can be assigned to values of the random variable (see the *probability mass function* in two slides).


## Examples of random variables {.smaller}

:::{.incremental}
- For two coin tosses a random variable can be **the number of HEADS**. In this case, each atomic event is mapped to a number: Either 0, 1, or 2. 
- For 62 randomly selected organ donations a random variable can be **the number of complications**. Each atomic event is mapped to an integer from 0 to 62. (Note, an atomic event are 62 randomly selected organ donations. So, the set of events is $2^{62} \approx 4.61\cdot 10^{18}$.)
- In the palmer penguins dataset we can consider a variable, e.g. flipper length, to be a random variable. The atomic event would be the random selection of a penguin and the random variable is its flipper length. So we map each penguin to its flipper length. 
:::

. . . 

[A random variable is a way to look at a *numerical* aspect of a sample space. It often *simplfies* because many atomic events may be mapped to the same number. ]{style='color:blue;'}


## Probability mass function (pmf) {.smaller}

For 

- a *random variable* $X$ and 
- a *probability function* $\text{Pr}$ 

the **probability mass function** $f_X: \mathbb{R} \to [0,1]$ is defined as

$$f_X(x) = \text{Pr}(X=x),$$ 

where $\text{Pr}(X=x)$ is an abbreviation for $\text{Pr}(\{a\in S\text{ for which } X(a) = x\})$. 

## Example pmf for 2 coin tosses {.smaller}

Two coin tosses $S = \{HH, HT, TH, TT\}$   

  - We define $X$ to be the number of heads:   
  $X(HH) = 2$, $X(TH) = 1$, $X(HT) = 1$, and $X(TT) = 0$.   
  - We assume the probability function $\text{Pr}$ assigns for each atomic event a probability of 0.25. 
  - Then the probability mass function is
$$\begin{align} f_X(0) = & \text{Pr}(X=0) = \text{Pr}(\{TT\}) & = 0.25 \\ 
f_X(1) = & \text{Pr}(X=1) = \text{Pr}(\{HT,TH\}) & = 0.25 + 0.25 = 0.5 \\
f_X(2) = &\text{Pr}(X=2) = \text{Pr}(\{HH\}) & = 0.25\end{align}$$
  - Note that  $\text{Pr}(\{HT,TH\}) = \text{Pr}(\{HT\}) + \text{Pr}(\{HT\})$ by adding the probabilities of the atomic events. 
  - For all $x$ which are not 0, 1, or 2 it is obviously $f_X(x) = 0$.

<!-- In a data frame `D` with $m$ cases, we can consider a variable `"varname"` (of type numeric) as a random variable with -->
<!-- $X: \{1,2,\dots,m\} \to \mathbb{R}$ and $X(i) =$`D[i,"varname"]`.   -->

<!-- Let us consider for that data frame that $X$ is the height of $m$ people measured very precisely. Then, the probability of a certain height, e.g. 1.6782, is $\text{Pr}(X=1.6782) = \frac{1}{m}$ as for any other height. This is not very interesting. More interesting is the probability that the height is between 1.6 and 1.7. Of course, these values can be computed for a data frame. Mathematically, we like to have a formal treatment for such continuous random variables. To that end, we will generalize the probability mass function to a *probability density function* later.  -->

## Example: Roll two dice 🎲 🎲{.smaller}

**Random variable:** The sum of both dice. 

. . .

**Events:** All 36 combinations of rolls 1+1, 1+2, 1+3, 1+4, 1+5, 1+6, 2+1, 2+2, 2+3, 2+4, 2+5, 2+6, 3+1, 3+2, 3+3, 3+4, 3+5, 3+6, 4+1, 4+2, 4+3, 4+4, 4+5, 4+6, 5+1, 5+2, 5+3, 5+4, 5+5, 5+6, 6+1, 6+2, 6+3, 6+4, 6+5, 6+6     

. . . 

**Possible values of the random variable: **
2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 (These are numbers.)

. . .

**Probability mass function:**
(Assuming each number has probability of $\frac{1}{6}$ for each die.)

:::: {.columns}

::: {.column width='40%'}
$\text{Pr}(2) = \text{Pr}(12) = \frac{1}{36}$  
$\text{Pr}(3) = \text{Pr}(11) = \frac{2}{36}$  
$\text{Pr}(4) = \text{Pr}(10) = \frac{3}{36}$  
$\text{Pr}(5) = \text{Pr}(9) = \frac{4}{36}$  
$\text{Pr}(6) = \text{Pr}(8) = \frac{5}{36}$  
$\text{Pr}(7) = \frac{6}{36}$  
:::

::: {.column width='60%'}
```{r}
#| echo: true
#| fig-height: 2.5
tibble(Value = 0:15, pmf=c(0,c(0:6,5:0)/36,0,0)) |> 
 ggplot(aes(Value,pmf)) + geom_line() + geom_point() +
 theme_minimal(base_size = 20)
```

:::

::::



# Binomial distribution

## Binomial distribution {.smaller}

The number of HEADS in several coin tosses and the number of complications in randomly selected organ donations are examples of random variable which have a **binomial distribution**. 

. . .

Definition: The **binomial distribution** with parameters $n$ and $p$ is the number of successes in a sequence of $n$ independent *Bernoulli trials* which each delivers a *success* with probability $p$ and a *failure* with probability $(1-p)$. 

- The default model for the number of successes drawn from a sample of size $n$ drawn from a population of size $N$ with replacement. 
- When $N$ is much larger than $n$ it is also a good approximation for drawing without replacement. 


## Binomial probability mass function {.smaller}

$$f(k,n,p) = \Pr(k;n,p) = \Pr(X = k) = \binom{n}{k}p^k(1-p)^{n-k}$$

where $k$ is the number of successes, $n$ is the number of Bernoulli trials, and $p$ the success probability. 

Probability to have *exactly* 3 complications in 62 randomly selected organ donations with complication probability $p=0.1$ is

```{r}
#| echo: true
# x represents k, and size represents n
dbinom(x = 3, size = 62, prob = 0.1)
```

The probability to have 3 complications *or less* can be computed as

```{r}
#| echo: true
dbinom(3, 62, 0.1) + dbinom(2, 62, 0.1) + dbinom(1, 62, 0.1) + dbinom(0, 62, 0.1)
```

. . . 

[This was the p-value we computed with simulation for the hypothesis testing example.]{style='color:blue;'} 

## Distribution functions are vectorized! {.smaller}

Compute the p-value:

```{r}
#| echo: true
dbinom(0:3, 62, 0.1) |> sum()
```

Plotting the probability mass function

```{r}
#| echo: true
#| fig-height: 2.5
#| output-location: column
tibble(x = 0:62) |> 
 mutate(pr = dbinom(x, size = 62, prob = 0.1)) |> 
 ggplot(aes(x, pr)) + 
 geom_col() + 
 theme_minimal(base_size = 24)
```


## Other plots of binomial mass function {.smaller}

Changing the sample size:

```{r}
#| echo: true
#| fig-height: 2.5
#| output-location: column
tibble(samplesize = 0:62) |> 
 mutate(pr = dbinom(3, size = samplesize, prob = 0.1)) |> 
 ggplot(aes(samplesize, pr)) + 
 geom_col() + 
 theme_minimal(base_size = 24)
```

[The probability of 3 successes is most likely for sample sizes around 30. Sensible?]{style='color:blue;'}

. . . 

Changing the success probability:

```{r}
#| echo: true
#| fig-height: 2.5
#| output-location: column
tibble(probs = seq(0,0.3,0.01)) |> 
 mutate(pr = dbinom(3, size = 62, prob = probs)) |> 
 ggplot(aes(probs, pr)) + 
 geom_col() + 
 scale_x_continuous(breaks = seq(0,0.3,0.05)) + 
 theme_minimal(base_size = 24)
```

[The probability of 3 successes is most likely for success probabilities around 0.05.]{style='color:blue;'}


## Expected value discrete RV {.smaller}

A **discrete random variable** takes only a *finite* (or at least discrete) set of values. 

For $X: S \to \mathbb{R}$, when $S$ is finite, there is naturally only a set of values $x_1,\dots,x_k\in\mathbb{R}$ which $X$ can be. We call their probabilities $p_1,\dots,p_k$ with $p_i = \text{Pr}(X=x_i) = \text{Pr}(\{a \in S \text{ for which } X(a) = x_i \})$.   
(The probability of all other values in $\mathbb{R}$ is zero.). 

The **expected value** of $X$ is $E(X) = \sum_{i=1}^k p_i x_i = p_1x_1 + \dots + p_kx_k.$

**Examples:** $X$ is a die roll 🎲. $E(X) = 1\cdot\frac{1}{6} + 2\cdot\frac{1}{6} + 3\cdot\frac{1}{6} + 4\cdot\frac{1}{6} + 5\cdot\frac{1}{6} + 6\cdot\frac{1}{6} = \frac{21}{6} = 3.5$    
$X$ sum of two die rolls 🎲🎲. 

$E(X) = 2\cdot\frac{1}{36} + 3\cdot\frac{2}{36} + 4\cdot\frac{3}{36} + 5\cdot\frac{4}{36} + 6\cdot\frac{5}{36} + 7\cdot\frac{6}{36} + 8\cdot\frac{5}{36} +$   
$+ 9\cdot\frac{4}{36} + 10\cdot\frac{3}{36} + 11\cdot\frac{2}{36} + 12\cdot\frac{1}{36} = 7$

## Expected value binomial distribution {.smaller}

For $X \sim \text{Binom}(n,p)$ (read "$X$ has a binomial distribution with samplesize $n$ and success probability $p$")

The expected value of $X$ is by definition

$$E(X) = \underbrace{\sum_{k = 0}^n k}_{\text{sum over successes}} \cdot \underbrace{\binom{n}{k}p^k(1-p)^{n-k}}_{\text{probability of successes}}$$

Computation shows that $E(X) = p\cdot n$.

**Example:** For $n = 62$ organ donations with complication probability $p=0.1$, the expected number of complications is $E(X) = 6.2$. 



# Distribution Functions

## General systematic of functions for distributions in R {.smaller}

In R we usually have 4 function for each distribution: The `d`, `p`, `q`, and `r` version. For the binomial distribution: 

- `dbinom` the density function (more on the name later)

- `pbinom` distribution function 

- `qbinom` the quantile function, and 

- `rbinom` random number generator.


## Probability mass function `d` {.smaller}

- The **mass function** (or *density function*, more on this later) `dbinom`

```{r}
#| echo: true
#| fig-height: 3.5
#| output-location: column
x <- 0:10
tibble(x = x) |> 
 mutate(pr = dbinom(x, size = 10, prob = 0.5)) |> 
 ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)
```

[Gives the probability for the number $x$: $\text{Pr}(X = x)$ or $f_X(x)$.]{style='color:blue;'}


## Distribution function `p` {.smaller}

- The **distribution function**, or cumulative probability function `pbinom`

```{r}
#| echo: true
#| fig-height: 3.5
#| output-location: column
x <- 0:10
tibble(x = x) |> 
 mutate(pr = pbinom(x, size = 10, prob = 0.5)) |> 
 ggplot(aes(x, pr)) + geom_col() + theme_minimal(base_size = 24)
```

[Gives the probability that the random variable is less or equal to $x$:  
$\text{Pr}(X \leq x)$.]{style='color:blue;'}


## Quantile function `q` {.smaller}

- The **quantile function**, `qbinom` with argument $p$ representing the fraction of lowest values of $X$ among all values for which we want the $x$ value for. 

```{r}
#| echo: true
#| fig-height: 3.5
#| output-location: column
probs <- seq(0, 1, by = 0.01)
tibble(p = probs) |> 
 mutate(x = qbinom(p, size = 10, prob = 0.5)) |> 
 ggplot(aes(p, x)) + geom_line() + theme_minimal(base_size = 24)
```

[A point $(p,x)$ means: When we want a $p$-fraction of the probability mass, we need all events with values lower or equal to $x$.]{style='color:blue;'}



## Calculus relations {.smaller}

- Quantile, distribution and mass function all carry the *full information* about the distribution of a random variable $X$. 

- The *mass function* is the **derivative** of the *distribution function*.   
(The *distribution function* is the anti-derivative of the *mass function*.)

```{r}
#| echo: true
pbinom(0:5, size = 5, prob = 0.5) 
# Next comes its derivative (have to append a 0 before first)
pbinom(0:5, size = 5, prob = 0.5) |> append(0, after = 0) |> diff()
dbinom(0:5, size = 5, prob = 0.5)
# Next comes its anti-derivative
dbinom(0:5, size = 5, prob = 0.5) |> cumsum()
```


## More calculus relations {.smaller}

- The quantile function is the inverse of the distribution function.
- We plot the inverse function by interchanging the `x` and `y` aesthetic.

```{r}
#| echo: true
#| output-location: column
probs <- seq(0, 1, by = 0.01)
x <- 0:10
q <- tibble(p = probs) |> mutate(x = qbinom(p, size = 10, prob = 0.5)) 
p <- tibble(x = x) |> mutate(p = pbinom(x, size = 10, prob = 0.5)) 
q_plot <- q |> ggplot(aes(p, x)) + geom_line()
qinv_plot <- q |> ggplot(aes(x, p)) + geom_line()
p_plot <- p |> ggplot(aes(x, p)) + geom_col()
pinv_plot <- p |> ggplot(aes(p, x)) + geom_col(orientation = "y")
library(patchwork)
(q_plot | p_plot) / (pinv_plot | qinv_plot)
```


## Random number generator `r` {.smaller}

- Random binomial numbers are drawn with `rbinom`

```{r}
#| echo: true
# 10 random binomial numbers for 62 trials with success probability 0.1
rbinom(10, size = 62, prob = 0.1)
```

- We can reproduce the null distribution from hypothesis testing with 62 organ donations and 10% complication probability this way.
  - We produce 100,000 random consultants
  - Then we compute the fraction of  which have 3 or less complications

```{r}
#| echo: true
set.seed(2022)
s <- rbinom(100000, size = 62, prob = 0.1)
sum(s<=3)/100000
# Two other samples
sum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000
sum(rbinom(100000, size = 62, prob = 0.1)<=3)/100000
```

## Empirical distributions {.smaller}

```{r}
ess_raw <- read_csv("data/ESS-Data-Wizard-subset-2022-09-17.csv",
                col_types = cols(
                 name = col_character(),
                 essround = col_double(),
                 edition = col_double(),
                 proddate = col_character(),
                 idno = col_double(),
                 cntry = col_character(),
                 dweight = col_double(),
                 pspwght = col_double(),
                 pweight = col_double(),
                 euftf = col_double(),
                 gincdif = col_double(),
                 lrscale = col_double(),
                 polintr = col_double(),
                 stflife = col_double(),
                 trstplc = col_double(),
                 vote = col_double(),
                 imueclt = col_double(),
                 atchctr = col_double(),
                 atcherp = col_double(),
                 crmvct = col_double(),
                 pray = col_double(),
                 rlgdgr = col_double(),
                 gndr = col_double(),
                 age = col_double()
                ))
ess <- ess_raw |> filter(essround == 9) |>
 mutate(atchctr = atchctr |> na_if(77) |> na_if(88) |> na_if(99),
        atcherp = atcherp |> na_if(77) |> na_if(88) |> na_if(99),
        euftf = euftf |> na_if(77) |> na_if(88) |> na_if(99),
        lrscale = lrscale |> na_if(77) |> na_if(88) |> na_if(99),
        imueclt = imueclt |> na_if(77) |> na_if(88) |> na_if(99))
```

- $X$: select a random person from Europe (in 2018, willing to answer survey) and ask its attitude towards the European union from 0 to 10


:::: {.columns}

::: {.column width='40%'}
- What is the distribution of the answer?

```{r}
#| echo: true
#| fig-height: 2
eu <- ess |> select(euftf) |> drop_na() |> 
 count(euftf) |> mutate(prob = n/sum(n)) 
eu
```
:::

::: {.column width='60%'}
Mass function and distribution function
```{r}
#| echo: true
#| fig-height: 3
eu_mass <- eu |> ggplot(aes(euftf, prob)) + geom_col()
eu_distr <- eu |> mutate(cumprob = cumsum(prob)) |> 
 ggplot(aes(euftf, cumprob)) + geom_col()
eu_mass | eu_distr
```

:::

::::



## What could be next?

- Continuous distribution function 
  - Theoretical and empirical
- The central limit theorem and why it is important empirically
- Independence of probabilistic events
- Conditional probability and the confusion matrix
- Markov chains

<!-- # The 3 problems: Regression, Classification, Clustering -->


# Principal component analysis (PCA)

## PCA Description {.smaller}

Principle component analysis 

- is a **dimensionality-reduction** technique, that means it can be used to reduce the number of variables
- computes new variables which represent the data in a different way
- transforms the data **linearly** to a new coordinate system where most of the variation in the data can be described with fewer variables than the original data
- can be seen as *unsupervised learning* technique because there is no response variable. (Response variable are often produced/supervised by humans for training, e.g., a spam dummy.)

**Today:** Quick walk through how to use and interpret it. 


<!-- ## PCA Application -->


## "Other" data of OWiD Corona data {.smaller}

:::: {.columns}

::: {.column width='48%'}

- Select the variables listed as *Others* in the [OWiD corona data documentation](https://github.com/owid/covid-19-data/tree/master/public/data)
- Remove those which have many `NA`s 

```{r}
#| echo: true
owid <- read_csv("data/owid-covid-data.csv")
owid_inds <- owid |> 
 # Filter for one day and remove rows where continent is NA
 # These are rows with data for continents or world regions
 filter(date == "2022-10-01", !is.na(continent)) |> 
 # These are the "Other" variables
 select(iso_code, continent, location, 
        population:human_development_index) |>
 # We remove the ones with many NA's
 select(-handwashing_facilities, -male_smokers, 
        - female_smokers, -extreme_poverty) |> 
 drop_na()
owid_inds |> count(continent)
```
:::

::: {.column width='48%'}
We have `r nrow(owid_inds)` countries and `r ncol(owid_inds)-3` numeric variables.

```{r}
#| echo: true
names(owid_inds)
```
:::

::::

## Two Variables {.smaller}

Example for the new axes. 

```{r}
#| fig-height: 4
pca1 <- owid_inds |> select(median_age, life_expectancy) |> 
 prcomp(~., data = _, scale = FALSE)
arrow_style <- arrow(
  angle = 20, ends = "first", type = "closed", length = grid::unit(8, "pt")
)
pca1 |> tidy(matrix = "rotation") |> 
 pivot_wider(names_from = "PC", names_prefix = "PC", values_from = "value") |>
 ggplot(aes(x = (PC1*pca1$sdev) + mean(owid_inds$median_age),
            y = (PC2*pca1$sdev) + mean(owid_inds$life_expectancy))) + 
 geom_point(data = owid_inds,
            mapping = aes(median_age, life_expectancy, color = continent)) + 
 geom_segment(xend=mean(owid_inds$median_age),
              yend=mean(owid_inds$life_expectancy), arrow = arrow_style) +
 coord_fixed() +
 labs(x="Median age",y="Life expectancy")
```

::: aside
The two arrows show the two eigenvectors of the covariance matrix of the two variables scaled by the square root of the corresponding eigenvalues, and shifted so their origins are at the means of both variables. 
:::


## Computation in R {.smaller}

The basic function is base-R's `prcomp` (there is an older `princomp` which is not advisable to use). 

These 3 commands all deliver identical results for `prcomp`
```{r}
#| echo: true
# prcomp can take a formula with no response variable as 1st argument
P <- prcomp(~ median_age + life_expectancy, data= owid_inds)
# prcomp can take a data frame with all numerical vectors as 1st argument
P <- owid_inds |> select(median_age, life_expectancy) |> prcomp()
# This is an example using the formula placeholder "." for 'take all
# and the pipe's placeholder "_" which one uses when the output should be 
# piped to an argument other than the 1st
P <- owid_inds |> select(median_age, life_expectancy) |> prcomp(~ ., data= _)
```


:::: {.columns}

::: {.column width='50%'}
The standard output

```{r}
#| echo: true
P
```
:::

::: {.column width='50%'}
The summary output
```{r}
#| echo: true
summary(P)
```
:::

::::

## The `prcomp` object {.smaller}

Includes 4 different related entities.

:::: {.columns}

::: {.column width='50%'}
The **standard deviations** related to each principal component.   
```{r}
#| echo: true
P$sdev
```

The matrix of variable **loadings**. (It is also the matrix which rotates the original data vectors.)
```{r}
#| echo: true
P$rotation
```
:::

::: {.column width='50%'}
The means for each original variable. 
```{r}
#| echo: true
P$center
```
Note, there are also standard deviations of original variables in `$scale` when this is set to be used.

The centered (scaled, if set) and rotated data.
```{r}
#| echo: true
P$x
```
:::

::::

## PCA as Exploratory Data Analysis {.smaller}

Suppose we do a PCA with all `r nrow(owid_inds)` countries (rows) and all `r ncol(owid_inds)-3` numeric variables.

- *How long will the vector of standard deviations be?* [`r ncol(owid_inds)-3`]{.fragment}  
- *What dimensions will the rotation matrix have?* [`r ncol(owid_inds)-3` x `r ncol(owid_inds)-3`]{.fragment}  
- *What dimensions will the rotated data frame have?* [`r nrow(owid_inds)` x `r ncol(owid_inds)-3`]{.fragment}

. . . 

When we do a PCA for exploration there are 3 things to look at: 

1. The data in PC coordinates - the centered (scaled, if set) and rotated data.  
2. The rotation matrix - the variable loadings.
3. The variance explained by each PC - based on the standard deviations. 

## All variables {.smaller}

Now, with `scale = TRUE` (recommended). Data will be centered and scaled (a.k.a. standardized) first. 

```{r}
#| echo: true
owid_PCA <- owid_inds |> select(-iso_code, -continent, -location) |> 
 prcomp(~ ., data = _, scale = TRUE)
owid_PCA
```


## Data in PC coordinates {.smaller}


:::: {.columns}

::: {.column width='35%'}
- Start plotting PC1 against PC2. By default these are the most important ones. Drill deeper later. 
- Use the function `augment` to append the original data. Here used to draw labels and color by continent. 
- Note: `augment` also created the variables names like `.fittedPC1` 
:::

::: {.column width='65%'}
```{r}
#| echo: true
#| fig-height: 5
plotdata <- owid_PCA |> parsnip::augment(owid_inds)
plotdata |> ggplot(aes(.fittedPC1, .fittedPC2, color = continent)) +
 geom_point() + 
 geom_text(data = plotdata |> 
            filter(.fittedPC2< -3 | .fittedPC1< -5 | .fittedPC1>4), 
           mapping = aes(.fittedPC1, .fittedPC2, label = iso_code),
           color = "black") +
 coord_fixed() + theme_minimal(base_size = 20)
```
:::

::::


## Variable loadings {.smaller}

- The columns of the rotation matrix shows how the original variables *load* on the principle components. 
- We can try to interpret these loadings and give names to components. 
- `tidy` extracts the rotation matrix in long format with a `PC`, a `column` (for the original variable name), and a `value` variable
. 
```{r}
#| echo: true
#| fig-height: 3
owid_PCA |> parsnip::tidy(matrix = "rotation") |> 
 filter(PC<=6) |> 
 ggplot(aes(value, column, fill=value)) + geom_col() +
 facet_wrap(~PC, nrow = 1)
```

## Variance explained {.smaller}

- Principle components are by default sorted by importance. 
- The squares of the standard deviation for each component gives its variances and variances have to sum up to the number of variables (in the standardized case). 
- This way the `tidy` command creates a variable `percent` which gives the fraction of the total variance explained by each component.  

```{r}
#| echo: true
#| fig-height: 3
owid_PCA |> tidy(matrix = "eigenvalues") |> 
 ggplot(aes(PC, percent)) + geom_col()
```


## Interpretations (1) {.smaller}

```{r}
#| fig-height: 3
owid_PCA |> tidy(matrix = "eigenvalues") |> 
 ggplot(aes(PC, percent)) + geom_col()
```

- The first component explains almost 50% of the variance. So most emphasize should be on this. 
- To reach more than 75% of the total variance the first four components are needed. 
- After the fifth component the added explained variance drops substantially. This is another typical reason to cut off the rest. 
- Taking the five components explains 89.9% of the variance of the original 11 variables!

## Interpretations (2) {.smaller}

```{r}
#| fig-height: 3
owid_PCA |> parsnip::tidy(matrix = "rotation") |> 
 filter(PC<=5) |> 
 ggplot(aes(value, column)) + geom_col() +
 facet_wrap(~PC, nrow = 1)
```

1. To score high on PC1 a country needs to be poor, and having few old people.
2. PC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths. 
3. PC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population.
4. PC4 focuses mostly on population. 
5. PC5 mostly on population density. 

## Interpretations (3) {.smaller}

```{r}
#| fig-height: 5
plotdata <- owid_PCA |> parsnip::augment(owid_inds)
g1 <- plotdata |> ggplot(aes(.fittedPC1, .fittedPC2, color = continent)) +
 geom_point() + 
 geom_text(data = plotdata |> 
            filter(.fittedPC2< -3 | .fittedPC1< -5 | .fittedPC1>4), 
           mapping = aes(.fittedPC1, .fittedPC2, label = iso_code),
           color = "black") +
 coord_fixed()
g2 <- owid_PCA |> parsnip::tidy(matrix = "rotation") |> 
 filter(PC<=2) |> 
 ggplot(aes(value, column)) + geom_col() +
 facet_wrap(~PC, nrow = 1)
library(patchwork)
g1 + g2
```

- To score high on PC1 a country needs to be poor, and having few old people.
- PC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths. 

## Interpretations (4) {.smaller}

```{r}
#| fig-height: 5
plotdata <- owid_PCA |> parsnip::augment(owid_inds)
g1 <- plotdata |> ggplot(aes(.fittedPC1, .fittedPC3, color = continent)) +
 geom_point() + 
 geom_text(data = plotdata |> 
            filter(.fittedPC3< -1.5 | .fittedPC1< -5 | .fittedPC3>1.9), 
           mapping = aes(.fittedPC1, .fittedPC3, label = iso_code),
           color = "black") +
 coord_fixed()
g2 <- owid_PCA |> parsnip::tidy(matrix = "rotation") |> 
 filter(PC==1 | PC==3) |> 
 ggplot(aes(value, column)) + geom_col() +
 facet_wrap(~PC, nrow = 1)
g1 + g2
```

- To score high on PC1 a country needs to be poor, and having few old people.
- PC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population.


## Interpretations (5) {.smaller}

```{r}
#| fig-height: 5
plotdata <- owid_PCA |> parsnip::augment(owid_inds)
g1 <- plotdata |> ggplot(aes(.fittedPC2, .fittedPC3, color = continent)) +
 geom_point() + 
 geom_text(data = plotdata |> 
            filter(.fittedPC2>2 | .fittedPC3>2), 
           mapping = aes(.fittedPC2, .fittedPC3, label = iso_code),
           color = "black") +
 coord_fixed()
g2 <- owid_PCA |> parsnip::tidy(matrix = "rotation") |> 
 filter(PC==2 | PC==3) |> 
 ggplot(aes(value, column)) + geom_col() +
 facet_wrap(~PC, nrow = 1)
g1 + g2
```

- PC2 characterizes countries with low population density low diabetes prevalence and low gdp, but a high number of hospital beds, and cardiovascular deaths. 
- PC3 characterizes countries with high diabetes prevalence and cardiovascular deaths, but also with large population.

## Interpretations (6) {.smaller}

```{r}
#| fig-height: 4
plotdata <- owid_PCA |> parsnip::augment(owid_inds)
g1 <- plotdata |> ggplot(aes(.fittedPC4, .fittedPC5, color = continent)) +
 geom_point() + 
 geom_text(data = plotdata |> 
            filter(.fittedPC5< -1.5 | .fittedPC4> 5 | .fittedPC5>3), 
           mapping = aes(.fittedPC4, .fittedPC5, label = iso_code),
           color = "black") +
 coord_fixed()
g2 <- owid_PCA |> parsnip::tidy(matrix = "rotation") |> 
 filter(PC==4 | PC==5) |> 
 ggplot(aes(value, column)) + geom_col() +
 facet_wrap(~PC, nrow = 1)
g1 + g2

```

- PC4 focuses mostly on population. 
- PC5 mostly on population density. 


## Apply PCA {.smaller}

- Besides standardization, PCA may benefit by **preprocessing** steps of **data transformation** with variables with skew distributions (log, square-root, or Box-Cox transformation). This may result in less outliers.
- PCA is a often a useful step of **exploratory data analysis** when you have a large number of numerical variables to show the empirical *dimensionality* of the data and its structure
- Limitation: PCA is only sensitive for linear relation ships (no U-shaped) or the like
- The can be used **as predictors** in a model instead of the raw variables. 

## Properties and relations of PCA {.smaller}

- The principal components (the columns of the rotation matrix) are maximally *uncorrelated* (actually they are even *orthogonal*).
- This also holds for the columns of the rotated data. 
- The total variances of all prinicipal components sum up to the number of variables (when variables are standardized)
- The PCA is unique. All principle components together are a complete representation of the data. (Unlike other technique of dimensionality reduction which may rely on starting values, random factors, or tuning parameters)
- A technique similar in spirit is *factor analysis* (e.g. `factanal`). It is more theory based as it requires to specify to the theoriezed number of factors up front. 


